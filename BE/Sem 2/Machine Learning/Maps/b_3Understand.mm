<map version="freeplane 1.7.0">
<!--To view this file, download free mind mapping software Freeplane from http://freeplane.sourceforge.net -->
<attribute_registry SHOW_ATTRIBUTES="hide"/>
<node TEXT="MachineLearning" LOCALIZED_STYLE_REF="AutomaticLayout.level.root" FOLDED="false" ID="ID_191153586" CREATED="1562675315160" MODIFIED="1563517413228" ICON_SIZE="36.0 pt" LINK="../1_Machine%20Learning_MasterLookup.mm"><hook NAME="MapStyle">
    <properties fit_to_viewport="false" show_icon_for_attributes="false" edgeColorConfiguration="#00cc33ff,#00cc33ff,#00cc33ff,#00cc33ff,#00cc33ff,#00cc33ff,#00cc33ff,#00cc33ff,#00cc33ff,#00cc33ff,#00cc33ff,#00cc33ff"/>

<map_styles>
<stylenode LOCALIZED_TEXT="styles.root_node" STYLE="oval" UNIFORM_SHAPE="true" VGAP_QUANTITY="24.0 pt">
<font SIZE="24"/>
<stylenode LOCALIZED_TEXT="styles.predefined" POSITION="right" STYLE="bubble">
<stylenode LOCALIZED_TEXT="default" ICON_SIZE="12.0 pt" COLOR="#000000" STYLE="fork">
<font NAME="SansSerif" SIZE="10" BOLD="false" ITALIC="false"/>
</stylenode>
<stylenode LOCALIZED_TEXT="defaultstyle.details"/>
<stylenode LOCALIZED_TEXT="defaultstyle.attributes">
<font SIZE="9"/>
</stylenode>
<stylenode LOCALIZED_TEXT="defaultstyle.note" COLOR="#000000" BACKGROUND_COLOR="#ffffff" TEXT_ALIGN="LEFT"/>
<stylenode LOCALIZED_TEXT="defaultstyle.floating">
<edge STYLE="hide_edge"/>
<cloud COLOR="#f0f0f0" SHAPE="ROUND_RECT"/>
</stylenode>
</stylenode>
<stylenode LOCALIZED_TEXT="styles.user-defined" POSITION="right" STYLE="bubble">
<stylenode LOCALIZED_TEXT="styles.topic" COLOR="#18898b" STYLE="fork">
<font NAME="Liberation Sans" SIZE="10" BOLD="true"/>
</stylenode>
<stylenode LOCALIZED_TEXT="styles.subtopic" COLOR="#cc3300" STYLE="fork">
<font NAME="Liberation Sans" SIZE="10" BOLD="true"/>
</stylenode>
<stylenode LOCALIZED_TEXT="styles.subsubtopic" COLOR="#669900">
<font NAME="Liberation Sans" SIZE="10" BOLD="true"/>
</stylenode>
<stylenode LOCALIZED_TEXT="styles.important">
<icon BUILTIN="yes"/>
</stylenode>
</stylenode>
<stylenode LOCALIZED_TEXT="styles.AutomaticLayout" POSITION="right" STYLE="bubble">
<stylenode LOCALIZED_TEXT="AutomaticLayout.level.root" ICON_SIZE="14.0 pt" COLOR="#000000" STYLE="oval">
<font NAME="Segoe Print" SIZE="22"/>
<edge COLOR="#ffffff"/>
</stylenode>
<stylenode LOCALIZED_TEXT="AutomaticLayout.level,1" ICON_SIZE="18.0 px" BORDER_WIDTH_LIKE_EDGE="true" COLOR="#000000" SHAPE_HORIZONTAL_MARGIN="0.1 pt" SHAPE_VERTICAL_MARGIN="0.1 pt">
<font SIZE="18" BOLD="false" ITALIC="true"/>
<edge STYLE="sharp_bezier" COLOR="#00cc33" WIDTH="8"/>
</stylenode>
<stylenode LOCALIZED_TEXT="AutomaticLayout.level,2" ICON_SIZE="16.0 px" BORDER_WIDTH_LIKE_EDGE="true" COLOR="#000000">
<font SIZE="16"/>
<edge STYLE="sharp_bezier" COLOR="#00cc33" WIDTH="3"/>
</stylenode>
<stylenode LOCALIZED_TEXT="AutomaticLayout.level,3" ICON_SIZE="14.0 px" BORDER_WIDTH_LIKE_EDGE="true" COLOR="#000000">
<font SIZE="14"/>
<edge STYLE="sharp_bezier" COLOR="#00cc33" WIDTH="3"/>
</stylenode>
<stylenode LOCALIZED_TEXT="AutomaticLayout.level,4" ICON_SIZE="14.0 px" BORDER_WIDTH_LIKE_EDGE="true" COLOR="#000000">
<font SIZE="13"/>
<edge STYLE="sharp_bezier" COLOR="#00cc33" WIDTH="2"/>
</stylenode>
<stylenode LOCALIZED_TEXT="AutomaticLayout.level,5" ICON_SIZE="14.0 px" BORDER_WIDTH_LIKE_EDGE="true">
<font SIZE="13"/>
<edge STYLE="sharp_bezier" COLOR="#00cc33" WIDTH="1"/>
</stylenode>
<stylenode LOCALIZED_TEXT="AutomaticLayout.level,6" ICON_SIZE="14.0 px">
<font SIZE="13"/>
<edge STYLE="bezier"/>
</stylenode>
<stylenode LOCALIZED_TEXT="AutomaticLayout.level,7" ICON_SIZE="14.0 px">
<font SIZE="13"/>
<edge STYLE="bezier"/>
</stylenode>
<stylenode LOCALIZED_TEXT="AutomaticLayout.level,8" ICON_SIZE="14.0 px">
<edge STYLE="bezier"/>
<font SIZE="13"/>
</stylenode>
<stylenode LOCALIZED_TEXT="AutomaticLayout.level,9" ICON_SIZE="14.0 px">
<font SIZE="13"/>
<edge STYLE="bezier"/>
</stylenode>
<stylenode LOCALIZED_TEXT="AutomaticLayout.level,10" ICON_SIZE="14.0 px">
<font SIZE="13"/>
<edge STYLE="bezier"/>
</stylenode>
<stylenode LOCALIZED_TEXT="AutomaticLayout.level,11" ICON_SIZE="14.0 px">
<edge STYLE="bezier"/>
</stylenode>
</stylenode>
</stylenode>
</map_styles>
</hook>
<hook NAME="AutomaticEdgeColor" COUNTER="27" RULE="ON_BRANCH_CREATION"/>
<hook NAME="accessories/plugins/AutomaticLayout.properties" VALUE="ALL"/>
<font SIZE="20"/>
<node TEXT="ADEPT" LOCALIZED_STYLE_REF="defaultstyle.floating" POSITION="left" ID="ID_356339431" CREATED="1549426121539" MODIFIED="1549434098401">
<node TEXT="A" ID="ID_621380803" CREATED="1549426179088" MODIFIED="1549426291603">
<edge STYLE="bezier"/>
<node TEXT="Analogy" ID="ID_182396775" CREATED="1549426193173" MODIFIED="1549426291602">
<edge STYLE="bezier"/>
</node>
</node>
<node TEXT="D" ID="ID_688331787" CREATED="1549426181833" MODIFIED="1549426291603">
<edge STYLE="bezier"/>
<node TEXT="Diagram" ID="ID_424948524" CREATED="1549426199017" MODIFIED="1549426291602">
<edge STYLE="bezier"/>
<node TEXT="Visualisation" ID="ID_1239223974" CREATED="1549426225828" MODIFIED="1549426291602">
<edge STYLE="bezier"/>
</node>
<node TEXT="Animation" ID="ID_993378749" CREATED="1549426230717" MODIFIED="1549426291601">
<edge STYLE="bezier"/>
</node>
</node>
</node>
<node TEXT="E" ID="ID_579018414" CREATED="1549426185556" MODIFIED="1549426291604">
<edge STYLE="bezier"/>
<node TEXT="Example" ID="ID_789271875" CREATED="1549426205652" MODIFIED="1549426291601">
<edge STYLE="bezier"/>
</node>
</node>
<node TEXT="P" ID="ID_396767372" CREATED="1549426187712" MODIFIED="1549426291604">
<edge STYLE="bezier"/>
<node TEXT="Plain English" ID="ID_1805083165" CREATED="1549426212197" MODIFIED="1549426291599">
<edge STYLE="bezier"/>
<node TEXT="LIM5YO" ID="ID_1482302333" CREATED="1551437777014" MODIFIED="1551437795883"/>
</node>
</node>
<node TEXT="T" ID="ID_1508620179" CREATED="1549426189926" MODIFIED="1549426291604">
<edge STYLE="bezier"/>
<node TEXT="Technical" ID="ID_874459994" CREATED="1549426217333" MODIFIED="1549426291601">
<edge STYLE="bezier"/>
<node TEXT="Colorized Formulae" ID="ID_1104101457" CREATED="1549426242238" MODIFIED="1549426291601">
<edge STYLE="bezier"/>
</node>
</node>
</node>
<node TEXT="Video" ID="ID_34939331" CREATED="1549451938894" MODIFIED="1549451940980"/>
</node>
<node TEXT="Introduction To Machine Learning" FOLDED="true" POSITION="right" ID="ID_1594697030" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<edge COLOR="#00cc33"/>
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Classical and Adaptive Machines" ID="ID_1070786091" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Classical and Adaptive Machines#$D$#" FOLDED="true" ID="ID_551192887" CREATED="1557224068596" MODIFIED="1557225483420">
<icon BUILTIN="stop-sign"/>
<node TEXT="Introduction - classic and adaptive machines - Machine Learning " FOLDED="true" ID="ID_615778129" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781785889622/1/ch01lvl1sec8/introduction-classic-and-adaptive-machines">
<node TEXT="Introduction - classic and adaptive machinesSince time immemorial human beings have built tools and machines to simplify their" ID="ID_775057903" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="Classic Machine And Adaptive Machine ll Machine Learning Course " FOLDED="true" ID="ID_1461953241" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://www.youtube.com/watch?v=YHcAQKrh3E4">
<node TEXT="Feb 16 2019  Classic Machine And Adaptive Machine ll Machine Learning Course Explained in Hindi. 5 Minutes Engineering. Loading Unsubscribe from 5&#xa0;" ID="ID_1547078448" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="Introduction &#x2013; classic and adaptive machines - Machine Learning " FOLDED="true" ID="ID_1156146738" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781789347999/1/ch01lvl1sec10/introduction-classic-and-adaptive-machines">
<node TEXT="Introduction &#x2013; classic and adaptive machinesSince time immemorial human beings have built tools and machines to simplify thei" ID="ID_1328506197" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="Introduction - classic and adaptive machines - Machine Learning " FOLDED="true" ID="ID_265652766" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://www.oreilly.com/library/view/machine-learning-algorithms/9781785889622/3a598806-5896-4de5-9949-8ea3ba4625c4.xhtml">
<node TEXT="Introduction - classic and adaptive machines Since time immemorial human beings have built tools and machines to simplify their work and reduce the overall&#xa0;" ID="ID_1695365418" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="Machine learning - Wikipedia" FOLDED="true" ID="ID_187012359" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://en.wikipedia.org/wiki/Machine_learning">
<node TEXT="Machine learning (ML) is the scientific study of algorithms and statistical models that computer .. Classic examples include principal components analysis and cluster analysis. Feature .. Agriculture &#xb7; Anatomy &#xb7; Adaptive websites &#xb7; Affective computing &#xb7; Bioinformatics &#xb7; Brain&#x2013;machine interfaces &#xb7; Cheminformatics &#xb7; Computer&#xa0;" ID="ID_195610842" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="Adaptive Computation and Machine Learning series | The MIT Press" FOLDED="true" ID="ID_1293918709" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://mitpress.mit.edu/books/series/adaptive-computation-and-machine-learning-series">
<node TEXT="Adaptive Computation and Machine Learning series  classical statistical theory minimum description length theory and statistical mechanics approaches." ID="ID_1972243619" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="Methodology for Adaptive Platform" FOLDED="true" ID="ID_1454474106" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://www.autosar.org/fileadmin/user_upload/standards/adaptive/17-10/AUTOSAR_TR_AdaptiveMethodology.pdf">
<node TEXT="In contrast to the AUTOSAR Classic Platform instances of Adaptive Applica- tions for  In this spirit one real ECU could run several machines even though the." ID="ID_837682621" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="Amazon.com: LectroFan High Fidelity White Noise Machine with 20 " FOLDED="true" ID="ID_1576239097" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://www.amazon.com/LectroFan-Fidelity-Machine-Unique-Non-Looping/dp/B00JU8P8VY">
<node TEXT="Adaptive Sound Technologies Lectrofan Travel Case Black 3.2 Ounce. + .. Marpac Dohm Classic (White) | White noise machine | 101 Night Trial  1 Year&#xa0;" ID="ID_1019452852" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="Adaptive hysteresis controller for the Switched Reluctance Machines " FOLDED="true" ID="ID_1191262747" CREATED="1557224068596" MODIFIED="1557225479614" LINK="http://ieeexplore.ieee.org/document/7077072">
<node TEXT="Abstract: This paper deals with an adaptive hysteresis controller for the Switched Reluctance Machine (SRM). The classical hysteresis controller provides a high&#xa0;" ID="ID_426995449" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="Adaptive machine learning algorithms for data streams subject to " FOLDED="true" ID="ID_922069107" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://tel.archives-ouvertes.fr/tel-01812044v2/document">
<node TEXT="Dec 3 2018  8.3.3.3 Adaptive machine learning for quantitative trading . .. of a machine learning algorithm in this framework is similar to the classical off-." ID="ID_1884171775" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
</node>
</node>
<node TEXT="Machine Learning Matters" ID="ID_1609612442" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
<node TEXT="Beyond Machine Learning" ID="ID_521141213" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Deep Learning" ID="ID_594133549" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Machine Learning Matters#$D$#" FOLDED="true" ID="ID_840326465" CREATED="1557224068596" MODIFIED="1557225483420">
<icon BUILTIN="stop-sign"/>
<node TEXT="Machine Learning Matters" FOLDED="true" ID="ID_1060317631" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://machinelearningmastery.com/machine-learning-matters/">
<node TEXT="Dec 9 2013  It is important to know why machine learning matters so that you know the intrinsic value of the field and of methods and open questions in the&#xa0;" ID="ID_1291491047" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="Machine Learning: What it is and why it matters | SAS" FOLDED="true" ID="ID_507336525" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://www.sas.com/en_us/insights/analytics/machine-learning.html">
<node TEXT="Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can&#xa0;" ID="ID_1320471041" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="A Beginners Guide to AI/ML &#xfffd;&#xfffd;   &#x2013; Machine Learning for Humans " FOLDED="true" ID="ID_474415884" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://medium.com/machine-learning-for-humans/why-machine-learning-matters-6164faf1df12">
<node TEXT="Aug 19 2017  Roadmap. Part 1: Why Machine Learning Matters. The big picture of artificial intelligence and machine learning &#x2014; past present and future." ID="ID_1781982626" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="Deep Reinforcement Learning that Matters" FOLDED="true" ID="ID_893769439" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://arxiv.org/abs/1709.06560">
<node TEXT="Sep 19 2017  Computer Science  Machine Learning  in solving challenging problems across various domains using deep reinforcement learning (RL)." ID="ID_1675883710" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="Machine Learning: What it is and Why it Matters" FOLDED="true" ID="ID_326307266" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://www.simplilearn.com/what-is-machine-learning-and-why-it-matters-article">
<node TEXT="Feb 14 2019  Find out all you need to know about machine learning and what its  how we live and its time we understood what it is and why it matters." ID="ID_541923950" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="Machine Learning that Matters" FOLDED="true" ID="ID_1076813695" CREATED="1557224068596" MODIFIED="1557225479614" LINK="http://www.wkiri.com/research/papers/wagstaff-MLmatters-12.pdf">
<node TEXT="Machine Learning that Matters. Kiri L. Wagstaff kiri.l.wagstaff@jpl.nasa.gov. Jet Propulsion Laboratory California Institute of Technology 4800 Oak Grove Drive&#xa0;" ID="ID_568294967" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="Machine Learning: Why it Matters? - insideBIGDATA" FOLDED="true" ID="ID_1804328578" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://insidebigdata.com/2017/02/18/machine-learning-matters/">
<node TEXT="Feb 18 2017  Are you into Machine Learning OR are you &#x201c;just&#x201d; a Statistician?  grow change and develop by themselves in a matter of seconds &#x2026; and we&#xa0;" ID="ID_1190845648" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="When machine learning matters &#xb7; Erik Bernhardsson" FOLDED="true" ID="ID_1837679035" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://erikbern.com/2016/08/05/when-machine-learning-matters.html">
<node TEXT="Aug 5 2016  I joined Spotify in 2008 to focus on machine learning and music recommendations. Its easy to forget but Spotifys key differentiator back then&#xa0;" ID="ID_1700462431" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="Why Machine Learning Matters" FOLDED="true" ID="ID_871888106" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://bigdata-madesimple.com/why-machine-learning-matters/">
<node TEXT="May 14 2014  While big data vendors offer many unique advantages and benefits to businesses machine learning in particular is an important feature all&#xa0;" ID="ID_416591647" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="Why Deep Learning Matters and Whats Next for AI - Algorithmia" FOLDED="true" ID="ID_278447183" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://blog.algorithmia.com/ai-why-deep-learning-matters/">
<node TEXT="Nov 24 2016  Deep learning matters because its ushering in an era that will fundamentally alter the way we live work and communicate like the industrial&#xa0;" ID="ID_1121730804" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
</node>
<node TEXT="Beyond Machine Learning#$D$#" FOLDED="true" ID="ID_842213902" CREATED="1557224068596" MODIFIED="1557225483421">
<icon BUILTIN="stop-sign"/>
<node TEXT="Beyond Machine Learning: Capturing Cause-and-Effect Relationships" FOLDED="true" ID="ID_1360412578" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://blogs.wsj.com/cio/2019/01/11/beyond-machine-learning-capturing-cause-and-effect-relationships/">
<node TEXT="Jan 11 2019  Companies using machine learning to identify that needle-in-the-haystack bit of business insight could learn from the scientists who succeeded&#xa0;" ID="ID_1006090429" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="Beyond Machine Learning &#x2013; The Future Beyond &#x2013; Medium" FOLDED="true" ID="ID_1837288495" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://medium.com/the-future-beyond/beyond-machine-learning-55000cae5da4">
<node TEXT="Jan 11 2016  We live in dangerous times for us humans. Technology is moving so fast and there is lots of noise an ocean of noise. Although isolated noise&#xa0;" ID="ID_275412806" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="Irving Wladawsky-Berger: Beyond Machine Learning: Capturing " FOLDED="true" ID="ID_42118505" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://blog.irvingwb.com/blog/2019/01/correlation-does-not-imply-causation.html">
<node TEXT="Jan 7 2019  Beyond Machine Learning: Capturing Cause-and-Effect Relationships. Artificial intelligence is rapidly becoming one of the most important&#xa0;" ID="ID_1244189269" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="Beyond Backpropagation: Can We Go Deeper Than Deep Learning?" FOLDED="true" ID="ID_43513031" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://www.topbots.com/deeper-than-deep-learning-beyond-backpropagation-geoffrey-hinton/">
<node TEXT="Nov 9 2017  As the public latches onto AI hype the pioneers behind deep learning question whether it is the right approach to achieve true machine&#xa0;" ID="ID_831026569" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="The Limits of Artificial Intelligence and Deep Learning | WIRED" FOLDED="true" ID="ID_981450959" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://www.wired.com/story/greedy-brittle-opaque-and-shallow-the-downsides-to-deep-learning/">
<node TEXT="Feb 2 2018  But there are many things that people can do quickly that smart machines cannot. Natural language is beyond deep learning; new situations&#xa0;" ID="ID_1549487870" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="Beyond Machine Learning and AI 10x Faster Deployments and Indico" FOLDED="true" ID="ID_227284067" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://blog.outsellinc.com/beyond-machine-learning-and-ai-10x-faster-deployments-and-indico-4f412fd87b22">
<node TEXT="Jul 3 2018  If youve listened to this show it is no secret that Ive covered a large number of AI and Machine Learning companies. I promise you this one is&#xa0;" ID="ID_1290010836" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="Council Post: Artificial Intelligence Beyond Deep Neural Networks" FOLDED="true" ID="ID_182399669" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://www.forbes.com/sites/forbestechcouncil/2019/03/26/artificial-intelligence-beyond-deep-neural-networks/">
<node TEXT="Mar 26 2019  Several AI techniques could potentially address the shortcomings of deep neural networks." ID="ID_1851190638" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="2018 Trends in Artificial Intelligence: Beyond Machine Learning for " FOLDED="true" ID="ID_581816377" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://analyticsweek.com/content/2018-trends-artificial-intelligence-beyond-machine-learning-internal-external-personalization/">
<node TEXT="Nov 13 2017  The collective form of Artificial Intelligence is evolving. Granted its still a fundamental aspect of big data analytics and numerous cloud&#xa0;" ID="ID_1764621746" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="Artificial intelligence beyond Machine Learning Daniel Gillblad SICS" FOLDED="true" ID="ID_1258018993" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://www.youtube.com/watch?v=vyRi9MTgUQU">
<node TEXT="May 18 2017  Daniel Gillblad PhD Director Decisions Networks and Analytics Laboratory RISE SICS AB .Artificial intelligence beyond Machine Learning&#xa0;" ID="ID_1803447487" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="Home &#x2014; Beyond Machine" FOLDED="true" ID="ID_1149979356" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://beyond-machine.com/">
<node TEXT="MIE (rebranded to Beyond Machine) was spawned from Lele and Irene  for a more developed community an outlet for media around Machine Intelligence." ID="ID_1756778842" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
</node>
<node TEXT="Statistical Learning Approaches#$D$#" FOLDED="true" ID="ID_234154688" CREATED="1557224068597" MODIFIED="1557225483423">
<icon BUILTIN="stop-sign"/>
<node TEXT="Statistical learning theory - Wikipedia" FOLDED="true" ID="ID_1024384588" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://en.wikipedia.org/wiki/Statistical_learning_theory">
<node TEXT="Statistical learning theory is a framework for machine learning drawing from the fields of statistics and functional analysis. Statistical learning theory deals with&#xa0;" ID="ID_1479279676" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Statistical learning approaches for discriminant features selection " FOLDED="true" ID="ID_406792989" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://link.springer.com/article/10.1007/BF03192556">
<node TEXT="Supervised statistical learning covers important models like Support Vector Machines (SVM) and Linear Discriminant Analysis (LDA). In this paper we describe&#xa0;" ID="ID_676926863" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="A statistical learning approach for estimating the reliability of crash " FOLDED="true" ID="ID_198130704" CREATED="1557224068597" MODIFIED="1557225479630" LINK="https://ieeexplore.ieee.org/document/7795911/">
<node TEXT="A statistical learning approach for estimating the reliability of crash severity  In this work a machine learning driven reliability estimator for crash severity&#xa0;" ID="ID_1063016512" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="A Computational Approach to Statistical Learning - CRC Press Book" FOLDED="true" ID="ID_397841714" CREATED="1557224068597" MODIFIED="1557225479630" LINK="https://www.crcpress.com/A-Computational-Approach-to-Statistical-Learning/Arnold-Kane-Lewis/p/book/9781138046375">
<node TEXT="A Computational Approach to Statistical Learning gives a novel introduction to predictive modeling by focusing on the algorithmic and numeric motivations&#xa0;" ID="ID_816001472" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="What Is The Difference Between Machine Learning  Statistical " FOLDED="true" ID="ID_1199056395" CREATED="1557224068597" MODIFIED="1557225479630" LINK="https://www.analyticsvidhya.com/blog/2015/07/difference-machine-learning-statistical-modeling/">
<node TEXT="Jul 1 2015  Both these approaches aim to learn about the underlying phenomena by  Differences between Machine Learning and Statistical Modeling:." ID="ID_1009986423" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="A Statistical Learning Approach to Modal Regression" FOLDED="true" ID="ID_1181363256" CREATED="1557224068597" MODIFIED="1557225479630" LINK="https://arxiv.org/abs/1702.05960">
<node TEXT="Feb 20 2017  Statistics  Machine Learning  This paper studies the nonparametric modal regression problem systematically from a statistical learning view." ID="ID_209586597" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="A Supervised Statistical Learning Approach for Accurate Legionella " FOLDED="true" ID="ID_1514101611" CREATED="1557224068597" MODIFIED="1557225479630" LINK="https://www.ncbi.nlm.nih.gov/pubmed/28821546">
<node TEXT="Nov 1 2017  A Supervised Statistical Learning Approach for Accurate Legionella pneumophila Source Attribution during Outbreaks. Buultjens AH(1)(2)&#xa0;" ID="ID_945589819" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Statistical learning approaches for discriminant features selection" FOLDED="true" ID="ID_1608894893" CREATED="1557224068597" MODIFIED="1557225479630" LINK="http://www.scielo.br/scielo.php?script=sci_arttextpid=S0104-65002008000200002">
<node TEXT="ARTICLES. Statistical learning approaches for discriminant features selection. Gilson A. GiraldiI; Paulo S. RodriguesIV; Edson C. KitaniII; Jo&#xe3;o R. SatoIII; Carlos&#xa0;" ID="ID_206348082" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Implicit learning and statistical learning: one phenomenon two " FOLDED="true" ID="ID_1537324752" CREATED="1557224068597" MODIFIED="1557225479630" LINK="https://www.ncbi.nlm.nih.gov/pubmed/16616590">
<node TEXT="Trends Cogn Sci. 2006 May;10(5):233-8. Epub 2006 Apr 17. Implicit learning and statistical learning: one phenomenon two approaches. Perruchet P(1) Pacton&#xa0;" ID="ID_2546428" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Statistical Learning for Data Science &#x2013; Towards Data Science" FOLDED="true" ID="ID_364379631" CREATED="1557224068597" MODIFIED="1557225479630" LINK="https://towardsdatascience.com/statistical-learning-for-data-science-b61b263c1196">
<node TEXT="Aug 19 2018  This is the 6th  last post of blog post series Probability  Statistics for Data Science this post covers these topics related to Statistical&#xa0;" ID="ID_692846765" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
</node>
</node>
<node TEXT="Bio Inspired adaptive Systems" ID="ID_198396965" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Bio Inspired adaptive Systems#$D$#" FOLDED="true" ID="ID_1955840330" CREATED="1557224068597" MODIFIED="1557225483422">
<icon BUILTIN="stop-sign"/>
<node TEXT="Bio-inspired computing - Wikipedia" FOLDED="true" ID="ID_1202316181" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://en.wikipedia.org/wiki/Bio-inspired_computing">
<node TEXT="Bio-inspired computing short for biologically inspired computing is a field of study that loosely . The most basic computer system such as storage and computational fusion pulse discharge mechanism the . SymbioticSphere: A Biologically-inspired Architecture for Scalable Adaptive and Survivable Network Systems&#xa0;" ID="ID_1416254507" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Beyond machine learning - deep learning and bio-inspired adaptive " FOLDED="true" ID="ID_1463692288" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781785889622/1/ch01lvl1sec10/beyond-machine-learning-deep-learning-and-bio-inspired-adaptive-systems">
<node TEXT="Introduction - classic and adaptive machines &#xb7; Only learning matters &#xb7; Beyond machine learning - deep learning and bio-inspired adaptive systems &#xb7; Machine&#xa0;" ID="ID_3670067" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Bio-inspired Adaptive Architectures and Systems" FOLDED="true" ID="ID_426920229" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://gow.epsrc.ukri.org/NGBOViewGrant.aspx?GrantRef=EP/K040820/1">
<node TEXT="EPSRC Reference: EP/K040820/1. Title: Bio-inspired Adaptive Architectures and Systems. Principal Investigator: Tyrrell Professor A. Other Investigators:&#xa0;" ID="ID_375958416" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Bio-inspired Approaches for Engineering Adaptive Systems " FOLDED="true" ID="ID_1432907275" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://www.sciencedirect.com/science/article/pii/S1877050914007030">
<node TEXT="Adaptive systems are composed of different heterogeneous parts or entities that interact and perform actions favouring the emer- gence of global desired&#xa0;" ID="ID_601737677" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Beyond machine learning - deep learning and bio-inspired adaptive " FOLDED="true" ID="ID_1996387092" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://www.oreilly.com/library/view/machine-learning-algorithms/9781785889622/ca8c45fe-2c94-476c-bbbc-6244d27df5d6.xhtml">
<node TEXT="Beyond machine learning - deep learning and bio-inspired adaptive systems During the last few years thanks to more powerful and cheaper computers many&#xa0;" ID="ID_1290722091" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="(&#xfffd;&#xfffd;&#xfffd;&#xfffd;&#xfffd;&#xfffd;) Bio-inspired Approaches for Engineering Adaptive Systems" FOLDED="true" ID="ID_1283269728" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://www.researchgate.net/publication/270979969_Bio-inspired_Approaches_for_Engineering_Adaptive_Systems">
<node TEXT="Oct 21 2015  &#xfffd;&#xfffd;&#xfffd;&#xfffd;&#xfffd;&#xfffd; | Adaptive systems are composed of different heterogeneous parts or entities that interact and perform actions favouring the emer- gence&#xa0;" ID="ID_232381067" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Bio-inspired Adaptive Morphology (BAM) Laboratory" FOLDED="true" ID="ID_1759200275" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://bamlab.mechse.illinois.edu/">
<node TEXT="Bio-inspired is defined as inspired by or based on biological structures or processes  It is a biological system that continuously changes and adapts its form in&#xa0;" ID="ID_537937364" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Bio-inspired underwater electrolocation through adaptive system " FOLDED="true" ID="ID_1481738318" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://ieeexplore.ieee.org/document/7172033/">
<node TEXT="Bio-inspired underwater electrolocation through adaptive system identification. Abstract: Electrolocation is a method of sensing and navigating around nearby&#xa0;" ID="ID_376721707" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Bio-inspired Approaches for Engineering Adaptive Systems" FOLDED="true" ID="ID_849852863" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://cyberleninka.org/article/n/195609.pdf">
<node TEXT="related to the development of adaptive systems and approaches and shed light on  Keywords: Natural and biological systems Adaptive systems Bio-inspired&#xa0;" ID="ID_1469876581" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Exploring Natural Strategies for Bio-Inspired Fault Adaptive Systems " FOLDED="true" ID="ID_1366947672" CREATED="1557224068597" MODIFIED="1557225479614" LINK="http://mechanicaldesign.asmedigitalcollection.asme.org/article.aspx?articleid=2682441">
<node TEXT="Fault adaptive design seeks to find the principles and properties that enable robustness reliability and resilience to implement those features into engineering&#xa0;" ID="ID_1436688230" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
</node>
</node>
</node>
<node TEXT="Machine Learning and Big Data" ID="ID_149028068" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Machine Learning and Big Data#$D$#" FOLDED="true" ID="ID_1840329845" CREATED="1557224068597" MODIFIED="1557225483422">
<icon BUILTIN="stop-sign"/>
<node TEXT="Machine Learning with Big Data &#x2013; Towards Data Science" FOLDED="true" ID="ID_634602372" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://towardsdatascience.com/machine-learning-with-big-data-86bcb39f2f0b">
<node TEXT="Mar 11 2019  Storing this data is one thing but what about processing it and developing machine learning algorithms to work with it? In this article we will&#xa0;" ID="ID_1769064803" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Machine Learning With Big Data | Coursera" FOLDED="true" ID="ID_673666224" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://www.coursera.org/learn/big-data-machine-learning">
<node TEXT="Learn Machine Learning With Big Data from University of California San Diego. Want to make sense of the volumes of data you have collected? Need to&#xa0;" ID="ID_144864898" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="5 Best Difference Between Big Data Vs Machine Learning" FOLDED="true" ID="ID_1371707256" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://www.educba.com/big-data-vs-machine-learning/">
<node TEXT="Feb 27 2018  In this Article Big Data vs Machine Learning we will look at their Meaning Head to Head Comparison and Key Differences in relatively easy&#xa0;" ID="ID_878145644" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="How are big data and machine learning related? - Quora" FOLDED="true" ID="ID_535765461" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://www.quora.com/How-are-big-data-and-machine-learning-related">
<node TEXT="Big data machine learning statistics statistical machine learning; so many terms surfacing. What are these and how are they related is a question the answer to&#xa0;" ID="ID_1167907522" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Machine Learning: What it is and why it matters | SAS" FOLDED="true" ID="ID_1936036775" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://www.sas.com/en_us/insights/analytics/machine-learning.html">
<node TEXT="While many machine learning algorithms have been around for a long time the ability to automatically apply complex mathematical calculations to big data&#xa0;" ID="ID_1432181083" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Machine Learning Applied to Big Data Explained" FOLDED="true" ID="ID_1851314956" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://www.kdnuggets.com/2017/07/machine-learning-big-data-explained.html">
<node TEXT="Machine learning with Big Data is in many ways different than regular machine learning. This informative image is helpful in identifying the steps in machine&#xa0;" ID="ID_944308391" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Council Post: Why Big Data And Machine Learning Are Important In " FOLDED="true" ID="ID_1506977649" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://www.forbes.com/sites/forbestechcouncil/2019/01/07/why-big-data-and-machine-learning-are-important-in-our-society/">
<node TEXT="Jan 7 2019  The singularity is near or maybe were already in it. Whatever the case is machine learning and big data will have a tremendous influence on&#xa0;" ID="ID_501963685" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="How Big Data Is Empowering AI and Machine Learning at Scale" FOLDED="true" ID="ID_1787557398" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://sloanreview.mit.edu/article/how-big-data-is-empowering-ai-and-machine-learning-at-scale/">
<node TEXT="May 8 2017  Big data is moving to a new stage of maturity &#x2014; one that promises even greater business impact and industry disruption over the course of the&#xa0;" ID="ID_429865966" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Machine Learning for Big Data Analytics - Expert System" FOLDED="true" ID="ID_143190919" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://www.expertsystem.com/machine-learning-big-data-analytics/">
<node TEXT="Lots of data equals lots of examples for the system equals good results. But is that really so? Learn more about machine learning for big data analytics." ID="ID_971241542" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Big Data and Machine Learning - YouTube" FOLDED="true" ID="ID_168290409" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://www.youtube.com/watch?v=b3jizIvpa20">
<node TEXT="Jun 21 2016  Chair: Steven Drucker Microsoft Research Speakers: Mike Zyskowski Microsoft Research Lihong Li Microsoft Research Jonathan Huang&#xa0;" ID="ID_1882919928" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
</node>
<node TEXT="Data formats#$D$#" FOLDED="true" ID="ID_189575949" CREATED="1557224068597" MODIFIED="1557225483423">
<icon BUILTIN="stop-sign"/>
<node TEXT="Data Types  File Formats | University of Virginia Library Research " FOLDED="true" ID="ID_667495620" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://data.library.virginia.edu/data-management/plan/format-types/">
<node TEXT="Data Types  File Formats. What types of data are we talking about? Data can mean many different things and there are many ways to classify it. Two of the&#xa0;" ID="ID_572489508" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Data format - Wikipedia" FOLDED="true" ID="ID_965147042" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://en.wikipedia.org/wiki/Data_format">
<node TEXT="Data format in information technology may refer to: Data type constraint placed upon the interpretation of data in a type system; Signal (electrical engineering)&#xa0;" ID="ID_711396775" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Apache Camel: Data Format" FOLDED="true" ID="ID_15954318" CREATED="1557224068597" MODIFIED="1557225479614" LINK="http://camel.apache.org/data-format.html">
<node TEXT="Data Format. Camel supports a pluggable DataFormat to allow messages to be marshalled to and from binary or text formats to support a kind of Message&#xa0;" ID="ID_688090301" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="File Formats" FOLDED="true" ID="ID_1226331746" CREATED="1557224068597" MODIFIED="1557225479614" LINK="http://opendatahandbook.org/guide/en/appendices/file-formats/">
<node TEXT="XML is a widely used format for data exchange because it gives good opportunities to keep the structure in the data and the way files are built on and allows&#xa0;" ID="ID_392455100" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="DataFormats Class (System.Windows) | Microsoft Docs" FOLDED="true" ID="ID_667309377" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://docs.microsoft.com/en-us/dotnet/api/system.windows.dataformats">
<node TEXT="The DataObject class and other classes that implement the IDataObject interface use the static formats defined by DataFormats to describe each data format that&#xa0;" ID="ID_1698463998" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Data Formats" FOLDED="true" ID="ID_1006940470" CREATED="1557224068597" MODIFIED="1557225479614" LINK="http://www.svds.com/dataformats/">
<node TEXT="There are several data formats to choose from to load your data into the Hadoop Distributed File System (HDFS). Each of the data formats has its own strengths&#xa0;" ID="ID_1318907662" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="DataFormats Class (System.Windows.Forms) | Microsoft Docs" FOLDED="true" ID="ID_759678416" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://docs.microsoft.com/en-us/dotnet/api/system.windows.forms.dataformats">
<node TEXT="DataFormats.Format myFormat = DataFormats.GetFormat(myFormat); /* Creates a new object and stores it in a DataObject using myFormat * as the type of&#xa0;" ID="ID_65864721" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Chapter 3: Data Formats - An Introduction to APIs | Zapier" FOLDED="true" ID="ID_1389381286" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://zapier.com/learn/apis/chapter-3-data-formats/">
<node TEXT="Apr 22 2014  Read or Download Chapter 3: Data Formats from our An Introduction to APIs e-book for FREE and start learning today!" ID="ID_1512158902" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Data Types  File Formats - Research Data Services - LibGuides at " FOLDED="true" ID="ID_360546165" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://guides.library.oregonstate.edu/research-data-services/data-management-types-formats">
<node TEXT="Apr 15 2019  The ETDplus project has published a File Formats guidance brief. It is a short how to document written for a student audience designed to&#xa0;" ID="ID_527176910" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Data Types and Data Formats" FOLDED="true" ID="ID_1576621521" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://www.ibm.com/support/knowledgecenter/en/ssw_ibm_i_72/rzasd/dtdf.htm">
<node TEXT="This chapter describes the data types supported by RPG IV and their special characteristics. The supported data types are: Character Format &#xb7; Numeric Data&#xa0;" ID="ID_1834559160" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
</node>
</node>
<node TEXT="Elements of Machine Learning" ID="ID_458794552" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Data formats" ID="ID_1121640281" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Elements of Machine Learning#$D$#" FOLDED="true" ID="ID_728531281" CREATED="1557224068597" MODIFIED="1557225483422">
<icon BUILTIN="stop-sign"/>
<node TEXT="The Elements of Statistical Learning" FOLDED="true" ID="ID_804965596" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://web.stanford.edu/~hastie/Papers/ESLII.pdf">
<node TEXT="The Elements of Statistical Learning  Support Vector Machines and. Flexible  &#x201c;Kernel Smoothing Methods&#x201d; to avoid confusion with the machine- learning&#xa0;" ID="ID_1604458616" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Amazon.com: The Elements of Statistical Learning: Data Mining " FOLDED="true" ID="ID_1619162787" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://www.amazon.com/Elements-Statistical-Learning-Prediction-Statistics/dp/0387848576">
<node TEXT="Amazon.com: The Elements of Statistical Learning: Data Mining Inference and  Deep Learning (Adaptive Computation and Machine Learning series)." ID="ID_1520599102" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Elements of Statistical Learning: data mining inference and " FOLDED="true" ID="ID_1767055907" CREATED="1557224068597" MODIFIED="1557225479614" LINK="http://web.stanford.edu/~hastie/ElemStatLearn/">
<node TEXT="The Elements of. Statistical Learning: Data Mining Inference and Prediction. Second Edition. February 2009. Trevor Hastie &#xb7; Robert Tibshirani." ID="ID_973195575" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Machine Learning And The 5 Key Elements Of A Layered Defense " FOLDED="true" ID="ID_62394165" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://www.itspmagazine.com/from-the-newsroom/machine-learning-and-the-5-key-elements-of-a-layered-defense">
<node TEXT="Mar 6 2018  While machine learning an application of artificial intelligence is not new customers still struggle to understand how it will benefit their efforts&#xa0;" ID="ID_1146099455" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Elements of machine learning - Data Science Central" FOLDED="true" ID="ID_114090055" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://www.datasciencecentral.com/profiles/blogs/elements-of-machine-learning">
<node TEXT="Sep 29 2014  The official title of this free book available in PDF format is Machine Learning Cheat Sheet. But its more about elements of machine learning&#xa0;" ID="ID_1050443966" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Basic Concepts in Machine Learning" FOLDED="true" ID="ID_834170350" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://machinelearningmastery.com/basic-concepts-in-machine-learning/">
<node TEXT="Dec 25 2015  Key Elements of Machine Learning. There are tens of thousands of machine learning algorithms and hundreds of new algorithms are&#xa0;" ID="ID_1401569311" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Classifying superheavy elements by machine learning" FOLDED="true" ID="ID_689590006" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://link.aps.org/doi/10.1103/PhysRevA.99.022110">
<node TEXT="Feb 8 2019  By using cutting-edge machine learning techniques we find the relationship between atomic data and classification of elements and further&#xa0;" ID="ID_1695258306" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Introduction to Statistical Learning" FOLDED="true" ID="ID_1130946393" CREATED="1557224068597" MODIFIED="1557225479614" LINK="http://www-bcf.usc.edu/~gareth/ISL/">
<node TEXT="Inspired by The Elements of Statistical Learning (Hastie Tibshirani and  As a textbook for an introduction to data science through machine learning there is&#xa0;" ID="ID_676409397" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Machine Learning | Coding Elements" FOLDED="true" ID="ID_1133618103" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://www.codingelements.com/course/ml">
<node TEXT="How does this transform my career? Machine Learning is one of the hottest fields for elite tech jobs. Almost all large companies have a data science team." ID="ID_1952600728" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Element AI: Global AI Software Provider" FOLDED="true" ID="ID_1107869411" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://www.elementai.com/">
<node TEXT="Element AI delivers products designed to help people work smarter and make businesses stronger safer and more agile. AI helps build and execute stronger&#xa0;" ID="ID_1651609765" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
</node>
</node>
<node TEXT="Learnability" ID="ID_384202286" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
</node>
<node TEXT="Elements of Information Theory" ID="ID_524155393" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Elements of Information Theory#$D$#" FOLDED="true" ID="ID_692785531" CREATED="1557224068597" MODIFIED="1557225483423">
<icon BUILTIN="stop-sign"/>
<node TEXT="Elements of Information Theory 2nd Edition (Wiley Series in " FOLDED="true" ID="ID_1905544750" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://www.amazon.com/Elements-Information-Theory-Telecommunications-Processing/dp/0471241954">
<node TEXT="Elements of Information Theory 2nd Edition (Wiley Series in Telecommunications and Signal Processing) [Thomas M. Cover Joy A. Thomas] on Amazon.com." ID="ID_1713515550" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Elements of Information Theory" FOLDED="true" ID="ID_1733296795" CREATED="1557224068597" MODIFIED="1557225479614" LINK="http://www.cs-114.org/wp-content/uploads/2015/01/Elements_of_Information_Theory_Elements.pdf">
<node TEXT="John Bellamy. Elements of Information Theory. Thomas M. Cover and Joy A. Thomas. Telecommunication System Engineering 2nd Edition. Roger L. Freeman." ID="ID_1656413339" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Elements of Information Theory (Wiley Series in " FOLDED="true" ID="ID_1941697406" CREATED="1557224068597" MODIFIED="1557225479614" LINK="http://staff.ustc.edu.cn/~cgong821/Wiley.Interscience.Elements.of.Information.Theory.Jul.2006.eBook-DDU.pdf">
<node TEXT="Elements of information theory/by Thomas M. Cover Joy A. Thomas.&#x2013;2nd ed. p. cm. &#x201c;A Wiley-Interscience publication.&#x201d; Includes bibliographical references and&#xa0;" ID="ID_1651927140" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Elements of Information Theory 2nd Edition | Information " FOLDED="true" ID="ID_753907755" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://www.wiley.com/en-us/Elements+of+Information+Theory%2C+2nd+Edition-p-9780471241959">
<node TEXT="All the essential topics in information theory are covered in detail including entropy data compression channel capacity rate distortion network information&#xa0;" ID="ID_403674018" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Elements of Information Theory (Wiley Series in " FOLDED="true" ID="ID_1323028843" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://dl.acm.org/citation.cfm?id=1146355">
<node TEXT="Sep 29 2017  Yoichiro Watanabe  Koichi Kamoi A formulation of the channel capacity of multiple-access channel IEEE Transactions on Information Theory&#xa0;" ID="ID_381375627" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Elements Of Information Theory 2nd Ed : Thomas M. Cover Joy A " FOLDED="true" ID="ID_1481775299" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://archive.org/details/ElementsOfInformationTheory2ndEd">
<node TEXT="Mar 25 2017  Elements Of Information Theory 2nd EdWiley 2006 Thomas M. CoverJoy A. ThomasISBN-13 978-0-471-24195-9ISBN-10 0-471-24195-4." ID="ID_1380676541" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Buy Elements of Information Theory 2ed (WILEY-Interscience) Book " FOLDED="true" ID="ID_163366019" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://www.amazon.in/Elements-Information-Theory-2ed-WILEY-Interscience/dp/8126541946">
<node TEXT="Read Elements of Information Theory 2ed (WILEY-Interscience) book reviews  author details and more at Amazon.in. Free delivery on qualified orders." ID="ID_591385523" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Elements of Information Theory by Thomas M. Cover" FOLDED="true" ID="ID_942634522" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://www.goodreads.com/book/show/433439.Elements_of_Information_Theory">
<node TEXT="Elements of Information Theory book. Read 9 reviews from the worlds largest community for readers. The latest edition of this classic is updated with ne" ID="ID_1033174983" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Elements of Information Theory | Wiley Online Books" FOLDED="true" ID="ID_1658740575" CREATED="1557224068597" MODIFIED="1557225479614" LINK="https://onlinelibrary.wiley.com/doi/book/10.1002/047174882X">
<node TEXT="Apr 7 2005  Share. Email; Facebook; Twitter; Linked In; Reddit; CiteULike. View Table of Contents for Elements of Information Theory&#xa0;" ID="ID_855531279" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
<node TEXT="Elements of Information Theory Second Edition 2006" FOLDED="true" ID="ID_302382903" CREATED="1557224068597" MODIFIED="1557225479614" LINK="http://www.elementsofinformationtheory.com/">
<node TEXT="Elements of Information Theory 2nd Edition Thomas M. Cover Joy A. Thomas ISBN: 0-471-24195-4. Hardcover 776 pages. July 2006&#xa0;" ID="ID_1746633638" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
</node>
</node>
<node TEXT="Statistical Learning Approaches" ID="ID_402321144" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
<node TEXT="Research Paper Map " ID="ID_1426825221" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
<node TEXT="Resources" ID="ID_1641750420" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Podcasts" ID="ID_1273389358" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Linear Disgressions " ID="ID_1400940697" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
</node>
</node>
</node>
<node TEXT="Feature Selection" POSITION="right" ID="ID_1769910463" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<edge COLOR="#00cc33"/>
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Scikit-learn" FOLDED="true" ID="ID_714755836" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Dataset" ID="ID_617902190" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
<node TEXT="Creating training and test sets" ID="ID_1170775304" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Creating training and test sets#$D$#" FOLDED="true" ID="ID_1404356426" CREATED="1557224068598" MODIFIED="1557225483434">
<icon BUILTIN="stop-sign"/>
<node TEXT="Tutorial to prepare train and test set using dataPreparation" FOLDED="true" ID="ID_190070146" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://cran.r-project.org/web/packages/dataPreparation/vignettes/train_test_prep.html">
<node TEXT="Mar 25 2019  This vignette is a tutorial to prepare a train and a test set using . Even if its not kept in the log a progress bar has been created to see if the&#xa0;" ID="ID_1863659892" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
<node TEXT="About Train Validation and Test Sets in Machine Learning" FOLDED="true" ID="ID_1118881845" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7">
<node TEXT="Dec 6 2017  About Train Validation and Test Sets in Machine Learning  Training Dataset: The sample of data used to fit the model. . Generally when you design or create a new model it helps to keep a separate test and validation set." ID="ID_1987167801" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
<node TEXT="How to split data into training/testing sets using sample function " FOLDED="true" ID="ID_1479070369" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://stackoverflow.com/questions/17200114/how-to-split-data-into-training-testing-sets-using-sample-function">
<node TEXT="bound - floor((nrow(df)/4)*3) #define % of training and test set df  is a good idea anyway not only for creating sets but also for traceability during your project." ID="ID_1510274477" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
<node TEXT="Training and test dataset creation with dplyr &#x2013; Holly Emblem &#x2013; Medium" FOLDED="true" ID="ID_727329082" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://medium.com/@HollyEmblem/training-and-test-dataset-creation-with-dplyr-41d9aa7eab31">
<node TEXT="Jul 7 2018  If youre prototyping models for machine learning then chances are you might need to create training and test sets from your existing dataset." ID="ID_1081723083" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
<node TEXT="Weka Tutorial 35: Creating Training Validation and Test Sets (Data " FOLDED="true" ID="ID_992806441" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://www.youtube.com/watch?v=uiDFa7iY9yo">
<node TEXT="Jan 20 2014  The tutorial that demonstrates how to create training test and cross validation sets from a given dataset." ID="ID_1891443776" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
<node TEXT="Machine Learning Tutorial Python - 7: Training and Testing Data " FOLDED="true" ID="ID_493579100" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://www.youtube.com/watch?v=fwY9Qv96DJY">
<node TEXT="Aug 9 2018  This way you can train and test on seperate datasets.  Weka Tutorial 35: Creating Training Validation and Test Sets (Data Preprocessing)&#xa0;" ID="ID_89107612" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
<node TEXT="Training and Testing Data Sets | Microsoft Docs" FOLDED="true" ID="ID_1626538934" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://docs.microsoft.com/en-us/sql/analysis-services/data-mining/training-and-testing-data-sets">
<node TEXT="The information about the size of the training and testing data sets and which&#xa0;" ID="ID_227810775" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
<node TEXT="Training and Test Sets: Splitting Data | Machine Learning Crash " FOLDED="true" ID="ID_1499810089" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://developers.google.com/machine-learning/crash-course/training-and-test-sets/splitting-data">
<node TEXT="Mar 5 2019  Assuming that your test set meets the preceding two conditions your goal is to create a model that generalizes well to new data. Our test set&#xa0;" ID="ID_1740433812" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
<node TEXT="How (and why) to create a good validation set &#xb7; fast.ai" FOLDED="true" ID="ID_1276287190" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://www.fast.ai/2017/11/13/validation-sets/">
<node TEXT="Nov 13 2017  Kaggle only provides training and test sets yet to do well you will need to split their training set into your own validation and training sets. Also&#xa0;" ID="ID_1561247041" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
</node>
</node>
<node TEXT="Managing Categorical data" ID="ID_244943393" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Managing Categorical data#$D$#" FOLDED="true" ID="ID_1903157067" CREATED="1557224068598" MODIFIED="1557225483434">
<icon BUILTIN="stop-sign"/>
<node TEXT="Handling Categorical Data in Python (article) - DataCamp" FOLDED="true" ID="ID_189976170" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://www.datacamp.com/community/tutorials/categorical-data">
<node TEXT="May 22 2018  Learn the common tricks to handle categorical data and preprocess it to build machine learning models!" ID="ID_775577568" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
<node TEXT="Categorical Data &#x2013; Towards Data Science" FOLDED="true" ID="ID_1482491737" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://towardsdatascience.com/understanding-feature-engineering-part-2-categorical-data-f54324193e63">
<node TEXT="Jan 6 2018  In this article we will look at another type of structured data which is discrete in nature and is popularly termed as categorical data. Dealing&#xa0;" ID="ID_466826450" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
<node TEXT="Simple Methods to deal with Categorical Variables in Predictive " FOLDED="true" ID="ID_1077832399" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://www.analyticsvidhya.com/blog/2015/11/easy-methods-deal-categorical-variables-predictive-modeling/">
<node TEXT="Nov 26 2015  Here are simple methods to treat categorical variables in a data set and their various levels using label encoding dummy one hot encoding." ID="ID_1460111830" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
<node TEXT="Handling Categorical Data in Machine Learning Models | Pluralsight" FOLDED="true" ID="ID_873759842" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://www.pluralsight.com/guides/handling-categorical-data-in-machine-learning-models">
<node TEXT="Feb 20 2019  Categorical Data is the data that generally takes a limited number of possible values. Also the data in the category need not be numerical&#xa0;" ID="ID_1280659462" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
<node TEXT="Managing categorical data - Machine Learning Algorithms" FOLDED="true" ID="ID_1148303835" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781785889622/3/ch03lvl1sec22/managing-categorical-data">
<node TEXT="Managing categorical dataIn many classification problems the target dataset is made up of categorical labels which cannot imm" ID="ID_1739506816" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
<node TEXT="Handling Categorical Data | Real Statistics Using Excel" FOLDED="true" ID="ID_170317647" CREATED="1557224068598" MODIFIED="1557225479630" LINK="http://www.real-statistics.com/logistic-regression/handling-categorical-data/">
<node TEXT="Describes how to code categorical data in Excel especially for logistic regression." ID="ID_335509094" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
<node TEXT="Handling Categorical Features in Machine Learning | Talentica Blog" FOLDED="true" ID="ID_1512369957" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://www.talentica.com/blogs/handling-categorical-features-in-machine-learning/">
<node TEXT="Mar 21 2017  Introduction: Every dataset has two type of variables Continuous(Numerical) and Categorical. Regression based algorithms use continuous&#xa0;" ID="ID_889216886" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
<node TEXT="Using categorical data in machine learning with python: from dummy " FOLDED="true" ID="ID_1534170159" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://blog.myyellowroad.com/using-categorical-data-in-machine-learning-with-python-from-dummy-variables-to-deep-category-66041f734512">
<node TEXT="Sep 19 2017  Categorical data is very common in business datasets for example users are typically described by country gender age group etc. products&#xa0;" ID="ID_1270236948" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
<node TEXT="Data management: How to convert categorical string variables to " FOLDED="true" ID="ID_831665477" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://www.youtube.com/watch?v=ZRWHjdIZyxo">
<node TEXT="Dec 1 2016  This video demonstrates how to convert categorical string variables to labeled numeric variables. Copyright 2011-2017 StataCorp LLC." ID="ID_388623330" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
<node TEXT="Handling categorical data with sklearn. | Kaggle" FOLDED="true" ID="ID_1987974829" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://www.kaggle.com/c/titanic/discussion/5379">
<node TEXT="I use sklearn(scikit-learn) package in python. In sklearn I cannot directly put categorical column Sex which has string like male and female. So I have to&#xa0;" ID="ID_935184886" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
</node>
</node>
<node TEXT="Managing Missing features" ID="ID_1823632424" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Managing Missing features#$D$#" FOLDED="true" ID="ID_752729996" CREATED="1557224068598" MODIFIED="1557225483434">
<icon BUILTIN="stop-sign"/>
<node TEXT="Managing missing features - Machine Learning Algorithms" FOLDED="true" ID="ID_1771177161" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781785889622/3/ch03lvl1sec23/managing-missing-features">
<node TEXT="Managing missing featuresSometimes a dataset can contain missing features so there are a few options that can be taken into a" ID="ID_1853918770" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
<node TEXT="How to Handle Missing Data &#x2013; Towards Data Science" FOLDED="true" ID="ID_33557853" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4">
<node TEXT="Jan 30 2018   faced in Data Cleaning/Exploratory Analysis is handling the missing values.  rows which have a feature to fill in each rows missing features" ID="ID_1196115677" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
<node TEXT="5 Ways To Handle Missing Values In Machine Learning Datasets" FOLDED="true" ID="ID_719361307" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://www.analyticsindiamag.com/5-ways-handle-missing-values-machine-learning-datasets/">
<node TEXT="Feb 9 2018  Handling the missing values is one of the greatest challenges faced by  This strategy can be applied on a feature which has numeric data like&#xa0;" ID="ID_878059704" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
<node TEXT="The Difficulty of &#x201c;Feature Parity&#x201d; &#x2013; All Things Product Management " FOLDED="true" ID="ID_373746330" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://medium.com/all-things-product-management/the-difficulty-of-feature-parity-9dafa4bc0b16">
<node TEXT="Feb 12 2017  &#x201c;What happens if we get bad PR about missing feature X?&#x201d; or leaderships requirements  Managing Feature Parity During a Product Rewrite." ID="ID_902294618" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
<node TEXT="Managing missing features - Machine Learning Algorithms - Second " FOLDED="true" ID="ID_1930029077" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://www.oreilly.com/library/view/machine-learning-algorithms/9781789347999/72fd6808-8b76-4c9c-91ff-7f13502e752f.xhtml">
<node TEXT="Managing missing features Sometimes a dataset can contain missing features so there are a few options that can be taken into account: Removing the whole&#xa0;" ID="ID_671417330" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
<node TEXT="Working with Fonts - Figma" FOLDED="true" ID="ID_1766849122" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://help.figma.com/article/250-working-with-fonts">
<node TEXT="In this article well cover: Applying Fonts; Using Local Fonts; Using Icon Fonts; Using OpenType Features; Managing Missing Fonts; Writing in Other Languages&#xa0;" ID="ID_388018490" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
<node TEXT="Managing Missing Features Explained with Examples in Hindi ll " FOLDED="true" ID="ID_274622915" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://www.youtube.com/watch?v=9agiwBvCpHA">
<node TEXT="Mar 9 2019  Managing Missing Features Explained with Examples in Hindi ll Machine Learning Course. 5 Minutes Engineering. Loading Unsubscribe&#xa0;" ID="ID_647712235" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
<node TEXT="Features | GitLab" FOLDED="true" ID="ID_1816225155" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://about.gitlab.com/features/">
<node TEXT=" authentication and more. You can manage an entire GitLab instance through the LDAP / AD integration.  Authentication and Authorization - Missing Features&#xa0;" ID="ID_376931344" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
<node TEXT="Handling Missing Values | Kaggle" FOLDED="true" ID="ID_1880782310" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://www.kaggle.com/dansbecker/handling-missing-values">
<node TEXT="2019 Kaggle Inc. Our Team Terms Privacy Contact/Support." ID="ID_899767684" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
<node TEXT="MISSING: app.Features that Make Managing Coworking Spaces " FOLDED="true" ID="ID_38640041" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://www.coworkingnext.com/features">
<node TEXT="Best in class features including automated invoicing online payments resource management members directory calendar and more." ID="ID_152765176" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
</node>
</node>
<node TEXT="Data scaling and normalization" ID="ID_267412613" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Data scaling and normalization#$D$#" FOLDED="true" ID="ID_737735452" CREATED="1557224068598" MODIFIED="1557225483434">
<icon BUILTIN="stop-sign"/>
<node TEXT="data transformation - Normalization vs. scaling - Cross Validated" FOLDED="true" ID="ID_1532765916" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://stats.stackexchange.com/questions/35591/normalization-vs-scaling">
<node TEXT="Normalizing can either mean applying a transformation so that you  I also see people using the term Normalization for Data Scaling as in&#xa0;" ID="ID_693888894" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Scaling vs Normalization" FOLDED="true" ID="ID_495089035" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://kharshit.github.io/blog/2018/03/23/scaling-vs-normalization">
<node TEXT="Mar 23 2018  Feature scaling (also known as data normalization) is the method used to standardize the range of features of data. Since the range of values&#xa0;" ID="ID_1591110531" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Data Cleaning Challenge: Scale and Normalize Data | Kaggle" FOLDED="true" ID="ID_1058063843" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://www.kaggle.com/rtatman/data-cleaning-challenge-scale-and-normalize-data">
<node TEXT="Context. Im a crowdfunding enthusiast and im watching kickstarter since its early days. Right now I just collect data and the only app ive made is this twitter bot&#xa0;" ID="ID_599627162" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="About Feature Scaling and Normalization" FOLDED="true" ID="ID_1848485952" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://sebastianraschka.com/Articles/2014_about_feature_scaling.html">
<node TEXT="Jul 11 2014  The result of standardization (or Z-score normalization) is that the features  In this approach the data is scaled to a fixed range - usually 0 to 1." ID="ID_1740118394" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Feature scaling - Wikipedia" FOLDED="true" ID="ID_1061128191" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://en.wikipedia.org/wiki/Feature_scaling">
<node TEXT=" as min-max scaling or min-max normalization is the  the target range depends on the nature of the data." ID="ID_1802714768" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Data scaling and normalization - Machine Learning Algorithms" FOLDED="true" ID="ID_1526015938" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781785889622/3/ch03lvl1sec24/data-scaling-and-normalization">
<node TEXT="Data scaling and normalizationA generic dataset (we assume here that it is always numerical) is made up of different values wh" ID="ID_550620472" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Standardize or Normalize? &#x2014; Examples in Python &#x2013; Robert R.F. " FOLDED="true" ID="ID_514997329" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://medium.com/@rrfd/standardize-or-normalize-examples-in-python-e3f174b65dfc">
<node TEXT="Apr 29 2018  Normalization makes training less sensitive to the scale of features  Or the scale between your data features does matters so you want to&#xa0;" ID="ID_1561178444" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Scale Standardize or Normalize with Scikit-Learn &#x2013; Towards Data " FOLDED="true" ID="ID_1789768742" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02">
<node TEXT="Mar 4 2019  Many machine learning algorithms work better when features are on a relatively similar scale and close to normally distributed. MinMaxScaler&#xa0;" ID="ID_1411963885" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Normalization (statistics) - Wikipedia" FOLDED="true" ID="ID_490962163" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://en.wikipedia.org/wiki/Normalization_(statistics)">
<node TEXT="In statistics and applications of statistics normalization can have a range of meanings. In the simplest cases normalization of ratings means adjusting values measured on different scales to a notionally common scale  when parameters are estimated particularly across different data points in regression analysis." ID="ID_1276942368" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Scaling? Normalizing/? Standardizing? Which do I use when? | Data " FOLDED="true" ID="ID_1898041330" CREATED="1557224068599" MODIFIED="1557225479630" LINK="http://datareality.blogspot.com/2016/11/scaling-normalizing-standardizing-which.html">
<node TEXT="Nov 29 2016  Whats the difference between scaling to 0 and 1 taking a unit norm or z score? Ive been taught all of these to normalize data but dont know&#xa0;" ID="ID_1295636349" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
</node>
</node>
<node TEXT="Feature Selection and Filtering" ID="ID_35838212" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Feature Selection#$D$#" FOLDED="true" ID="ID_457758042" CREATED="1557224068598" MODIFIED="1557225483423">
<icon BUILTIN="stop-sign"/>
<node TEXT="Feature selection - Wikipedia" FOLDED="true" ID="ID_1507637636" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://en.wikipedia.org/wiki/Feature_selection">
<node TEXT="In machine learning and statistics feature selection also known as variable selection attribute selection or variable subset selection is the process of selecting&#xa0;" ID="ID_671589642" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
<node TEXT="1.13. Feature selection &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_194571426" CREATED="1557224068598" MODIFIED="1557225479630" LINK="http://scikit-learn.org/stable/modules/feature_selection.html">
<node TEXT="The classes in the sklearn.feature_selection module can be used for feature selection/dimensionality reduction on sample sets either to improve estimators&#xa0;" ID="ID_858849307" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
<node TEXT="Feature Selection Techniques in Machine Learning with Python" FOLDED="true" ID="ID_1266133535" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e">
<node TEXT="Oct 27 2018  Feature Selection is one of the core concepts in machine learning which hugely impacts the performance of your model. The data features that&#xa0;" ID="ID_482416267" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
<node TEXT="An Introduction to Feature Selection" FOLDED="true" ID="ID_838953473" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://machinelearningmastery.com/an-introduction-to-feature-selection/">
<node TEXT="Oct 6 2014  It is possible to automatically select those features in your data that are most useful or most relevant for the problem you are working on. This is&#xa0;" ID="ID_359743196" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
<node TEXT="Chapter 7 Feature Selection" FOLDED="true" ID="ID_40658732" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://www.cs.cmu.edu/~kdeng/thesis/feature.pdf">
<node TEXT="Feature selection is not used in the system classification experiments which will  However as an autonomous system OMEGA includes feature selection as." ID="ID_658764658" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
<node TEXT="Feature Selection methods with example (Variable selection methods)" FOLDED="true" ID="ID_1453608247" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/">
<node TEXT="Dec 1 2016  This article is on feature selection used to build an effective predictive model. Learn about flitter method wrapper method and embedded&#xa0;" ID="ID_1506147719" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
<node TEXT="Feature Selection - Ten Effective Techniques with Examples | ML+" FOLDED="true" ID="ID_514742408" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://www.machinelearningplus.com/machine-learning/feature-selection/">
<node TEXT="In machine learning Feature selection is the process of choosing variables that are useful in predicting the response (Y). It is considered a good practice to&#xa0;" ID="ID_1128660552" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
<node TEXT="What is Feature Selection? - Definition from Techopedia" FOLDED="true" ID="ID_822230787" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://www.techopedia.com/definition/32755/feature-selection">
<node TEXT="With feature selection engineers and data scientists are able to tune out a lot of the &#x201c;noise&#x201d; in a given system. Using feature selection helps to discard redundant&#xa0;" ID="ID_84739575" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
<node TEXT="How do I select features for Machine Learning? - YouTube" FOLDED="true" ID="ID_1645427502" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://www.youtube.com/watch?v=YaKMeAlHgqQ">
<node TEXT="Nov 13 2018  Selecting the best features for your Machine Learning model will result in a better performing easier to understand and faster running model." ID="ID_523188389" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
<node TEXT="Beginners Guide to Feature Selection in Python (article) - DataCamp" FOLDED="true" ID="ID_439583169" CREATED="1557224068598" MODIFIED="1557225479630" LINK="https://www.datacamp.com/community/tutorials/feature-selection-python">
<node TEXT="Sep 25 2018  Learn about the basics of feature selection and how to implement and investigate various feature selection techniques in Python." ID="ID_433766269" CREATED="1557224068598" MODIFIED="1557224068598"/>
</node>
</node>
<node TEXT="Feature Selection and Filtering#$D$#" FOLDED="true" ID="ID_819590017" CREATED="1557224068599" MODIFIED="1557225483434">
<icon BUILTIN="stop-sign"/>
<node TEXT="Feature selection - Wikipedia" FOLDED="true" ID="ID_1043940545" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://en.wikipedia.org/wiki/Feature_selection">
<node TEXT="Filter Method for feature selection. Filter type methods select variables regardless of the model. They are based only on general&#xa0;" ID="ID_1692974043" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="An Introduction to Feature Selection" FOLDED="true" ID="ID_174381689" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://machinelearningmastery.com/an-introduction-to-feature-selection/">
<node TEXT="Oct 6 2014  Filter feature selection methods apply a statistical measure to assign a scoring to each feature. The features are ranked by the score and either&#xa0;" ID="ID_1722659285" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Filter Based Feature Selection - Azure Machine Learning Studio " FOLDED="true" ID="ID_1640531063" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/filter-based-feature-selection">
<node TEXT="Jan 16 2018  This article describes how to use the Filter Based Feature Selection module in Azure Machine Learning Studio to identify the columns in your&#xa0;" ID="ID_1648370080" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Feature Selection methods with example (Variable selection methods)" FOLDED="true" ID="ID_920414474" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/">
<node TEXT="Dec 1 2016  Filter methods are generally used as a preprocessing step. The selection of features is independent of any machine learning algorithms." ID="ID_695659398" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Weka Tutorial 10: Feature Selection with Filter (Data Dimensionality " FOLDED="true" ID="ID_394478098" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://www.youtube.com/watch?v=UOadhDKRbPM">
<node TEXT="Jun 6 2012  This tutorial shows how to select features from a set of features that performs best with a classification algorithm using filter method." ID="ID_1011841226" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Applying Filter Methods in Python for Feature Selection" FOLDED="true" ID="ID_765687916" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://stackabuse.com/applying-filter-methods-in-python-for-feature-selection/">
<node TEXT="Oct 30 2018  One category of such methods is called filter methods. In this article we will study some of the basic filter methods for feature selection." ID="ID_1542467301" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="19 Feature Selection using Univariate Filters | The caret Package" FOLDED="true" ID="ID_426675873" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://topepo.github.io/caret/feature-selection-using-univariate-filters.html">
<node TEXT="The caret function sbf (for selection by filter) can be used to cross-validate such feature selection schemes. Similar to rfe  functions can be passed into sbf for the&#xa0;" ID="ID_1255199426" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Feature Selection &#x2022; mlr" FOLDED="true" ID="ID_188153576" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://mlr.mlr-org.com/articles/tutorial/feature_selection.html">
<node TEXT="In the literature two different approaches exist: One is called &#x201c;Filtering&#x201d; and the other approach is often referred to as &#x201c;feature subset selection&#x201d; or &#x201c;wrapper&#xa0;" ID="ID_32389750" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="What is the difference between filter wrapper and embedded " FOLDED="true" ID="ID_1827981944" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://sebastianraschka.com/faq/docs/feature_sele_categories.html">
<node TEXT="In contrast the filter methods pick up the intrinsic properties of the features (i.e.  recursive feature elimination; sequential feature selection algorithms; genetic&#xa0;" ID="ID_1095176928" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Filter Methods for Feature Selection - A Comparative Study." FOLDED="true" ID="ID_905991235" CREATED="1557224068599" MODIFIED="1557225479630" LINK="http://dl.ifip.org/db/conf/ideal/ideal2007/Sanchez-MaronoAT07.pdf">
<node TEXT="Filter methods for feature selection. A comparative study *. Noelia S&#xe1;nchez-Maro&#x2dc;no Amparo Alonso-Betanzos and Mar&#x131;a. Tombilla-Sanrom&#xe1;n. University of A&#xa0;" ID="ID_1310590500" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
</node>
</node>
</node>
<node TEXT="Principal Component Analysis" ID="ID_114737170" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Non negative matrix factorization" ID="ID_1235406897" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Principal Component Analysis#$D$#" FOLDED="true" ID="ID_1103589671" CREATED="1557224068599" MODIFIED="1557225483434">
<icon BUILTIN="stop-sign"/>
<node TEXT="Principal component analysis - Wikipedia" FOLDED="true" ID="ID_18130387" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://en.wikipedia.org/wiki/Principal_component_analysis">
<node TEXT="Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated&#xa0;" ID="ID_1039958546" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Principal Component Analysis explained visually" FOLDED="true" ID="ID_176871539" CREATED="1557224068599" MODIFIED="1557225479630" LINK="http://setosa.io/ev/principal-component-analysis/">
<node TEXT="Principal component analysis (PCA) is a technique used to emphasize variation and bring out strong patterns in a dataset. Its often used to make data easy to&#xa0;" ID="ID_658426074" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="A One-Stop Shop for Principal Component Analysis &#x2013; Towards Data " FOLDED="true" ID="ID_1058819658" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c">
<node TEXT="Apr 17 2017  Principal component analysis is a technique for feature extraction &#x2014; so it combines our input variables in a specific way then we can drop the&#xa0;" ID="ID_1344900427" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Principal Component Analysis" FOLDED="true" ID="ID_1998865201" CREATED="1557224068599" MODIFIED="1557225479630" LINK="ftp://statgen.ncsu.edu/pub/thorne/molevoclass/AtchleyOct19.pdf">
<node TEXT="Principal component analysis (PCA) is a mathematical procedure that transforms a number of (possibly) correlated variables into a (smaller) number of." ID="ID_1554435667" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="StatQuest: PCA main ideas in only 5 minutes!!! - YouTube" FOLDED="true" ID="ID_692728301" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://www.youtube.com/watch?v=HMOI_lkzW08">
<node TEXT="Dec 4 2017  The main ideas behind PCA are actually super simple and that means its easy to interpret a PCA plot: Samples that are correlated will cluster together apart  StatQuest: Principal Component Analysis (PCA) Step-by-Step&#xa0;" ID="ID_1685136454" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Principal component analysis | Nature Methods" FOLDED="true" ID="ID_989556791" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://www.nature.com/articles/nmeth.4346">
<node TEXT="Jun 29 2017  Principal component analysis (PCA) simplifies the complexity in high-dimensional data while retaining trends and patterns. It does this by&#xa0;" ID="ID_379371581" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="StatQuest: Principal Component Analysis (PCA) Step-by-Step " FOLDED="true" ID="ID_75193029" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://www.youtube.com/watch?v=FgakZw6K1QQ">
<node TEXT="Apr 2 2018  Principal Component Analysis is one of the most useful data analysis and machine learning methods out there. It can be used to identify&#xa0;" ID="ID_1082871628" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Principal component analysis" FOLDED="true" ID="ID_795043299" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://www.utdallas.edu/~herve/abdi-awPCA2010.pdf">
<node TEXT="Principal component analysis (PCA) is a multivariate technique that analyzes a data  The quality of the PCA model can be evaluated using cross-validation." ID="ID_808839102" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Principal Component Analysis | I.T. Jolliffe | Springer" FOLDED="true" ID="ID_1176160221" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://www.springer.com/us/book/9780387954424">
<node TEXT="Principal component analysis is central to the study of multivariate data. Although one of the earliest multivariate techniques it continues to be the subject of&#xa0;" ID="ID_1071735586" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Lesson 11: Principal Components Analysis (PCA) | STAT 505" FOLDED="true" ID="ID_177636044" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://newonlinecourses.science.psu.edu/stat505/node/49/">
<node TEXT="Printer-friendly version. Introduction. Sometimes data are collected on a large number of variables from a single population. As an example consider the Places&#xa0;" ID="ID_672159379" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
</node>
<node TEXT="Non negative matrix factorization#$D$#" FOLDED="true" ID="ID_1270031287" CREATED="1557224068599" MODIFIED="1557225483435">
<icon BUILTIN="stop-sign"/>
<node TEXT="Non-negative matrix factorization - Wikipedia" FOLDED="true" ID="ID_1090813714" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://en.wikipedia.org/wiki/Non-negative_matrix_factorization">
<node TEXT="Non-negative matrix factorization (NMF or NNMF) also non-negative matrix approximation is a group of algorithms in multivariate analysis and linear algebra&#xa0;" ID="ID_232942632" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="A Practical Introduction to NMF (nonnegative matrix factorization " FOLDED="true" ID="ID_1398795186" CREATED="1557224068599" MODIFIED="1557225479630" LINK="http://mlexplained.com/2017/12/28/a-practical-introduction-to-nmf-nonnegative-matrix-factorization/">
<node TEXT="Dec 28 2017  NMF (Nonnegative Matrix Factorization) is a matrix factorization method where we constrain the matrices to be nonnegative. In order to&#xa0;" ID="ID_1071152753" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="The why and how of nonnegative matrix factorization | the morning " FOLDED="true" ID="ID_834537504" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://blog.acolyer.org/2019/02/18/the-why-and-how-of-nonnegative-matrix-factorization/">
<node TEXT="Feb 18 2019  NMF was first introduced by Paatero andTapper in 1994 and popularised in a article by Lee and Seung in 1999. Since then the number of&#xa0;" ID="ID_1432092677" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Algorithms for Non-negative Matrix Factorization" FOLDED="true" ID="ID_419096403" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf">
<node TEXT="Non-negative matrix factorization (NMF) has previously been shown to be a useful decomposition for multivariate data. Two different multi- plicative algorithms&#xa0;" ID="ID_619673278" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="The Why and How of Nonnegative Matrix Factorization" FOLDED="true" ID="ID_768807411" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://arxiv.org/abs/1401.5226">
<node TEXT="Jan 21 2014  We first illustrate this property of NMF on three applications in image  and computer science closely related to NMF via the nonnegative rank." ID="ID_554078836" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="sklearn.decomposition.NMF &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_1815405385" CREATED="1557224068599" MODIFIED="1557225479630" LINK="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html">
<node TEXT="Non-Negative Matrix Factorization (NMF). Find two non-negative matrices (W H) whose product approximates the non- negative matrix X. This factorization can&#xa0;" ID="ID_1214674872" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Non-negative Matrix Factorization with Sparseness Constraints" FOLDED="true" ID="ID_1578446888" CREATED="1557224068599" MODIFIED="1557225479630" LINK="http://www.jmlr.org/papers/volume5/hoyer04a/hoyer04a.pdf">
<node TEXT="Abstract. Non-negative matrix factorization (NMF) is a recently developed technique for finding parts-based linear representations of non-negative data." ID="ID_1352385119" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="10701: Non-Negative Matrix Factorization - YouTube" FOLDED="true" ID="ID_472078431" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://www.youtube.com/watch?v=UQGEB3Q5-fQ">
<node TEXT="Dec 11 2013  This is an extra credit assignment for the class 10-701 at Carnegie Mellon University. Here are my sources:&#xa0;" ID="ID_556337376" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Non-Negative Matrix Factorization" FOLDED="true" ID="ID_755638200" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://docs.oracle.com/cd/B28359_01/datamine.111/b28129/algo_nmf.htm">
<node TEXT="About NMF. Non-negative Matrix Factorization (NMF) is a state of the art feature extraction algorithm. NMF is useful when there are many attributes and the&#xa0;" ID="ID_106362966" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Non-negative matrix factorization" FOLDED="true" ID="ID_1847411711" CREATED="1557224068599" MODIFIED="1557225479630" LINK="http://statweb.stanford.edu/~tibs/sta306bfiles/nnmf.pdf">
<node TEXT="Figure 1 Non-negative matrix factorization (NMF) learns a parts-based representation of faces whereas vector quantization (VQ) and principal components&#xa0;" ID="ID_176899831" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
</node>
</node>
<node TEXT="Sparse PCA" FOLDED="true" ID="ID_198096891" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Sparse PCA#$D$#" FOLDED="true" ID="ID_1337827623" CREATED="1557224068599" MODIFIED="1557225483435">
<icon BUILTIN="stop-sign"/>
<node TEXT="Sparse Principal Component Analysis" ID="ID_186701612" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://web.stanford.edu/~hastie/Papers/spc_jcgs.pdf"/>
<node TEXT="Sparse PCA - Wikipedia" ID="ID_141282387" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://en.wikipedia.org/wiki/Sparse_PCA"/>
<node TEXT="sklearn.decomposition.SparsePCA &#x2014; scikit-learn 0.20.3 " ID="ID_907271570" CREATED="1557224068599" MODIFIED="1557225479630" LINK="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.SparsePCA.html"/>
<node TEXT="Cai  Ma  Wu : Sparse PCA: Optimal rates and adaptive estimation" ID="ID_560945773" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://projecteuclid.org/euclid.aos/1388545679"/>
<node TEXT="Sparse Principal Component Analysis: Journal of Computational " ID="ID_745801477" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://www.tandfonline.com/doi/abs/10.1198/106186006X113430"/>
<node TEXT="machine learning - How exactly is sparse PCA better than PCA " ID="ID_1202413964" CREATED="1557224068599" MODIFIED="1557225479630" LINK="https://stats.stackexchange.com/questions/79168/how-exactly-is-sparse-pca-better-than-pca"/>
<node TEXT="Sparse PCA" ID="ID_683426222" CREATED="1557224068599" MODIFIED="1557225479645" LINK="https://www.ml.uni-saarland.de/code/sparsePCA/sparsePCA.htm"/>
<node TEXT="Optimal solutions for Sparse Principal Component Analysis" ID="ID_973037813" CREATED="1557224068599" MODIFIED="1557225479645" LINK="https://www.di.ens.fr/~fbach/SPO.pdf"/>
<node TEXT="Sparse PCA in High Dimensions" ID="ID_468344872" CREATED="1557224068599" MODIFIED="1557225479645" LINK="http://www.stat.cmu.edu/~jinglei/201312-simons.pdf"/>
<node TEXT="Sparse PCA in High Dimensions - YouTube" ID="ID_668271336" CREATED="1557224068599" MODIFIED="1557225479645" LINK="https://www.youtube.com/watch?v=QtS3xQzNMlY"/>
</node>
</node>
<node TEXT="Kernel PCA" ID="ID_159001383" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
<node TEXT="Interactive Visualization " ID="ID_1082675605" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
</node>
<node TEXT="Atom Extraction" FOLDED="true" ID="ID_362664360" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Atom Extraction#$D$#" FOLDED="true" ID="ID_1572125485" CREATED="1557224068599" MODIFIED="1557225483435">
<icon BUILTIN="stop-sign"/>
<node TEXT="Atom-by-atom extraction using the scanning tunneling microscope " FOLDED="true" ID="ID_1170852951" CREATED="1557224068599" MODIFIED="1557225479645" LINK="https://www.ncbi.nlm.nih.gov/pubmed/17358656">
<node TEXT="Phys Rev Lett. 2007 Jan 12;98(2):028304. Epub 2007 Jan 11. Atom-by-atom extraction using the scanning tunneling microscope tip-cluster interaction." ID="ID_1954562832" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Atom extraction and dictionary learning - Machine Learning Algorithms" FOLDED="true" ID="ID_875059573" CREATED="1557224068599" MODIFIED="1557225479645" LINK="https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781785889622/3/ch03lvl1sec27/atom-extraction-and-dictionary-learning">
<node TEXT="Atom extraction and dictionary learningDictionary learning is a technique which allows rebuilding a sample starting from a spa" ID="ID_547533953" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Atom-by-atom extraction using scanning tunneling microscope tip " FOLDED="true" ID="ID_42511879" CREATED="1557224068599" MODIFIED="1557225479645" LINK="https://arxiv.org/abs/cond-mat/0611099">
<node TEXT="Nov 3 2006  Abstract: We investigate atomistic details of a single atom extraction process realized by using scanning tunneling microscope (STM) tip-cluster&#xa0;" ID="ID_947127103" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="(PDF) Atom-By-Atom Extraction Using the Scanning Tunneling " FOLDED="true" ID="ID_817666148" CREATED="1557224068599" MODIFIED="1557225479645" LINK="https://www.researchgate.net/publication/6447875_Atom-By-Atom_Extraction_Using_the_Scanning_Tunneling_Microscope_Tip-Cluster_Interaction">
<node TEXT="Apr 16 2019  PDF | We investigate atomistic details of a single atom extraction process realized by using scanning tunneling microscope (STM) tip-cluster&#xa0;" ID="ID_1775984803" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="ATOM - Extraction - YouTube" FOLDED="true" ID="ID_252440316" CREATED="1557224068599" MODIFIED="1557225479645" LINK="https://www.youtube.com/watch?v=xGmYLBpDNCw">
<node TEXT="Dec 16 2014  Lien Itunes // https://itunes.apple.com/fr/album/extraction/id950747664?i=950747667uo=4 ATOM - Extraction [EP-EXTRACTOR] Website&#xa0;" ID="ID_404520726" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Atom-By-Atom Extraction Using the Scanning Tunneling Microscope " FOLDED="true" ID="ID_337236411" CREATED="1557224068599" MODIFIED="1557225479645" LINK="https://link.aps.org/doi/10.1103/PhysRevLett.98.028304">
<node TEXT="Jan 11 2007  We investigate the atomistic details of a single atom-extraction process realized by using the scanning tunneling microscope tip-cluster&#xa0;" ID="ID_1405425530" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="ATOM - Extraction Live [Pad Session] - YouTube" FOLDED="true" ID="ID_602178725" CREATED="1557224068599" MODIFIED="1557225479645" LINK="https://www.youtube.com/watch?v=dBGf5e9lUnY">
<node TEXT="Apr 30 2015  Session Live du titre Extraction extrait de notre EP Extractor Lien EP - https://itunes.apple.com/fr/album/extractor-ep/id950747664 Facebook&#xa0;" ID="ID_285024098" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Atom extraction and dictionary learning - Machine Learning " FOLDED="true" ID="ID_786780897" CREATED="1557224068599" MODIFIED="1557225479645" LINK="https://www.oreilly.com/library/view/machine-learning-algorithms/9781789347999/b99bfb97-9a32-4421-9659-b5b3236928c5.xhtml">
<node TEXT="Atom extraction and dictionary learning Dictionary learning is a technique that allows you to rebuild a sample starting from a sparse dictionary of atoms (similar&#xa0;" ID="ID_955331526" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="psd-extract" FOLDED="true" ID="ID_1745625255" CREATED="1557224068599" MODIFIED="1557225479645" LINK="https://atom.io/packages/psd-extract">
<node TEXT="psd-extract. Atom.io package which helps to extract information from photoshop layers. psd-extract. Its very early version so it only collects css information&#xa0;" ID="ID_477730416" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Single-Atom Extraction by Scanning Tunneling Microscope Tip " FOLDED="true" ID="ID_1049855541" CREATED="1557224068599" MODIFIED="1557225479645" LINK="https://pubs.acs.org/doi/abs/10.1021/nl0487065">
<node TEXT="We report a novel atom extraction mechanism from the native substrate by means of a scanning tunneling microscope tip crash on a Ag(111) surface at 5 K." ID="ID_591080324" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
</node>
</node>
<node TEXT="Dictionary Learning" FOLDED="true" ID="ID_618052741" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Dictionary Learning#$D$#" FOLDED="true" ID="ID_679004634" CREATED="1557224068599" MODIFIED="1557225483435">
<icon BUILTIN="stop-sign"/>
<node TEXT="Sparse dictionary learning - Wikipedia" FOLDED="true" ID="ID_1271891966" CREATED="1557224068599" MODIFIED="1557225479645" LINK="https://en.wikipedia.org/wiki/Sparse_dictionary_learning">
<node TEXT="Sparse dictionary learning is a representation learning method which aims at finding a sparse representation of the input data in the form of a linear combination&#xa0;" ID="ID_985522234" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Dictionary learning: theory and algorithms | PANAMA" FOLDED="true" ID="ID_698701014" CREATED="1557224068599" MODIFIED="1557225479645" LINK="https://team.inria.fr/panama/en/projects/please/dictionary-learning/">
<node TEXT="Dictionary learning is a branch of signal processing and machine learning that aims at finding a frame (called dictionary) in which some training data admits a&#xa0;" ID="ID_366243905" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="What is dictionary learning? - Quora" FOLDED="true" ID="ID_1484412037" CREATED="1557224068599" MODIFIED="1557225479645" LINK="https://www.quora.com/What-is-dictionary-learning">
<node TEXT="In Machine Learning complex input can be seen as a sparse (mostly zeros) combination of primitive components (dictionary elements). An example is: digit&#xa0;" ID="ID_1320309119" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Dictionary learning for sparse coding: Algorithms and convergence " FOLDED="true" ID="ID_1022412911" CREATED="1557224068599" MODIFIED="1557225479645" LINK="http://www.math.nus.edu.sg/~matzuows/BJQS1.pdf">
<node TEXT="Dictionary learning for sparse coding: Algorithms and convergence analysis. Chenglong Bao Hui Ji Yuhui Quan and Zuowei Shen. Abstract&#x2014;In recent years&#xa0;" ID="ID_813511844" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Merriam-Websters Learners Dictionary" FOLDED="true" ID="ID_1671585545" CREATED="1557224068600" MODIFIED="1557225479645" LINK="http://learnersdictionary.com/">
<node TEXT="Clear and simple definitions in basic American English from North Americas leading language experts. More usage examples than any other dictionary." ID="ID_1843604351" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Dictionary Learning - an overview | ScienceDirect Topics" FOLDED="true" ID="ID_528715065" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://www.sciencedirect.com/topics/engineering/dictionary-learning">
<node TEXT="In [35] we described a dictionary learning approach to the image-based analysis of heterogeneity. The underlying idea behind dictionary learning is in the&#xa0;" ID="ID_664059402" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="sklearn.decomposition.DictionaryLearning &#x2014; scikit-learn 0.20.3 " FOLDED="true" ID="ID_1570880740" CREATED="1557224068600" MODIFIED="1557225479645" LINK="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.DictionaryLearning.html">
<node TEXT="Dictionary learning. Finds a dictionary (a set of atoms) that can best be used to represent data using a sparse code. Solves the optimization problem: (U^*V^*)&#xa0;" ID="ID_626273500" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Dictionary Learning Algorithms for Sparse Representation" FOLDED="true" ID="ID_837332583" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2944020/">
<node TEXT="Algorithms for data-driven learning of domain-specific overcomplete dictionaries are developed to obtain maximum likelihood and maximum a posteriori&#xa0;" ID="ID_1733340600" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Wasserstein Dictionary Learning: Optimal Transport-based " FOLDED="true" ID="ID_4948713" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://arxiv.org/abs/1708.01955">
<node TEXT="Aug 7 2017  Abstract: This paper introduces a new nonlinear dictionary learning method for histograms in the probability simplex. The method leverages&#xa0;" ID="ID_577698521" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Dictionary Learning" FOLDED="true" ID="ID_143426626" CREATED="1557224068600" MODIFIED="1557225479645" LINK="http://www.numerical-tours.com/matlab/sparsity_4_dictionary_learning/">
<node TEXT="Given a set Y=(yj)mj=1&#x2208;Rn&#xd7;m of m signals yj&#x2208;Rm dictionary learning aims at finding the best dictionary D=(di)pi=1 of p atoms di&#x2208;Rn to sparse code all the&#xa0;" ID="ID_1960563736" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
</node>
</node>
</node>
<node TEXT="Regression" FOLDED="true" POSITION="right" ID="ID_1484471850" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<edge COLOR="#00cc33"/>
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Linear Regression" FOLDED="true" ID="ID_1656709819" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Linear Models" ID="ID_1383662250" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Linear Regression#$D$#" FOLDED="true" ID="ID_1892064283" CREATED="1557224068600" MODIFIED="1557225483435">
<icon BUILTIN="stop-sign"/>
<node TEXT="Linear regression - Wikipedia" FOLDED="true" ID="ID_483420749" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://en.wikipedia.org/wiki/Linear_regression">
<node TEXT="In statistics linear regression is a linear approach to modelling the relationship between a scalar response (or dependent variable) and one or more explanatory&#xa0;" ID="ID_741754546" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="1.1 - What is Simple Linear Regression? | STAT 501" FOLDED="true" ID="ID_961069570" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://newonlinecourses.science.psu.edu/stat501/node/251/">
<node TEXT="Simple linear regression is a statistical method that allows us to summarize and study relationships between two continuous (quantitative) variables:." ID="ID_1252535144" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Linear Regression" FOLDED="true" ID="ID_761629115" CREATED="1557224068600" MODIFIED="1557225479645" LINK="http://www.stat.yale.edu/Courses/1997-98/101/linreg.htm">
<node TEXT="A linear regression line has an equation of the form Y = a + bX where X is the explanatory variable and Y is the dependent variable. The slope of the line is b&#xa0;" ID="ID_1816321857" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Linear Regression &#x2014; Detailed View &#x2013; Towards Data Science" FOLDED="true" ID="ID_1408605138" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://towardsdatascience.com/linear-regression-detailed-view-ea73175f6e86">
<node TEXT="Feb 26 2018  Linear regression is used for finding linear relationship between target and one or more predictors. There are two types of linear regression-&#xa0;" ID="ID_383146833" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="sklearn.linear_model.LinearRegression &#x2014; scikit-learn 0.20.3 " FOLDED="true" ID="ID_1131426410" CREATED="1557224068600" MODIFIED="1557225479645" LINK="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html">
<node TEXT="Estimated coefficients for the linear regression problem. If multiple targets are passed during the fit (y 2D) this is a 2D array of shape (n_targets n_features)&#xa0;" ID="ID_1894248838" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="An Introduction to Linear Regression Analysis - YouTube" FOLDED="true" ID="ID_599912774" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://www.youtube.com/watch?v=zPG4NjIkCjc">
<node TEXT="Feb 5 2012  Tutorial introducing the idea of linear regression analysis and the least square method. Typically used in a statistics class. Playlist on Linear&#xa0;" ID="ID_318733390" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Regression line example (video) | Khan Academy" FOLDED="true" ID="ID_1567396021" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data/more-on-regression/v/regression-line-example">
<node TEXT="The regression part just ended up stuck as part of the name from then on. Other than that linear regression has nothing to do with regression to the mean." ID="ID_1962130865" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Linear Regression" FOLDED="true" ID="ID_582428153" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://stattrek.com/regression/linear-regression.aspx">
<node TEXT="Simple linear regression. How to define least-squares regression line. How to find coefficient of determination. Includes video lesson on regression analysis." ID="ID_1391614383" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="What is Linear Regression? - Statistics Solutions" FOLDED="true" ID="ID_1056670317" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://www.statisticssolutions.com/what-is-linear-regression/">
<node TEXT="Linear regression is a basic and commonly used type of predictive analysis. The overall idea of regression is to examine two things: (1) does a set of predictor&#xa0;" ID="ID_580495805" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="xkcd: Linear Regression" FOLDED="true" ID="ID_1919769497" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://xkcd.com/1725/">
<node TEXT="sarcasm math and language. Still mourning the demise of Google Reader? You can sign up to get new comics delivered by email here. Linear Regression." ID="ID_395400515" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
</node>
<node TEXT="Linear Models#$D$#" FOLDED="true" ID="ID_784479341" CREATED="1557224068600" MODIFIED="1557225483435">
<icon BUILTIN="stop-sign"/>
<node TEXT="Linear model - Wikipedia" FOLDED="true" ID="ID_1868719593" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://en.wikipedia.org/wiki/Linear_model">
<node TEXT="In statistics the term linear model is used in different ways according to the context. The most common occurrence is in connection with regression models and&#xa0;" ID="ID_1860439897" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Linear Model - MATLAB  Simulink" FOLDED="true" ID="ID_1433146309" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://www.mathworks.com/discovery/linear-model.html">
<node TEXT="Learn about MATLAB support for linear models. Resources include code examples documentation and videos describing linear model and regression&#xa0;" ID="ID_375774528" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Linear models example 1 | Algebra I | Khan Academy - YouTube" FOLDED="true" ID="ID_1108349081" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://www.youtube.com/watch?v=W3flX500w5g">
<node TEXT="Aug 6 2015  Linear model for book reading Practice this lesson yourself on KhanAcademy.org right now:&#xa0;" ID="ID_392286328" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="1.1. Generalized Linear Models &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_356031997" CREATED="1557224068600" MODIFIED="1557225479645" LINK="http://scikit-learn.org/stable/modules/linear_model.html">
<node TEXT="LinearRegression will take in its fit method arrays X y and will store the coefficients w of the linear model in its coef_ member:   from sklearn import&#xa0;" ID="ID_666282244" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Linear models word problem: book (video) | Khan Academy" FOLDED="true" ID="ID_1069311575" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://www.khanacademy.org/math/algebra/linear-word-problems/linear-models-word-problems/v/linear-models-1">
<node TEXT="Sal solves a word problem about a person reading a book The solution involves the modeling of the situation as a linear function." ID="ID_1251513896" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="LINEAR MODELS IN STATISTICS" FOLDED="true" ID="ID_471080466" CREATED="1557224068600" MODIFIED="1557225479645" LINK="http://www.utstat.toronto.edu/~brunner/books/LinearModelsInStatistics.pdf">
<node TEXT="LINEAR MODELS IN. STATISTICS. Second Edition. Alvin C. Rencher and G. Bruce Schaalje. Department of Statistics Brigham Young University Provo Utah&#xa0;" ID="ID_1508849355" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Linear models | Khan Academy" FOLDED="true" ID="ID_1536288804" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://www.khanacademy.org/tag/linear-models">
<node TEXT="Learn for free about math art computer programming economics physics chemistry biology medicine finance history and more. Khan Academy is a&#xa0;" ID="ID_1752321609" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="What is a Linear Model?" FOLDED="true" ID="ID_1591537781" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://content.nexosis.com/blog/what-is-linear-model">
<node TEXT="Nov 10 2016  Our data scientist dons his nerd hat and explains linear models and when and why to use them." ID="ID_800312406" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Estimating with linear regression (linear models) (video) | Khan " FOLDED="true" ID="ID_139684879" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://www.khanacademy.org/math/cc-eighth-grade-math/cc-8th-data/cc-8th-line-of-best-fit/v/example-estimating-from-regression-line">
<node TEXT="Yes. The question specifically asks you to use the equation in which case the answer would be 96 not 97 but since the true values seem to be varying by up to&#xa0;" ID="ID_1820769582" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Machine Learning with Python: from Linear Models to Deep " FOLDED="true" ID="ID_430910865" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://www.edx.org/course/machine-learning-with-python-from-linear-models-to-deep-learning">
<node TEXT="An in-depth introduction to the field of machine learning from linear models to deep learning and reinforcement learning through hands-on Python projects." ID="ID_690820390" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
</node>
<node TEXT="Linear Classification#$D$#" FOLDED="true" ID="ID_1822241724" CREATED="1557224068601" MODIFIED="1557225483436">
<icon BUILTIN="stop-sign"/>
<node TEXT="Linear classifier - Wikipedia" FOLDED="true" ID="ID_202038803" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://en.wikipedia.org/wiki/Linear_classifier">
<node TEXT="In the field of machine learning the goal of statistical classification is to use an objects characteristics to identify which class (or group) it belongs to. A linear&#xa0;" ID="ID_710052388" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Linear Classification" FOLDED="true" ID="ID_549438229" CREATED="1557224068601" MODIFIED="1557225479661" LINK="http://compneurosci.com/wiki/images/c/c0/Linear_Classification.pdf">
<node TEXT="Linear classification. A classification algorithm (Classifier) that makes its classification based on a linear predictor function combining a set of weights with the&#xa0;" ID="ID_1764151002" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Lecture 3: Linear Classification" FOLDED="true" ID="ID_1823926676" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/readings/L03%20Linear%20Classifiers.pdf">
<node TEXT="Lecture 3: Linear Classification. Roger Grosse. 1 Introduction. Last week we saw an example of a learning task called regression. There the goal was to predict&#xa0;" ID="ID_1669198229" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Linear classification: Support Vector Machine Softmax" FOLDED="true" ID="ID_630768963" CREATED="1557224068601" MODIFIED="1557225479661" LINK="http://cs231n.github.io/linear-classify/">
<node TEXT="Intro to Linear classification; Linear score function; Interpreting a linear classifier; Loss function. Multiclass SVM; Softmax classifier; SVM vs Softmax. Interactive&#xa0;" ID="ID_1354272344" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Machine Learning Basics Lecture 2: linear classification" FOLDED="true" ID="ID_1388482020" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://www.cs.princeton.edu/courses/archive/spring16/cos495/slides/ML_basics_lecture2_linear_classification.pdf">
<node TEXT="Machine Learning Basics. Lecture 2: Linear Classification. Princeton University COS 495. Instructor: Yingyu Liang. Page 2. Review: machine learning basics&#xa0;" ID="ID_648800418" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Multiclass SVM optimization demo" FOLDED="true" ID="ID_979116085" CREATED="1557224068601" MODIFIED="1557225479661" LINK="http://vision.stanford.edu/teaching/cs231n/linear-classify-demo/">
<node TEXT="Linear Classification Loss Visualization. These linear classifiers were written in Javascript for Stanfords CS231n: Convolutional Neural Networks for Visual&#xa0;" ID="ID_657827773" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Lecture 6 - Linear Classification" FOLDED="true" ID="ID_1998379884" CREATED="1557224068601" MODIFIED="1557225479661" LINK="http://www.cs.ucf.edu/~mtappen/cap5415/lecs/lec6.pdf">
<node TEXT="In building mathematical models for classifying we are going to focus on dividing these points with a straight line. &#x25b7; This is called linear classification. -5. 0. 5. -6." ID="ID_1052336771" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Linear versus nonlinear classifiers" FOLDED="true" ID="ID_774551061" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://nlp.stanford.edu/IR-book/html/htmledition/linear-versus-nonlinear-classifiers-1.html">
<node TEXT="Linear classification at first seems trivial given the simplicity of this algorithm. However the difficulty is in training the linear classifier that is in determining the&#xa0;" ID="ID_1462419185" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="17: Linear Classification - YouTube" FOLDED="true" ID="ID_410562916" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://www.youtube.com/watch?v=h08kTk6p2Zs">
<node TEXT="Nov 6 2014  Linear Classification. See http://www.brml.org/  17: Linear Classification. Patrick van der Smagt. Loading Unsubscribe from Patrick van der&#xa0;" ID="ID_1969252916" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Actionable Recourse in Linear Classification" FOLDED="true" ID="ID_731645144" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://arxiv.org/abs/1809.06514">
<node TEXT="Sep 18 2018  In this paper we propose to audit a linear classification model in terms of recourse which we define as the ability of a person to change the&#xa0;" ID="ID_256890974" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
</node>
</node>
<node TEXT="A bi-dimensional example" ID="ID_477978821" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
<node TEXT="Linear Regression and Higher dimensionality" ID="ID_1510664775" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Linear Regression bi-dimensional example#$D$#" FOLDED="true" ID="ID_1708275511" CREATED="1557224068600" MODIFIED="1557225483435">
<icon BUILTIN="stop-sign"/>
<node TEXT="Bidimensional Regression" FOLDED="true" ID="ID_230368325" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1538-4632.1994.tb00320.x">
<node TEXT="example in biology for the comparison of shapes of leaves fish faces or skulls afer . In the bidimensional case each variable has two components. Thus the&#xa0;" ID="ID_1157257759" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Bidimensional Regression: Assessing the Configural Similarity and " FOLDED="true" ID="ID_1032437716" CREATED="1557224068600" MODIFIED="1557225479645" LINK="http://web.psych.ualberta.ca/~alinda/PDFs/Friedman%20Kohler%20%5B03-Psych%20Methods%5D.pdf">
<node TEXT="Bidimensional regression is a method for comparing the degree of resemblance between 2 planar  For example it can assess the similarity be- tween location&#xa0;" ID="ID_102874245" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Bidimensional regression: Issues with interpolation" FOLDED="true" ID="ID_1715153281" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://pdfs.semanticscholar.org/3b57/ba5bbdb0f6a8ab8bf33305ba65f03f86374d.pdf">
<node TEXT="the fit of bidimensional regression models increased with the  For example interpolation maintains . pairs per sample in terms of inflation and the linear." ID="ID_785854032" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Package BiDimRegression" FOLDED="true" ID="ID_52413283" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://cran.r-project.org/web/packages/BiDimRegression/BiDimRegression.pdf">
<node TEXT="May 16 2018  Title Calculates the Bidimensional Regression Between Two 2D . Example 1 from the domain of aesthetics to show how the method can be&#xa0;" ID="ID_373699318" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="(PDF) Bidimensional Regression in Spatial Analysis | Regional " FOLDED="true" ID="ID_1090797617" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://www.academia.edu/10155026/Bidimensional_Regression_in_Spatial_Analysis">
<node TEXT="In his demonstrative example Tobler compared a medieval map2 of Britannia  This paper presents the main characteristics of the bidimensional regression&#xa0;" ID="ID_1869612436" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Simple linear regression - Wikipedia" FOLDED="true" ID="ID_45132345" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://en.wikipedia.org/wiki/Simple_linear_regression">
<node TEXT="In statistics simple linear regression is a linear regression model with a single explanatory variable. That is it concerns two-dimensional sample points with one&#xa0;" ID="ID_238626140" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Multiple Linear Regression" FOLDED="true" ID="ID_14983114" CREATED="1557224068600" MODIFIED="1557225479645" LINK="http://mezeylab.cb.bscb.cornell.edu/labmembers/documents/supplement%205%20-%20multiple%20regression.pdf">
<node TEXT="be understood as a two-dimensional surface in space. The shape of  Example: The simplest multiple regression model for two predictor variables is y = &#x3b2;0 +&#xa0;" ID="ID_1860679408" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="A Comparison of Regression Techniques for a Two-Dimensional " FOLDED="true" ID="ID_361014681" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3446205/">
<node TEXT="Numerous studies over the past two decades show that scalp-recorded EEG .. For example if the features are normalized the coefficients of a regression that&#xa0;" ID="ID_1101292317" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Simple linear regression for two-dimensional sample data points #298" FOLDED="true" ID="ID_899796113" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://github.com/morelinq/MoreLINQ/issues/298">
<node TEXT="May 17 2017  Given a sample of two-dimensional (x y) data points the operator will calculate and return the following components of the a simple linear&#xa0;" ID="ID_1936522646" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Example of non-linear regression; the two-dimensional data points " FOLDED="true" ID="ID_1224327384" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://www.researchgate.net/figure/Example-of-non-linear-regression-the-two-dimensional-data-points-xi-yi-in-this-sample_fig1_273872578">
<node TEXT="Example of non-linear regression; the two-dimensional data points (xi yi) in this sample were generated using yi = 0.2 x 3 i + 0.4 x 2 i &#x2212; 2.1 xi &#x2212; 1.5 + where&#xa0;" ID="ID_117629164" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
</node>
<node TEXT="Linear Regression and Higher dimensionality#$D$#" FOLDED="true" ID="ID_1330651134" CREATED="1557224068600" MODIFIED="1557225483435">
<icon BUILTIN="stop-sign"/>
<node TEXT="Data Science - High dimensional regression" FOLDED="true" ID="ID_1763334011" CREATED="1557224068600" MODIFIED="1557225479645" LINK="http://www.math.univ-toulouse.fr/~besse/Wikistat/pdf/st-m-datSc2-regHD.pdf">
<node TEXT="Data Science - High dimensional regression. Summary. Linear models are popular methods for providing a regression of a response variable Y  that depends&#xa0;" ID="ID_54445994" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="High-dimensional regression" FOLDED="true" ID="ID_256581788" CREATED="1557224068600" MODIFIED="1557225479645" LINK="http://www.stat.cmu.edu/~ryantibs/advmethods/notes/highdim.pdf">
<node TEXT="High-dimensional regression. Advanced Methods for Data Analysis (36-402/36-608). Spring 2014. 1 Back to linear regression. 1.1 Shortcomings. &#x2022; Suppose that&#xa0;" ID="ID_1592099612" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Lecture 16: High-dimensional regression non-linear regression" FOLDED="true" ID="ID_70740447" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://web.stanford.edu/class/stats202/content/lec16.pdf">
<node TEXT="Nov 2 2018  Lecture 16: High-dimensional regression non-linear regression. Reading: Sections 6.4 7.1. STATS 202: Data mining and analysis. Jonathan&#xa0;" ID="ID_1217862191" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Fit linear regression model to high-dimensional data - MATLAB " FOLDED="true" ID="ID_609604580" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://www.mathworks.com/help/stats/fitrlinear.html">
<node TEXT="fitrlinear efficiently trains linear regression models with high-dimensional full or sparse predictor data. Available linear regression models include regularized&#xa0;" ID="ID_1446190580" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Statistical challenges of high-dimensional data" FOLDED="true" ID="ID_1521391403" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2865881/">
<node TEXT="In this section we attempt to indicate the nature of the general issue of high dimension&#xa0;" ID="ID_1313862291" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Privacy-Preserving Distributed Linear Regression on High " FOLDED="true" ID="ID_231700518" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://eprint.iacr.org/2016/892.pdf">
<node TEXT="High-Dimensional Data. Abstract: We propose privacy-preserving protocols for com- puting linear regression models in the setting where the train- ing dataset is&#xa0;" ID="ID_693518508" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Supervised learning: predicting an output variable from high " FOLDED="true" ID="ID_284881538" CREATED="1557224068600" MODIFIED="1557225479645" LINK="http://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html">
<node TEXT="All supervised estimators in scikit-learn implement a fit(X y) method to fit the . A solution in high-dimensional statistical learning is to shrink the regression&#xa0;" ID="ID_1071437746" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="high dimensional - Is Multiple Linear Regression in 3 dimensions a " FOLDED="true" ID="ID_299655845" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://stats.stackexchange.com/questions/226172/is-multiple-linear-regression-in-3-dimensions-a-plane-of-best-fit-or-a-line-of-b">
<node TEXT="Youre right the solution surface is going to be a hyperplane in general. Its just that the word hyperplane is a mouthful plane is shorter and&#xa0;" ID="ID_311087456" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Partially functional linear regression in high dimensions | Biometrika " FOLDED="true" ID="ID_1223627377" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://academic.oup.com/biomet/article/103/1/147/2389939">
<node TEXT="Jan 19 2016  We propose a unified framework that combines the regularization of each functional predictor as a whole with a penalty on high-dimensional&#xa0;" ID="ID_1034448879" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Can you recommend a model to perform regression with high " FOLDED="true" ID="ID_181549375" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://www.researchgate.net/post/Can_you_recommend_a_model_to_perform_regression_with_high_dimension_data">
<node TEXT="My data-set has 23377 instances for training (7792 for testing). The dimension of the data is approximately 28000. Each instance represents a document and the&#xa0;" ID="ID_657074896" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
</node>
</node>
<node TEXT="Regularization" ID="ID_1353679299" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Ridge" ID="ID_634568555" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Blog " ID="ID_1456541363" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Regularization Ridge#$D$#" FOLDED="true" ID="ID_1499117241" CREATED="1557224068600" MODIFIED="1557225483436">
<icon BUILTIN="stop-sign"/>
<node TEXT="Regularization: Ridge Lasso and Elastic Net (article) - DataCamp" FOLDED="true" ID="ID_1174637270" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net">
<node TEXT="Nov 29 2018  In this tutorial you will get acquainted with the bias-variance trade-off problem in linear regression and how it can be solved with regularization." ID="ID_1031697267" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Regularization Part 1: Ridge Regression - YouTube" FOLDED="true" ID="ID_1447039229" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://www.youtube.com/watch?v=Q81RR3yKn30">
<node TEXT="Sep 24 2018  Ridge Regression is a neat little way to ensure you dont overfit your training data - essentially you are desensitizing your model to the training&#xa0;" ID="ID_1131921391" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Tikhonov regularization - Wikipedia" FOLDED="true" ID="ID_1760786737" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://en.wikipedia.org/wiki/Tikhonov_regularization">
<node TEXT="Tikhonov regularization named for Andrey Tikhonov is the most commonly used method of regularization of ill-posed problems. In statistics the method is known as ridge regression in machine learning it&#xa0;" ID="ID_1870841796" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Regularization: Ridge Regression and the LASSO" FOLDED="true" ID="ID_1976512065" CREATED="1557224068600" MODIFIED="1557225479645" LINK="http://statweb.stanford.edu/~tibs/sta305files/Rudyregularization.pdf">
<node TEXT="Nov 29 2006  Part I. The Bias-Variance Tradeoff. Statistics 305: Autumn Quarter 2006/2007. Regularization: Ridge Regression and the LASSO&#xa0;" ID="ID_1125358780" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="L1 and L2 Regularization Methods &#x2013; Towards Data Science" FOLDED="true" ID="ID_1002485212" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c">
<node TEXT="Oct 13 2017  A regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression." ID="ID_174021645" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="sklearn.linear_model.Ridge &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_1719185556" CREATED="1557224068600" MODIFIED="1557225479645" LINK="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html">
<node TEXT="Also known as Ridge Regression or Tikhonov regularization. This estimator has built-in support for multi-variate regression (i.e. when y is a 2d-array of shape&#xa0;" ID="ID_1067456411" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="A comprehensive beginners guide for Linear Ridge and Lasso " FOLDED="true" ID="ID_922884106" CREATED="1557224068600" MODIFIED="1557225479645" LINK="https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/">
<node TEXT="Jun 22 2017  A comprehensive beginners guide for Linear Ridge and Lasso  Bias and Variance; Regularization; Ridge Regression; Lasso Regression&#xa0;" ID="ID_1345427812" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="The Math Behind Ridge Regularization and Lasso Regularization" FOLDED="true" ID="ID_525446247" CREATED="1557224068600" MODIFIED="1557225479661" LINK="https://medium.com/datadriveninvestor/the-math-behind-ridge-regularization-and-lasso-regularization-c4473332dbda">
<node TEXT="Oct 10 2018  This blog post is part 1 in a series about strategies to select and engineer quality features for supervised machine learning models. &#x201c;You are&#xa0;" ID="ID_1324999920" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="A Complete Tutorial on Ridge and Lasso Regression in Python" FOLDED="true" ID="ID_1412925758" CREATED="1557224068600" MODIFIED="1557225479661" LINK="https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/">
<node TEXT="Jan 28 2016  Here is a complete tutorial on the regularization techniques of ridge and lasso regression to prevent overfitting in prediction in python." ID="ID_968061506" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Regularization Ridge Regression" FOLDED="true" ID="ID_1168630447" CREATED="1557224068600" MODIFIED="1557225479661" LINK="https://courses.cs.washington.edu/courses/csep546/14wi/slides/regularization-xvalidation-lasso.pdf">
<node TEXT="Jan 13 2014  Regularization in Linear Regression  Regularized or penalized regression aims to impose a  Ridge Regression: Effect of Regularization. 14." ID="ID_1373001734" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
</node>
<node TEXT="Regularization Lasso#$D$#" FOLDED="true" ID="ID_567534877" CREATED="1557224068600" MODIFIED="1557225483436">
<icon BUILTIN="stop-sign"/>
<node TEXT="Lasso (statistics) - Wikipedia" FOLDED="true" ID="ID_1893797619" CREATED="1557224068600" MODIFIED="1557225479661" LINK="https://en.wikipedia.org/wiki/Lasso_(statistics)">
<node TEXT="In statistics and machine learning lasso is a regression analysis method that performs both variable selection and regularization in order to enhance the&#xa0;" ID="ID_1128680877" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="L1 and L2 Regularization Methods &#x2013; Towards Data Science" FOLDED="true" ID="ID_1420833357" CREATED="1557224068600" MODIFIED="1557225479661" LINK="https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c">
<node TEXT="Oct 13 2017  A regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression." ID="ID_1161920521" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Regularization Part 2: Lasso Regression - YouTube" FOLDED="true" ID="ID_1599882259" CREATED="1557224068600" MODIFIED="1557225479661" LINK="https://www.youtube.com/watch?v=NGf0voTMlcs">
<node TEXT="Oct 1 2018  Lasso Regression is super similar to Ridge Regression but there is one big huge difference between the two. In this video I start by talking&#xa0;" ID="ID_581843671" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Regularization: Ridge Lasso and Elastic Net (article) - DataCamp" FOLDED="true" ID="ID_179125409" CREATED="1557224068600" MODIFIED="1557225479661" LINK="https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net">
<node TEXT="Nov 29 2018  In this tutorial you will get acquainted with the bias-variance trade-off problem in linear regression and how it can be solved with regularization." ID="ID_1735570199" CREATED="1557224068600" MODIFIED="1557224068600"/>
</node>
<node TEXT="Regularization: Ridge Regression and the LASSO" FOLDED="true" ID="ID_292201055" CREATED="1557224068601" MODIFIED="1557225479661" LINK="http://statweb.stanford.edu/~tibs/sta305files/Rudyregularization.pdf">
<node TEXT="Nov 29 2006  Part I. The Bias-Variance Tradeoff. Statistics 305: Autumn Quarter 2006/2007. Regularization: Ridge Regression and the LASSO&#xa0;" ID="ID_26770299" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="A Complete Tutorial on Ridge and Lasso Regression in Python" FOLDED="true" ID="ID_1999699186" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/">
<node TEXT="Jan 28 2016  Here is a complete tutorial on the regularization techniques of ridge and lasso regression to prevent overfitting in prediction in python." ID="ID_1882435541" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Practical machine learning: Ridge Regression vs. Lasso" FOLDED="true" ID="ID_1233525189" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://hackernoon.com/practical-machine-learning-ridge-regression-vs-lasso-a00326371ece">
<node TEXT="Aug 28 2017  Lasso is another extension built on regularized linear regression but with  The only difference from Ridge regression is that the regularization&#xa0;" ID="ID_1082100591" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="A comprehensive beginners guide for Linear Ridge and Lasso " FOLDED="true" ID="ID_898689777" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/">
<node TEXT="Jun 22 2017  A comprehensive beginners guide for Linear Ridge and Lasso  Bias and Variance; Regularization; Ridge Regression; Lasso Regression&#xa0;" ID="ID_1131846278" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="terminology - Is regression with L1 regularization the same as Lasso " FOLDED="true" ID="ID_18860613" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://stats.stackexchange.com/questions/200416/is-regression-with-l1-regularization-the-same-as-lasso-and-with-l2-regularizati">
<node TEXT="LASSO is actually an acronym (least absolute shrinkage and selection  Ridge regression should probably be called Tikhonov regularization&#xa0;" ID="ID_1524659300" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="1.1. Generalized Linear Models &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_1162336182" CREATED="1557224068601" MODIFIED="1557225479661" LINK="http://scikit-learn.org/stable/modules/linear_model.html">
<node TEXT="This combination allows for learning a sparse model where few of the weights are non-zero like Lasso  while still maintaining the regularization properties of&#xa0;" ID="ID_1356033264" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
</node>
<node TEXT="Regularization ElasticNet#$D$#" FOLDED="true" ID="ID_1535644421" CREATED="1557224068601" MODIFIED="1557225483436">
<icon BUILTIN="stop-sign"/>
<node TEXT="Elastic net regularization - Wikipedia" FOLDED="true" ID="ID_194768878" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://en.wikipedia.org/wiki/Elastic_net_regularization">
<node TEXT="In statistics and in particular in the fitting of linear or logistic regression models the elastic net is a regularized regression method that linearly combines the L1&#xa0;" ID="ID_1244596286" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Regularization and variable selection via the elastic net" FOLDED="true" ID="ID_132304611" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://web.stanford.edu/~hastie/Papers/B67.2%20%282005%29%20301-320%20Zou%20%20Hastie.pdf">
<node TEXT="Summary. We propose the elastic net a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often&#xa0;" ID="ID_95542627" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Regularization: Ridge Lasso and Elastic Net (article) - DataCamp" FOLDED="true" ID="ID_1159200685" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net">
<node TEXT="Nov 29 2018  In this tutorial you will get acquainted with the bias-variance trade-off problem in linear regression and how it can be solved with regularization." ID="ID_1742114268" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Regularization Part 3: Elastic Net Regression - YouTube" FOLDED="true" ID="ID_1912116603" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://www.youtube.com/watch?v=1dKRdX9bfIo">
<node TEXT="Oct 8 2018  Elastic-Net Regression is combines Lasso Regression with Ridge Regression to give you the best of both worlds. It works well when there are&#xa0;" ID="ID_468715728" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Regularization and Variable Selection via the Elastic Net" FOLDED="true" ID="ID_1312292980" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://web.stanford.edu/~hastie/TALKS/enet_talk.pdf">
<node TEXT="ElasticNet. Hui Zou Stanford University. 2. Outline. &#x2022; Variable selection problem. &#x2022; Sparsity by regularization and the lasso. &#x2022; The elastic net&#xa0;" ID="ID_870736102" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Lasso Ridge and Elastic Net Regularization &#x2013; Jayesh Bapu Ahire " FOLDED="true" ID="ID_661969034" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://medium.com/@jayeshbahire/lasso-ridge-and-elastic-net-regularization-4807897cb722">
<node TEXT="Mar 20 2018  Regularization techniques in Generalized Linear Models (GLM) are used during a modeling process for many reasons. A regularization&#xa0;" ID="ID_276535880" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="regression - What is elastic net regularization and how does it solve " FOLDED="true" ID="ID_806982782" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://stats.stackexchange.com/questions/184029/what-is-elastic-net-regularization-and-how-does-it-solve-the-drawbacks-of-ridge">
<node TEXT="1. Which method is preferred? Yes elastic net is always preferred over lasso  ridge regression because it solves the limitations of both&#xa0;" ID="ID_343598472" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="What is the difference between Ridge Regression the LASSO and " FOLDED="true" ID="ID_241265895" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://blog.alexlenail.me/what-is-the-difference-between-ridge-regression-the-lasso-and-elasticnet-ec19c71c9028">
<node TEXT="Jul 31 2017  tldr: &#x201c;Ridge&#x201d; is a fancy name for L2-regularization &#x201c;LASSO&#x201d; means L1-regularization &#x201c;ElasticNet&#x201d; is a ratio of L1 and L2 regularization." ID="ID_166044359" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="sklearn.linear_model.ElasticNet &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_62556362" CREATED="1557224068601" MODIFIED="1557225479661" LINK="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html">
<node TEXT="ElasticNet (alpha=1.0 l1_ratio=0.5 fit_intercept=True normalize=False precompute=False max_iter=1000 . Number of alphas along the regularization path." ID="ID_1314681155" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Elastic-Net Regularization in Learning Theory" FOLDED="true" ID="ID_913546551" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://arxiv.org/abs/0807.3423">
<node TEXT="Jul 22 2008   we analyze in detail the so-called elastic-net regularization scheme  the elastic-net estimator is consistent not only for prediction but also for&#xa0;" ID="ID_1273479484" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
</node>
</node>
</node>
<node TEXT="Lasso" ID="ID_1824622633" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Blog " ID="ID_374422213" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
</node>
<node TEXT="ElasticNet" ID="ID_1263257695" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Blog " ID="ID_26432251" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
</node>
<node TEXT="Example Anecdote " ID="ID_630151500" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
<node TEXT="Code " ID="ID_1976797020" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
</node>
<node TEXT="Interactive Visualization " ID="ID_1176462349" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
</node>
<node TEXT="Robust Regression with Random Sample Consensus" ID="ID_682588513" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Robust Regression with Random Sample Consensus#$D$#" FOLDED="true" ID="ID_929415712" CREATED="1557224068601" MODIFIED="1557225483436">
<icon BUILTIN="stop-sign"/>
<node TEXT="Robust regression with random sample consensus - Machine " FOLDED="true" ID="ID_474349083" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781785889622/4/ch04lvl1sec34/robust-regression-with-random-sample-consensus">
<node TEXT="Robust regression with random sample consensusA common problem with linear regressions is caused by the presence of outliers. " ID="ID_1362083366" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Random sample consensus - Wikipedia" FOLDED="true" ID="ID_929446643" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://en.wikipedia.org/wiki/Random_sample_consensus">
<node TEXT="Random sample consensus (RANSAC) is an iterative method to estimate parameters of a  This is done by fitting linear models to several random samplings of the data and returning the model that has the best fit to a subset of the data." ID="ID_1199467098" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="sklearn.linear_model.RANSACRegressor &#x2014; scikit-learn 0.20.3 " FOLDED="true" ID="ID_1065323723" CREATED="1557224068601" MODIFIED="1557225479661" LINK="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html">
<node TEXT="RANSAC (RANdom SAmple Consensus) algorithm. RANSAC is an iterative algorithm for the robust estimation of parameters from a subset of inliers from the  Note that the current implementation only supports regression estimators." ID="ID_1268740879" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Robust regression with random sample consensus - Machine " FOLDED="true" ID="ID_365256121" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://www.oreilly.com/library/view/machine-learning-algorithms/9781785889622/a5cfbf2b-764b-42df-82b8-452f7f49a5af.xhtml">
<node TEXT="Robust regression with random sample consensus A common problem with linear regressions is caused by the presence of outliers. An ordinary least square&#xa0;" ID="ID_1271695448" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Using RANSAC for Robust Regression | Ensemble Blogging" FOLDED="true" ID="ID_878143713" CREATED="1557224068601" MODIFIED="1557225479661" LINK="http://hameddaily.blogspot.com/2015/04/using-ransac-for-robust-regression.html">
<node TEXT="Apr 11 2015  In this post I would like to touch the surface of outlier detection and removal by introducing Random Sample Consensus. RANSAC is a a&#xa0;" ID="ID_211288414" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Model-wise and point-wise random sample consensus for robust " FOLDED="true" ID="ID_1892819716" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://www.sciencedirect.com/science/article/pii/S0893608014001464">
<node TEXT="However the robustness gained from M-estimators is still low. This paper addresses robust regression and outlier detection in a random sample consensus&#xa0;" ID="ID_188045278" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Model-wise and point-wise random sample consensus for robust " FOLDED="true" ID="ID_22849153" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://www.ncbi.nlm.nih.gov/pubmed/25047916">
<node TEXT="Jul 7 2014  Model-wise and point-wise random sample consensus for robust regression and outlier detection. El-Melegy MT(1). Author information:" ID="ID_641569562" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Lecture 15 Robust Estimation : RANSAC" FOLDED="true" ID="ID_1099199342" CREATED="1557224068601" MODIFIED="1557225479661" LINK="http://www.cse.psu.edu/~rtc12/CSE486/lecture15.pdf">
<node TEXT="regression with outliers. Solution: Estimation methods that are robust to outliers.  Random. Sample Consensus: A Paradigm for Model Fitting with. Applications&#xa0;" ID="ID_162667168" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Robust regression using sparse learning for high dimensional " FOLDED="true" ID="ID_1909607390" CREATED="1557224068601" MODIFIED="1557225479661" LINK="http://ieeexplore.ieee.org/abstract/document/5495830/">
<node TEXT="Algorithms such as Least Median of Squares (LMedS) and Random Sample Consensus (RANSAC) have been very successful for low-dimensional robust&#xa0;" ID="ID_1340317748" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Robust Scale Estimation from Ensemble Inlier Sets for Random " FOLDED="true" ID="ID_314775609" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://link.springer.com/chapter/10.1007/978-3-540-88690-7_14">
<node TEXT="Fischler M.A. Bolles R.C.: Random sample consensus: A paradigm for  Subbarao R. Meer P.: Beyond RANSAC: User independent robust regression." ID="ID_1344179203" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
</node>
</node>
<node TEXT="Polynomial Regression" ID="ID_451550627" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Polynomial Regression#$D$#" FOLDED="true" ID="ID_474089743" CREATED="1557224068601" MODIFIED="1557225483436">
<icon BUILTIN="stop-sign"/>
<node TEXT="Polynomial regression - Wikipedia" FOLDED="true" ID="ID_1190711777" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://en.wikipedia.org/wiki/Polynomial_regression">
<node TEXT="In statistics polynomial regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is&#xa0;" ID="ID_1701187836" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Polynomial Regression &#x2013; Towards Data Science" FOLDED="true" ID="ID_603710522" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://towardsdatascience.com/polynomial-regression-bbe8b9d97491">
<node TEXT="Oct 8 2018  This is my third blog in the Machine Learning series. This blog requires prior knowledge of Linear Regression. If you dont know about Linear&#xa0;" ID="ID_1546163859" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="POLYNOMIAL REGRESSION - YouTube" FOLDED="true" ID="ID_1216306469" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://www.youtube.com/watch?v=Qnt2vBRW8Io">
<node TEXT="Oct 26 2017  In statistics polynomial regression is a form of regression analysis in which the relationship between the independent variable x and the&#xa0;" ID="ID_836852007" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Introduction to Linear Regression and Polynomial Regression" FOLDED="true" ID="ID_1403670981" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://towardsdatascience.com/introduction-to-linear-regression-and-polynomial-regression-f8adc96f31cb">
<node TEXT="Jan 13 2019  In this blog we will discuss two important topics that will form a base for Machine Learning which is &#x201c;Linear Regression&#x201d; and &#x201c;Polynomial&#xa0;" ID="ID_375631361" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Polynomial Regression - StatsDirect" FOLDED="true" ID="ID_1562735035" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://www.statsdirect.com/help/regression_and_correlation/polynomial.htm">
<node TEXT="where Y caret is the predicted outcome value for the polynomial model with regression coefficients b1 to k for each degree and Y intercept b0. The model is&#xa0;" ID="ID_339618024" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Regression Models:How do you know you need a polynomial? - The " FOLDED="true" ID="ID_1661672536" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://www.theanalysisfactor.com/regression-modelshow-do-you-know-you-need-a-polynomial/">
<node TEXT="A polynomial term&#x2013;a quadratic (squared) or cubic (cubed) term turns a linear regression model into a curve. But because it is X that is squared or cubed not the&#xa0;" ID="ID_1876998497" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="9.7 - Polynomial Regression | STAT 501" FOLDED="true" ID="ID_473201411" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://newonlinecourses.science.psu.edu/stat501/node/324/">
<node TEXT="9.7 - Polynomial Regression. Printer-friendly version. In our earlier discussions on multiple linear regression we have outlined ways to check assumptions of&#xa0;" ID="ID_1408983768" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Python | Implementation of Polynomial Regression - GeeksforGeeks" FOLDED="true" ID="ID_1534920878" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://www.geeksforgeeks.org/python-implementation-of-polynomial-regression/">
<node TEXT="Polynomial Regression is a form of linear regression in which the relationship between the independent variable x and dependent variable y is modeled as an&#xa0;" ID="ID_594511369" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Regression Tools - Online Polynomial Regression" FOLDED="true" ID="ID_1360447549" CREATED="1557224068601" MODIFIED="1557225479661" LINK="http://www.xuru.org/rt/pr.asp">
<node TEXT="This page allows performing polynomial regressions (polynomial least squares fittings). For the relation between two variables it finds the polynomial function&#xa0;" ID="ID_1584249477" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Chapter 12 Polynomial Regression Models Polynomial models in " FOLDED="true" ID="ID_699276137" CREATED="1557224068601" MODIFIED="1557225479661" LINK="http://home.iitk.ac.in/~shalab/regression/Chapter12-Regression-PolynomialRegression.pdf">
<node TEXT="is a polynomial regression model in one variable and is called as second  fitting of higher order polynomials can be a serious abuse of regression analysis." ID="ID_1419570713" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
</node>
</node>
<node TEXT="Isotonic Regression" ID="ID_692183878" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Isotonic Regression#$D$#" FOLDED="true" ID="ID_144814865" CREATED="1557224068601" MODIFIED="1557225483436">
<icon BUILTIN="stop-sign"/>
<node TEXT="Isotonic regression - Wikipedia" FOLDED="true" ID="ID_273301952" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://en.wikipedia.org/wiki/Isotonic_regression">
<node TEXT="In statistics isotonic regression or monotonic regression is the technique of fitting a free-form line to a sequence of observations under the following constraints:&#xa0;" ID="ID_1376033512" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Isotonic Regression &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_402833476" CREATED="1557224068601" MODIFIED="1557225479661" LINK="http://scikit-learn.org/stable/auto_examples/plot_isotonic_regression.html">
<node TEXT="An illustration of the isotonic regression on generated data. The isotonic regression finds a non-decreasing approximation of a function while minimizing the&#xa0;" ID="ID_761497103" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Isotonic regression | Stat Wiki | FANDOM powered by Wikia" FOLDED="true" ID="ID_214864405" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://stat.fandom.com/wiki/Isotonic_regression">
<node TEXT="Linear ordering isotonic regression can be understood as approximating given series of 1-dimensional observations with non-decreasing function. It is similar to&#xa0;" ID="ID_1699033802" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="1.15. Isotonic regression &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_1543247632" CREATED="1557224068601" MODIFIED="1557225479661" LINK="http://scikit-learn.org/stable/modules/isotonic.html">
<node TEXT="where each w i is strictly positive and each y i is an arbitrary real number. It yields the vector which is composed of non-decreasing elements the closest in terms&#xa0;" ID="ID_16292861" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Isotonic regression - RDD-based API - Spark 2.4.1 Documentation" FOLDED="true" ID="ID_1017879934" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://spark.apache.org/docs/latest/mllib-isotonic-regression.html">
<node TEXT="Isotonic regression belongs to the family of regression algorithms. Formally isotonic regression is a problem where given a finite set of real numbers Y=y1y2" ID="ID_89802771" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Nearly-Isotonic Regression" FOLDED="true" ID="ID_1309448945" CREATED="1557224068601" MODIFIED="1557225479661" LINK="http://www.stat.cmu.edu/~ryantibs/papers/neariso.pdf">
<node TEXT="Key words: isotonic regression pool adjacent violators path algorithm degrees  Isotonic regression solves the following problem: given a sequence of n data&#xa0;" ID="ID_1000936249" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Isotonic Regression" FOLDED="true" ID="ID_1720167812" CREATED="1557224068601" MODIFIED="1557225479661" LINK="http://fa.bianp.net/blog/2013/isotonic-regression/">
<node TEXT="Apr 16 2013  My latest contribution for scikit-learn is an implementation of the isotonic regression model that I coded with Nelle Varoquaux and Alexandre&#xa0;" ID="ID_1562067469" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Causal isotonic regression" FOLDED="true" ID="ID_1228210564" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://arxiv.org/abs/1810.03269">
<node TEXT="Oct 8 2018   our proposed estimation procedure generalizes the classical least-squares isotonic regression estimator of a monotone regression function." ID="ID_112289163" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Zhang : Risk bounds in isotonic regression" FOLDED="true" ID="ID_352620546" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://projecteuclid.org/euclid.aos/1021379864">
<node TEXT="Nonasymptotic risk bounds are provided for maximum likelihood-type isotonic estimators of an unknown nondecreasing regression function with general&#xa0;" ID="ID_1332118483" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Isotonic regression in general dimensions" FOLDED="true" ID="ID_1540368993" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://arxiv.org/abs/1708.09468">
<node TEXT="Aug 30 2017  Further we prove a sharp oracle inequality which reveals in particular that when the true regression function is piecewise constant on&#xa0;" ID="ID_1971781770" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
</node>
</node>
<node TEXT="Logistic Regression" FOLDED="true" ID="ID_1910396275" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Linear Classification" ID="ID_368186500" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Logistic Regression#$D$#" FOLDED="true" ID="ID_1381107734" CREATED="1557224068601" MODIFIED="1557225483436">
<icon BUILTIN="stop-sign"/>
<node TEXT="Logistic regression - Wikipedia" FOLDED="true" ID="ID_83615752" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://en.wikipedia.org/wiki/Logistic_regression">
<node TEXT="In statistics the logistic model (or logit model) is a widely used statistical model that in its basic form uses a logistic function to model a binary dependent variable&#xa0;" ID="ID_262742246" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Logistic Regression &#x2014; Detailed Overview &#x2013; Towards Data Science" FOLDED="true" ID="ID_554425531" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc">
<node TEXT="Mar 15 2018  Logistic Regression was used in the biological sciences in early twentieth century. It was then used in many social science applications." ID="ID_1483984760" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="What is Logistic Regression? - Statistics Solutions" FOLDED="true" ID="ID_749514" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://www.statisticssolutions.com/what-is-logistic-regression/">
<node TEXT="Like all regression analyses the logistic regression is a predictive analysis. Logistic regression is used to describe data and to explain the relationship between&#xa0;" ID="ID_100414559" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="sklearn.linear_model.LogisticRegression &#x2014; scikit-learn 0.20.3 " FOLDED="true" ID="ID_1301840289" CREATED="1557224068601" MODIFIED="1557225479661" LINK="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">
<node TEXT="This class implements regularized logistic regression using the liblinear library newton-cg sag and lbfgs solvers. It can handle both dense and sparse input&#xa0;" ID="ID_1727658910" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Logistic Regression &#x2014; ML Cheatsheet documentation" FOLDED="true" ID="ID_633256714" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html">
<node TEXT="Logistic regression is a classification algorithm used to assign observations to a discrete set of classes. Unlike linear regression which outputs continuous&#xa0;" ID="ID_543356884" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="StatQuest: Logistic Regression - YouTube" FOLDED="true" ID="ID_1569826318" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://www.youtube.com/watch?v=yIYKR4sgzI8">
<node TEXT="Mar 5 2018  Logistic regression is a traditional statistics technique that is also very popular as a machine learning tool. In this StatQuest I go over the main&#xa0;" ID="ID_1953396127" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Logistic Regression | Machine Learning Crash Course | Google " FOLDED="true" ID="ID_1193159632" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://developers.google.com/machine-learning/crash-course/logistic-regression/video-lecture">
<node TEXT="Mar 5 2019  Instead of predicting exactly 0 or 1 logistic regression generates a probability&#x2014;a value between 0 and 1 exclusive. For example consider a&#xa0;" ID="ID_1124325912" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Logistic Regression Analysis - an overview | ScienceDirect Topics" FOLDED="true" ID="ID_200123797" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://www.sciencedirect.com/topics/medicine-and-dentistry/logistic-regression-analysis">
<node TEXT="An advantage of logistic regression is that it allows the evaluation of multiple explanatory variables by extension of the basic principles. The general equation is." ID="ID_1021322472" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Logistic Regression for Machine Learning" FOLDED="true" ID="ID_606952679" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://machinelearningmastery.com/logistic-regression-for-machine-learning/">
<node TEXT="Apr 1 2016  Logistic regression is another technique borrowed by machine learning from the field of statistics. It is the go-to method for binary classification&#xa0;" ID="ID_765815657" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="Lesson 6: Logistic Regression | STAT 504" FOLDED="true" ID="ID_1513868491" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://newonlinecourses.science.psu.edu/stat504/node/149/">
<node TEXT="Binary Logistic Regression is a special type of regression where binary response variable is related to a set of explanatory variables which can be discrete&#xa0;" ID="ID_1519080650" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
</node>
</node>
<node TEXT="Implementation" ID="ID_631068756" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
<node TEXT="Optimizations" ID="ID_1640583517" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
</node>
<node TEXT="Stochastic Gradient Descent Algorithms" FOLDED="true" ID="ID_1119740706" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Finding the Optimal hyper-parameters through Grid Search" ID="ID_467208729" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Stochastic Gradient Descent Algorithms#$D$#" FOLDED="true" ID="ID_1320963416" CREATED="1557224068602" MODIFIED="1557225483437">
<icon BUILTIN="stop-sign"/>
<node TEXT="Stochastic gradient descent - Wikipedia" FOLDED="true" ID="ID_42958663" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">
<node TEXT="Stochastic gradient descent (often shortened to SGD) also known as incremental gradient .. models originally under the name ADALINE. Another stochastic gradient descent algorithm is the least mean squares (LMS) adaptive filter." ID="ID_295528884" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="An overview of gradient descent optimization algorithms" FOLDED="true" ID="ID_824581286" CREATED="1557224068602" MODIFIED="1557225479661" LINK="http://ruder.io/optimizing-gradient-descent/">
<node TEXT="Jan 19 2016  Gradient descent is one of the most popular algorithms to perform  Stochastic gradient descent (SGD) in contrast performs a parameter&#xa0;" ID="ID_206062363" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="Stochastic Gradient Descent with momentum &#x2013; Towards Data Science" FOLDED="true" ID="ID_99913295" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d">
<node TEXT="Dec 4 2017  Part 1 was about Stochastic gradient descent. In this post I presume basic knowledge about neural networks and gradient descent algorithm." ID="ID_1391413969" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="ML | Stochastic Gradient Descent (SGD) - GeeksforGeeks" FOLDED="true" ID="ID_555031337" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://www.geeksforgeeks.org/ml-stochastic-gradient-descent-sgd/">
<node TEXT="Before talking about Stochastic Gradient Descent (SGD) lets first understand  Deep Learning and it can be used with most if not all of the learning algorithms." ID="ID_1019731106" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="Difference between Batch Gradient Descent and Stochastic Gradient " FOLDED="true" ID="ID_1446964593" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://towardsdatascience.com/difference-between-batch-gradient-descent-and-stochastic-gradient-descent-1187f1291aa1">
<node TEXT="Sep 20 2017  Above algorithm says to perform the GD we need to calculate the gradient of the cost function J. And to calculate the gradient of the cost&#xa0;" ID="ID_1058741279" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="Stochastic Gradient Descent" FOLDED="true" ID="ID_524299203" CREATED="1557224068602" MODIFIED="1557225479661" LINK="http://deeplearning.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/">
<node TEXT="Stochastic Gradient Descent (SGD) addresses both of these issues by following the  The standard gradient descent algorithm updates the parameters &#x3b8; of the&#xa0;" ID="ID_382549482" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="1.5. Stochastic Gradient Descent &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_798140926" CREATED="1557224068602" MODIFIED="1557225479661" LINK="http://scikit-learn.org/stable/modules/sgd.html">
<node TEXT="Stochastic Gradient Descent (SGD) is a simple yet very efficient approach to . is available with Stochastic Average Gradient (SAG) algorithm available as a&#xa0;" ID="ID_328310024" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="Gradient Descent For Machine Learning" FOLDED="true" ID="ID_255954776" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://machinelearningmastery.com/gradient-descent-for-machine-learning/">
<node TEXT="Mar 23 2016  Gradient descent is an optimization algorithm used to find the . The learning can be much faster with stochastic gradient descent for very large&#xa0;" ID="ID_856044770" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="Gradient descent and stochastic gradient descent from scratch " FOLDED="true" ID="ID_1309721262" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://gluon.mxnet.io/chapter06_optimization/gd-sgd-scratch.html">
<node TEXT="Why doesnt the gradient descent algorithm get stuck on the way to a low loss? . To motivate the use of stochastic optimization algorithms note that when&#xa0;" ID="ID_1165990683" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="Uniform-in-Time Weak Error Analysis for Stochastic Gradient " FOLDED="true" ID="ID_59678504" CREATED="1557224068602" MODIFIED="1557225479661" LINK="http://arxiv.org/abs/1902.00635">
<node TEXT="Feb 2 2019   for Stochastic Gradient Descent Algorithms via Diffusion Approximation  error analysis of numerical stochastic differential equations into the&#xa0;" ID="ID_1596830530" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
</node>
<node TEXT="Stochastic Gradient Descent Algorithms Finding the Optimal hyper-parameters through Grid Search#$D$#" FOLDED="true" ID="ID_1353887158" CREATED="1557224068602" MODIFIED="1557225483437">
<icon BUILTIN="stop-sign"/>
<node TEXT="Hyper-parameter Tuning Techniques in Deep Learning &#x2013; Towards " FOLDED="true" ID="ID_1868940221" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8">
<node TEXT="Mar 16 2019  Before discussing the ways to find the optimal hyper-parameters let us first  In the gradient descent algorithm we start with random model  In grid search [3] we try every possible configuration of the parameters. Steps:." ID="ID_303035551" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="Hyperparameter optimization - Wikipedia" FOLDED="true" ID="ID_747760307" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://en.wikipedia.org/wiki/Hyperparameter_optimization">
<node TEXT="In machine learning hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning  Both parameters are continuous so to perform grid search one selects a finite set of reasonable&#xa0;" ID="ID_1391807625" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="Hyperparameter Tuning &#x2013; Towards Data Science" FOLDED="true" ID="ID_1003210161" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://towardsdatascience.com/hyperparameter-tuning-c5619e7e6624">
<node TEXT="Feb 16 2019   tuning is choosing a set of optimal hyperparameters for a learning algorithm&#x201d;.  In sklearn hyperparameters are passed in as arguments to the  Our top performing models here are logistic regression and stochastic gradient descent.  The benefit of grid search is that it is guaranteed to find the optimal&#xa0;" ID="ID_671515457" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="How to Grid Search Hyperparameters for Deep Learning Models in " FOLDED="true" ID="ID_781221845" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/">
<node TEXT="Aug 9 2016  Update Oct/2016: Updated examples for Keras 1.1.0 TensorFlow . This is not the best way to grid search because parameters can .. By far the most common optimization algorithm is plain old Stochastic Gradient Descent&#xa0;" ID="ID_1108315412" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="3.2. Tuning the hyper-parameters of an estimator &#x2014; scikit-learn 0.20 " FOLDED="true" ID="ID_201562616" CREATED="1557224068602" MODIFIED="1557225479661" LINK="http://scikit-learn.org/stable/modules/grid_search.html">
<node TEXT="Specifically to find the names and current values for all parameters for a given  The grid search provided by GridSearchCV exhaustively generates  Y. Random search for hyper-parameter optimization The Journal of Machine  Cross-validated Lasso using the LARS algorithm.  Gradient Boosting for classification." ID="ID_515936738" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="Random Search for Hyper-Parameter Optimization" FOLDED="true" ID="ID_1184749214" CREATED="1557224068602" MODIFIED="1557225479661" LINK="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf">
<node TEXT="manual search and grid search purely random search over the same  called hyper-parameters &#x3bb; and the actual learning algorithm is the one obtained  binations of values for the stochastic gradient descent learning rate of the supervised." ID="ID_1474876209" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="An introduction to high-dimensional hyper-parameter tuning" FOLDED="true" ID="ID_31014306" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://medium.freecodecamp.org/an-introduction-to-high-dimensional-hyper-parameter-tuning-df5c0106e5a4">
<node TEXT="Dec 13 2018  Hyper-parameter tuning refers to the problem of finding an optimal set of parameter values for a learning algorithm. Even for  Grid Search is usually a good choice when we have a small number of parameters to optimize. For two or . Machine Learning 101: An Intuitive Introduction to Gradient Descent" ID="ID_1554756910" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="1.5. Stochastic Gradient Descent &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_1855796682" CREATED="1557224068602" MODIFIED="1557225479661" LINK="http://scikit-learn.org/stable/modules/sgd.html">
<node TEXT="Stochastic Gradient Descent (SGD) is a simple yet very efficient approach to  classifiers in this module easily scale to problems with more than 10^5 training examples and  SGD requires a number of hyperparameters such as the regularization . with Stochastic Average Gradient (SAG) algorithm available as a solver in&#xa0;" ID="ID_1917099579" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="Finding the optimal hyperparameters through grid search - Machine " FOLDED="true" ID="ID_1861905545" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781785889622/5/ch05lvl1sec43/finding-the-optimal-hyperparameters-through-grid-search">
<node TEXT="Finding the optimal hyperparameters through grid searchFinding the best hyperparameters (called this  Stochastic gradient descent algorithms  As an example we show how to use it to find the best penalty and strength factors for a linear&#xa0;" ID="ID_1194087068" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="Intro to Model Tuning: Grid and Random Search | Kaggle" FOLDED="true" ID="ID_816563324" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search">
<node TEXT="Ill try to stick to using model hyperparameters or model settings and Ill point out when Im  use methods such as gradient descent Bayesian Optimization or evolutionary algorithms to conduct a guided search for the best hyperparameters." ID="ID_260873769" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
</node>
</node>
<node TEXT="Classification Metric" ID="ID_1659849781" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Stochastic Gradient Descent Algorithms Classification Metric#$D$#" FOLDED="true" ID="ID_297684779" CREATED="1557224068602" MODIFIED="1557225483437">
<icon BUILTIN="stop-sign"/>
<node TEXT="Scalable Large-Margin Distance Metric Learning Using Stochastic " FOLDED="true" ID="ID_1739823686" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://ieeexplore.ieee.org/document/8552662/">
<node TEXT="Nov 29 2018  The main challenge of distance metric learning is the positive  we develop an efficient algorithm based on a stochastic gradient descent.  metric learning approaches regarding classification accuracy and training time." ID="ID_340983662" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="1.5. Stochastic Gradient Descent &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_1253964629" CREATED="1557224068602" MODIFIED="1557225479661" LINK="http://scikit-learn.org/stable/modules/sgd.html">
<node TEXT="Stochastic Gradient Descent (SGD) is a simple yet very efficient approach to . SGDClassifier supports multi-class classification by combining multiple binary  is available with Stochastic Average Gradient (SAG) algorithm available as a&#xa0;" ID="ID_1164902695" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="Understand the Impact of Learning Rate on Model Performance With " FOLDED="true" ID="ID_1241086145" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/">
<node TEXT="Jan 25 2019  Stochastic gradient descent is an optimization algorithm that estimates the error . The ReduceLROnPlateau requires you to specify the metric to monitor . We will use a small multi-class classification problem as the basis to&#xa0;" ID="ID_1582701203" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="3.2.4.3.5. sklearn.ensemble.GradientBoostingClassifier &#x2014; scikit " FOLDED="true" ID="ID_1793571160" CREATED="1557224068602" MODIFIED="1557225479661" LINK="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html">
<node TEXT="Binary classification is a special case where only a single regression tree is induced.  For loss exponential gradient boosting recovers the AdaBoost algorithm.  If smaller than 1.0 this results in Stochastic Gradient Boosting. subsample .. In multi-label classification this is the subset accuracy which is a harsh metric&#xa0;" ID="ID_127512091" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="Stochastic Gradient Descent Algorithms - Fundamentals of Machine " FOLDED="true" ID="ID_140815535" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://subscription.packtpub.com/video/big_data_and_business_intelligence/9781789134377/55288/55290/stochastic-gradient-descent-algorithms">
<node TEXT="Mar 28 2018  Generate example using function make classification - Check the score - Use the perceptron class t.  Stochastic Gradient Descent Algorithms&#xa0;" ID="ID_342336582" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="Stochastic Gradient Descent Algorithm | Intel&#xae; Data Analytics " FOLDED="true" ID="ID_769025130" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://software.intel.com/en-us/daal-programming-guide-stochastic-gradient-descent-algorithm">
<node TEXT="Mar 7 2019  The stochastic gradient descent (SGD) algorithm is a special case of an iterative solver. For more details see Iterative Solver. The following&#xa0;" ID="ID_756268841" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="How to Control the Speed and Stability of Training Neural Networks " FOLDED="true" ID="ID_320009371" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/">
<node TEXT="Jan 21 2019  Batch Size and Gradient Descent; Stochastic Batch and Minibatch Gradient Descent in Keras; Multi-Class Classification Problem; MLP Fit  Optimization algorithms that use the entire training set are called batch or .. model.compile(loss=categorical_crossentropy optimizer=opt metrics=[accuracy])." ID="ID_1273763204" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="Scalable Large-Margin Distance Metric Learning Using Stochastic " FOLDED="true" ID="ID_515310911" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://www.researchgate.net/publication/329299351_Scalable_Large-Margin_Distance_Metric_Learning_Using_Stochastic_Gradient_Descent">
<node TEXT="Feb 19 2019  Extensive experiments show that the proposed algorithm is scalable to large  distance metric learning approaches regarding classification accuracy and training time. . Stochastic gradient descent with only one projection." ID="ID_809250888" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="5 Model Training and Tuning | The caret Package" FOLDED="true" ID="ID_1275920177" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://topepo.github.io/caret/model-training-and-tuning.html">
<node TEXT="The first step in tuning the model (line 1 in the algorithm below) is to choose a set of  Stochastic Gradient Boosting ## ## 157 samples ## 60 predictor ## 2 classes: M . for regression while accuracy and Kappa are computed for classification.  The metric argument of the train function allows the user to control which the&#xa0;" ID="ID_969845674" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="Machine Learning Glossary | Google Developers" FOLDED="true" ID="ID_1940400899" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://developers.google.com/machine-learning/glossary/">
<node TEXT="Jan 22 2019  A sophisticated gradient descent algorithm that rescales the gradients of  An evaluation metric that considers all possible classification thresholds. .. Similarly many variations of stochastic gradient descent have a high&#xa0;" ID="ID_1690783773" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
</node>
</node>
<node TEXT="ROC Curve" ID="ID_1063740574" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Stochastic Gradient Descent Algorithms ROC Curve#$D$#" FOLDED="true" ID="ID_1179998" CREATED="1557224068602" MODIFIED="1557225483437">
<icon BUILTIN="stop-sign"/>
<node TEXT="Machine Learning Glossary | Google Developers" FOLDED="true" ID="ID_47498996" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://developers.google.com/machine-learning/glossary/">
<node TEXT="Jan 22 2019  A sophisticated gradient descent algorithm that rescales the gradients of  The Area Under the ROC curve is the probability that a classifier will be more .. Similarly many variations of stochastic gradient descent have a high&#xa0;" ID="ID_734723994" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="Understanding Gradient Boosting Machines &#x2013; Towards Data Science" FOLDED="true" ID="ID_262006376" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://towardsdatascience.com/understanding-gradient-boosting-machines-9be756fe76ab">
<node TEXT="Nov 3 2018  The gradient boosting algorithm (gbm) can be most easily explained by  I have ensured that the same set of random numbers are generated every time. . positives I produced the ROC curve and calculated our AUC value." ID="ID_1600298497" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="Package gbm" FOLDED="true" ID="ID_344006581" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://cran.r-project.org/web/packages/gbm/gbm.pdf">
<node TEXT="Jan 14 2019  algorithm and Friedmans gradient boosting machine. Includes regression .. to the Area under the ROC Curve. : Fraction of concordant pairs;&#xa0;" ID="ID_1520771094" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="Optimising Area Under the ROC Curve Using Gradient Descent" FOLDED="true" ID="ID_1221347103" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://icml.cc/Conferences/2004/proceedings/papers/132.pdf">
<node TEXT="Optimising Area Under the ROC Curve Using Gradient Descent. Alan Herschtal. ALAN.  ROC curve (the AUC).  In this paper we introduce RankOpt an algorithm that op-  be expressed as the probability that for such a random ob-." ID="ID_512654375" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="21 Machine Learning Interview Questions and Answers" FOLDED="true" ID="ID_1779783060" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://elitedatascience.com/machine-learning-interview-questions-answers">
<node TEXT="Oct 9 2017  Algorithms for finding the best parameters for a model.  In stochastic gradient descent youll evaluate only 1 training sample for the set of . AUC is area under the ROC curve and its a common performance metric for&#xa0;" ID="ID_1535564868" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="Projects" FOLDED="true" ID="ID_787285088" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://hansfricke.github.io/projects.html">
<node TEXT="I challenged myself to go beyond applying familiar algorithms such as  learning algorithms I used random forests stochastic gradient descent with  The model parameters were determined using the area under the ROC curve in the grid&#xa0;" ID="ID_22877670" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="Stochastic Proximal Algorithms for AUC Maximization" FOLDED="true" ID="ID_1211450429" CREATED="1557224068602" MODIFIED="1557225479661" LINK="http://proceedings.mlr.press/v80/natole18a/natole18a.pdf">
<node TEXT="Abstract. Stochastic optimization algorithms such as stochastic gradient descent (SGD) update the  under the ROC curve (AUC) (Hanley  McNeil 1982;." ID="ID_414428688" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="Stratis Ioannidis - &#x3a3;&#x3c4;&#x3c1;&#x3b1;&#x3c4;&#x3ae;&#x3c2; &#x399;&#x3c9;&#x3b1;&#x3bd;&#x3bd;&#x3af;&#x3b4;&#x3b7;&#x3c2;: Teaching" FOLDED="true" ID="ID_1026471347" CREATED="1557224068602" MODIFIED="1557225479661" LINK="http://www.ece.neu.edu/fac-ece/ioannidis/EECE5698.html">
<node TEXT="This course covers the fundamentals of parallel machine learning algorithms tailored specifically to  ROC curves and AUC. Stochastic gradient descent." ID="ID_1069936035" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="CS 189/289A: Introduction to Machine Learning" FOLDED="true" ID="ID_1561570081" CREATED="1557224068602" MODIFIED="1557225479661" LINK="http://people.eecs.berkeley.edu/~jrs/189/">
<node TEXT="This class introduces algorithms for learning which constitute an important part of artificial  Lecture 3 (January 30): Gradient descent stochastic gradient descent and the  Optional: here is a fine short discussion of ROC curves&#x2014;but skip the&#xa0;" ID="ID_1475571264" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="Hilbert maps: scalable continuous occupancy mapping with " FOLDED="true" ID="ID_1437653590" CREATED="1557224068602" MODIFIED="1557225479661" LINK="http://www.roboticsproceedings.org/rss11/p02.pdf">
<node TEXT="trained and updated using stochastic gradient descent making the computation .. acteristic (ROC) curve for the four approaches (Hilbert maps with Fourier&#xa0;" ID="ID_1193978867" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
</node>
</node>
</node>
</node>
<node TEXT="Naive Bayes and Support Vector Machine" POSITION="left" ID="ID_986073883" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<edge COLOR="#00cc33"/>
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Naive Bayes" ID="ID_898218157" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Bayes Theorem" ID="ID_27345984" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="ADEPT" ID="ID_1391391514" CREATED="1564730305537" MODIFIED="1564730305537">
<node TEXT="A" ID="ID_1050588146" CREATED="1564730305537" MODIFIED="1564730305537">
<node TEXT="Analogy" ID="ID_1388225826" CREATED="1564730305537" MODIFIED="1564730305537"/>
</node>
<node TEXT="D" ID="ID_1700214812" CREATED="1564730305537" MODIFIED="1564730305537">
<node TEXT="Diagram" ID="ID_1172958616" CREATED="1564730305537" MODIFIED="1564730305537">
<node TEXT="Visualisation" ID="ID_1519144318" CREATED="1564730318758" MODIFIED="1564730321918">
<node TEXT="Bayes Theorem | Visual.ly" ID="ID_1777590169" CREATED="1557224068628" MODIFIED="1557224903579" LINK="https://visual.ly/community/infographic/education/bayes-theorem"/>
</node>
</node>
</node>
<node TEXT="E" ID="ID_1804961556" CREATED="1564730305537" MODIFIED="1564730305537">
<node TEXT="Example" ID="ID_1913084765" CREATED="1564730305537" MODIFIED="1564730305537">
<node TEXT="Bayes Examples" ID="ID_632242348" CREATED="1564734585100" MODIFIED="1576422626624" LINK="https://www.statisticshowto.datasciencecentral.com/bayes-theorem-problems/"/>
</node>
</node>
<node TEXT="P" ID="ID_523375773" CREATED="1564730305537" MODIFIED="1564730305537">
<node TEXT="Plain English" ID="ID_1102745401" CREATED="1564730305537" MODIFIED="1564730305537">
<node TEXT="An Intuitive (and Short) Explanation of Bayes Theorem | Better Explained" ID="ID_1300111454" CREATED="1557224068603" MODIFIED="1564730686188" LINK="https://betterexplained.com/articles/an-intuitive-and-short-explanation-of-bayes-theorem/"/>
</node>
</node>
<node TEXT="T" ID="ID_1830728639" CREATED="1564730305537" MODIFIED="1564730305537">
<node TEXT="Technical" ID="ID_1428989643" CREATED="1564730305537" MODIFIED="1564730305537">
<node TEXT="Colorized formula" ID="ID_1224449356" CREATED="1565261341851" MODIFIED="1565261395727" LINK="https://betterexplained.com/topics/Bayes_Theorem.png"/>
</node>
</node>
</node>
<node TEXT="Video" ID="ID_503191314" CREATED="1564730356847" MODIFIED="1564730358954">
<node TEXT="Conditional probability with Bayes Theorem (video) | Khan Academy" ID="ID_637566461" CREATED="1557224068603" MODIFIED="1557225479661" LINK="https://www.khanacademy.org/math/ap-statistics/probability-ap/stats-conditional-probability/v/bayes-theorem-visualized"/>
<node TEXT="Bayes Theorem | Crash Course" ID="ID_129684281" CREATED="1564730419951" MODIFIED="1585914000375" LINK="https://www.youtube.com/watch?v=9TDjifpGj-k&amp;t=3m30s"/>
</node>
</node>
<node TEXT="Naive Bayes Classifiers" FOLDED="true" ID="ID_758956338" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Naive Bayes#$D$#" FOLDED="true" ID="ID_1931599025" CREATED="1557224068602" MODIFIED="1557225483437">
<icon BUILTIN="stop-sign"/>
<node TEXT="Naive Bayes classifier - Wikipedia" FOLDED="true" ID="ID_1674569310" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">
<node TEXT="In machine learning naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes theorem with strong (naive) independence&#xa0;" ID="ID_110903721" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="1.9. Naive Bayes &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_1638932138" CREATED="1557224068602" MODIFIED="1557225479661" LINK="http://scikit-learn.org/stable/modules/naive_bayes.html">
<node TEXT="Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes theorem with the &#x201c;naive&#x201d; assumption of conditional independence&#xa0;" ID="ID_396755788" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="6 Easy Steps to Learn Naive Bayes Algorithm (with code in Python)" FOLDED="true" ID="ID_701140179" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/">
<node TEXT="Sep 11 2017  This article describes the basic principle behind Naive Bayes algorithm its application pros  cons along with its implementation in Python&#xa0;" ID="ID_879232369" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="Introduction to Naive Bayes Classification &#x2013; Towards Data Science" FOLDED="true" ID="ID_188782280" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://towardsdatascience.com/introduction-to-naive-bayes-classification-4cffabb1ae54">
<node TEXT="May 16 2018  Naive Bayes is a simple yet effective and commonly-used machine learning classifier. It is a probabilistic classifier that makes classifications&#xa0;" ID="ID_1671352786" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="Na&#xef;ve Bayes Classifier - Fun and Easy Machine Learning - YouTube" FOLDED="true" ID="ID_1590109918" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://www.youtube.com/watch?v=CPqOCI0ahss">
<node TEXT="Aug 26 2017  Naive Bayes Classifier- Fun and Easy Machine Learning &#x25bb;FREE YOLO GIFT - http://augmentedstartups.info/yolofreegiftsp &#x25bb;KERAS COURSE&#xa0;" ID="ID_153758326" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="Naive Bayes Classifier" FOLDED="true" ID="ID_1591805741" CREATED="1557224068602" MODIFIED="1557225479661" LINK="http://www.statsoft.com/textbook/naive-bayes-classifier">
<node TEXT="The Naive Bayes Classifier technique is based on the so-called Bayesian theorem and is particularly suited when the dimensionality of the inputs is high." ID="ID_1210890250" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="Naive Bayes for Machine Learning" FOLDED="true" ID="ID_1398849207" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://machinelearningmastery.com/naive-bayes-for-machine-learning/">
<node TEXT="Apr 11 2016  Naive Bayes is a simple but surprisingly powerful algorithm for predictive modeling. In this post you will discover the Naive Bayes algorithm for&#xa0;" ID="ID_1259896205" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="Naive Bayes - Georgia Tech - Machine Learning - YouTube" FOLDED="true" ID="ID_397296104" CREATED="1557224068603" MODIFIED="1557225479661" LINK="https://www.youtube.com/watch?v=M59h7CFUwPU">
<node TEXT="Feb 23 2015  Watch on Udacity: https://www.udacity.com/course/viewer#!/c-ud262/l-478818537/m-482228628 Check out the full Advanced Operating&#xa0;" ID="ID_469642790" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="Naive Bayesian" FOLDED="true" ID="ID_1113566084" CREATED="1557224068603" MODIFIED="1557225479661" LINK="https://www.saedsayad.com/naive_bayesian.htm">
<node TEXT="The Naive Bayesian classifier is based on Bayes theorem with the independence assumptions between predictors. A Naive Bayesian model is easy to build&#xa0;" ID="ID_1426346198" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="Naive Bayes Classifiers - GeeksforGeeks" FOLDED="true" ID="ID_1314860866" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://www.geeksforgeeks.org/naive-bayes-classifiers/">
<node TEXT="Naive Bayes classifiers are a collection of classification algorithms based on Bayes Theorem. It is not a single algorithm but a family of algorithms where all of&#xa0;" ID="ID_244597750" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
</node>
<node TEXT="Naive Bayes Classifiers#$D$#" FOLDED="true" ID="ID_743377860" CREATED="1557224068603" MODIFIED="1557225483437">
<icon BUILTIN="stop-sign"/>
<node TEXT="Naive Bayes classifier - Wikipedia" FOLDED="true" ID="ID_629645211" CREATED="1557224068603" MODIFIED="1557225479661" LINK="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">
<node TEXT="In machine learning naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes theorem with strong (naive) independence&#xa0;" ID="ID_1319240778" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="Naive Bayes Classifiers - GeeksforGeeks" FOLDED="true" ID="ID_1966935517" CREATED="1557224068603" MODIFIED="1557225479661" LINK="https://www.geeksforgeeks.org/naive-bayes-classifiers/">
<node TEXT="Naive Bayes classifiers are a collection of classification algorithms based on Bayes Theorem. It is not a single algorithm but a family of algorithms where all of&#xa0;" ID="ID_1917482097" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="Naive Bayes Classifier" FOLDED="true" ID="ID_602123734" CREATED="1557224068603" MODIFIED="1557225479661" LINK="http://www.statsoft.com/textbook/naive-bayes-classifier">
<node TEXT="The Naive Bayes Classifier technique is based on the so-called Bayesian theorem and is particularly suited when the dimensionality of the inputs is high." ID="ID_650465761" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="Na&#xef;ve Bayes Classifier - Fun and Easy Machine Learning - YouTube" FOLDED="true" ID="ID_1320267752" CREATED="1557224068603" MODIFIED="1557225479661" LINK="https://www.youtube.com/watch?v=CPqOCI0ahss">
<node TEXT="Aug 26 2017  Naive Bayes Classifier- Fun and Easy Machine Learning &#x25bb;FREE YOLO GIFT - http://augmentedstartups.info/yolofreegiftsp &#x25bb;KERAS COURSE&#xa0;" ID="ID_1837949378" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="6 Easy Steps to Learn Naive Bayes Algorithm (with code in Python)" FOLDED="true" ID="ID_455958021" CREATED="1557224068603" MODIFIED="1557225479661" LINK="https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/">
<node TEXT="Sep 11 2017  In simple terms a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other&#xa0;" ID="ID_1036897213" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="Introduction to Naive Bayes Classification &#x2013; Towards Data Science" FOLDED="true" ID="ID_805188346" CREATED="1557224068603" MODIFIED="1557225479661" LINK="https://towardsdatascience.com/introduction-to-naive-bayes-classification-4cffabb1ae54">
<node TEXT="May 16 2018  Naive Bayes is a simple yet effective and commonly-used machine learning classifier. It is a probabilistic classifier that makes classifications&#xa0;" ID="ID_1121758043" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="1.9. Naive Bayes &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_1082350223" CREATED="1557224068603" MODIFIED="1557225479661" LINK="http://scikit-learn.org/stable/modules/naive_bayes.html">
<node TEXT="Naive Bayes methods are a set of supervised learning algorithms based on . The different naive Bayes classifiers differ mainly by the assumptions they make&#xa0;" ID="ID_1397728150" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="Naive Bayes Classifier &#x2013; Towards Data Science" FOLDED="true" ID="ID_1954282568" CREATED="1557224068603" MODIFIED="1557225479661" LINK="https://towardsdatascience.com/naive-bayes-classifier-81d512f50a7c">
<node TEXT="May 5 2018  A classifier is a machine learning model that is used to discriminate different objects based on certain features. A Naive Bayes classifier is a&#xa0;" ID="ID_527881041" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="Naive Bayes for Machine Learning" FOLDED="true" ID="ID_1098013204" CREATED="1557224068603" MODIFIED="1557225479661" LINK="https://machinelearningmastery.com/naive-bayes-for-machine-learning/">
<node TEXT="Apr 11 2016  Naive Bayes Classifier. Naive Bayes is a classification algorithm for binary (two-class) and multi-class classification problems. The technique is&#xa0;" ID="ID_396818099" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="Tutorial: Building a Text Classification System &#x2014; TextBlob 0.15.2 " FOLDED="true" ID="ID_45956777" CREATED="1557224068603" MODIFIED="1557225479661" LINK="https://textblob.readthedocs.io/en/dev/classifiers.html">
<node TEXT="The textblob.classifiers module makes it simple to create custom classifiers.  Now well create a Naive Bayes classifier passing the training data into the&#xa0;" ID="ID_1555370547" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
</node>
<node TEXT="Naive User based systems#$D$#" FOLDED="true" ID="ID_1349761713" CREATED="1557224068611" MODIFIED="1557225483445">
<icon BUILTIN="stop-sign"/>
<node TEXT="Naive user-based systems - Machine Learning Algorithms" FOLDED="true" ID="ID_1114061712" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781785889622/11/ch11lvl1sec71/naive-user-based-systems">
<node TEXT="Naive user-based systemsIn this first scenario we assume that we have a set of users represented by feature vectors:" ID="ID_106482095" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Naive user-based systems - Machine Learning Algorithms [Book]" FOLDED="true" ID="ID_1473419947" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://www.oreilly.com/library/view/machine-learning-algorithms/9781785889622/cc56897a-011e-4204-9010-a29f44a4b8a7.xhtml">
<node TEXT="Naive user-based systems In this first scenario we assume that we have a set of users represented by feature vectors: Typical features are age gender interests&#xa0;" ID="ID_908775217" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Naive user-based systems - Machine Learning Algorithms - Second " FOLDED="true" ID="ID_120946306" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781789347999/12/ch12lvl1sec81/naive-user-based-systems">
<node TEXT="Naive user-based systemsIn this first scenario we assume that we have " ID="ID_114951026" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Authenticating Users of Recommender Systems Using Naive Bayes " FOLDED="true" ID="ID_1998066518" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://link.springer.com/chapter/10.1007/978-3-642-41230-1_17">
<node TEXT="Knowledge Based Authentication (KBA) verifies the credibility of claimed identities by matching various user-related data. Popular recommender systems hold&#xa0;" ID="ID_1582576772" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="new recommender system using naive bayes for e- learning" FOLDED="true" ID="ID_1154087706" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://www.isres.org/books/chapters/RHES2016-8_10-09-2017.pdf">
<node TEXT="Computer-based recommender systems are the most appropriate methods in  and eventually the system recommends to user with Na&#xef;ve Bayesian Classifier." ID="ID_1775923863" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="User Preference Awareness in City Traveler Helper System Based " FOLDED="true" ID="ID_222604314" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://ieeexplore.ieee.org/document/4609586/">
<node TEXT="User Preference Awareness in City Traveler Helper System Based on Na&#xef;ve Bayes Classification. Abstract: City traveler helper system (Wang and Qi 2008)&#xa0;" ID="ID_1777250920" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Movement imagery classification in EMOTIV cap based system by " FOLDED="true" ID="ID_1511318435" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://www.ncbi.nlm.nih.gov/pubmed/28269262">
<node TEXT="Movement imagery classification in EMOTIV cap based system by Na&#xef;ve Bayes.  in assistive technology which do not require motor activity from the user." ID="ID_1202045521" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Targeted Help for Spoken Dialogue Systems: intelligent feedback " FOLDED="true" ID="ID_494917044" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://www.aclweb.org/anthology/E03-1075">
<node TEXT="intelligent feedback improves naive users performance. Beth Ann  are out-of-coverage of the main dialogue system . We have designed a rule based system." ID="ID_1904080112" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="TIIS - Enhanced Cloud Service Discovery for Na&#xef;ve users with " FOLDED="true" ID="ID_831709598" CREATED="1557224068611" MODIFIED="1557225479739" LINK="http://www.itiis.org/digital-library/manuscript/1190">
<node TEXT="Jan 31 2016  KSII Transactions on Internet and Information Systems Monthly  Enhanced Cloud Service Discovery for Na&#xef;ve users with Ontology based Representation  to find services especially from Internet-based service repositories." ID="ID_1146314771" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Incorporating User Control into Recommender Systems Based on " FOLDED="true" ID="ID_738420370" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://www.macs.hw.ac.uk/~dwcorne/ACMRecSys07/p73-pronk.pdf">
<node TEXT="Oct 20 2007  Keywords classification machine learning naive Bayes recommender user  A recommender learns the taste of a user based on ratings that." ID="ID_1743946552" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
</node>
</node>
<node TEXT="Naive Bayes in Scikit-learn" FOLDED="true" ID="ID_151252490" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Naive Bayes in Scikit-learn#$D$#" FOLDED="true" ID="ID_948548147" CREATED="1557224068603" MODIFIED="1557225483438">
<icon BUILTIN="stop-sign"/>
<node TEXT="1.9. Naive Bayes &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_205642850" CREATED="1557224068603" MODIFIED="1557225479661" LINK="http://scikit-learn.org/stable/modules/naive_bayes.html">
<node TEXT="Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes theorem with the &#x201c;naive&#x201d; assumption of conditional independence&#xa0;" ID="ID_306156652" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="Naive Bayes Classification using Scikit-learn (article) - DataCamp" FOLDED="true" ID="ID_231750482" CREATED="1557224068603" MODIFIED="1557225479661" LINK="https://www.datacamp.com/community/tutorials/naive-bayes-scikit-learn">
<node TEXT="Dec 4 2018  Learn how to build and evaluate a Naive Bayes Classifier using Pythons Scikit-learn package." ID="ID_1661848895" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="sklearn.naive_bayes.GaussianNB &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_574683196" CREATED="1557224068603" MODIFIED="1557225479661" LINK="http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html">
<node TEXT="Examples using sklearn.naive_bayes.  Gaussian Naive Bayes (GaussianNB)  fit (X y[ sample_weight]) Fit Gaussian Naive Bayes according to X y." ID="ID_1915620030" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="Naive Bayes Classification with Sklearn &#x2013; Sicaras blog" FOLDED="true" ID="ID_451459408" CREATED="1557224068603" MODIFIED="1557225479661" LINK="https://blog.sicara.com/naive-bayes-classifier-sklearn-python-example-tips-42d100429e44">
<node TEXT="Feb 28 2018  This tutorial details Naive Bayes classifier algorithm its principle pros  cons and provide an example using the Sklearn python Library." ID="ID_1903050740" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="sklearn.naive_bayes.MultinomialNB &#x2014; scikit-learn 0.20.3 " FOLDED="true" ID="ID_893995945" CREATED="1557224068603" MODIFIED="1557225479661" LINK="http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html">
<node TEXT="Naive Bayes classifier for multinomial models. The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g. word counts for text&#xa0;" ID="ID_1884507585" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="The Naive Bayes Algorithm in Python with Scikit-Learn" FOLDED="true" ID="ID_1737482649" CREATED="1557224068603" MODIFIED="1557225479661" LINK="https://stackabuse.com/the-naive-bayes-algorithm-in-python-with-scikit-learn/">
<node TEXT="Jul 10 2018  The Naive Bayes Classifier brings the power of this theorem to Machine Learning building a very simple yet powerful classifier. In this article&#xa0;" ID="ID_1212606186" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="sklearn.naive_bayes.BernoulliNB &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_1421395898" CREATED="1557224068603" MODIFIED="1557225479661" LINK="http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html">
<node TEXT="Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB this classifier is suitable for discrete data. The difference is that while MultinomialNB&#xa0;" ID="ID_516131901" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="scikit-learn/naive_bayes.py at master &#xb7; scikit-learn/scikit-learn &#xb7; GitHub" FOLDED="true" ID="ID_788596366" CREATED="1557224068603" MODIFIED="1557225479661" LINK="https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/naive_bayes.py">
<node TEXT="The :mod:`sklearn.naive_bayes` module implements Naive Bayes algorithms. These. are supervised learning methods based on applying Bayes theorem with&#xa0;" ID="ID_1044791896" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="Implementing 3 Naive Bayes classifiers in scikit-learn | Packt Hub" FOLDED="true" ID="ID_1149404276" CREATED="1557224068603" MODIFIED="1557225479661" LINK="https://hub.packtpub.com/implementing-3-naive-bayes-classifiers-in-scikit-learn/">
<node TEXT="May 7 2018  Scikit-learn provide three naive Bayes classifiers: Bernoulli multinomial and Gaussian. The only difference is about the probability distribution&#xa0;" ID="ID_891960497" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="Machine Learning Algorithm Recipes in scikit-learn" FOLDED="true" ID="ID_1423823238" CREATED="1557224068603" MODIFIED="1557225479661" LINK="https://machinelearningmastery.com/get-your-hands-dirty-with-scikit-learn-now/">
<node TEXT="Jun 20 2014  The scikit-learn Python library is very easy to get up and running.  Naive Bayes uses Bayes Theorem to model the conditional relationship of&#xa0;" ID="ID_1068729464" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
</node>
</node>
<node TEXT="Types" FOLDED="true" ID="ID_1887900412" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Bernoulli Naive Bayes" ID="ID_1178685457" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Bernoulli Naive Bayes#$D$#" FOLDED="true" ID="ID_700881619" CREATED="1557224068603" MODIFIED="1557225483438">
<icon BUILTIN="stop-sign"/>
<node TEXT="sklearn.naive_bayes.BernoulliNB &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_1109624203" CREATED="1557224068603" MODIFIED="1557225479661" LINK="http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html">
<node TEXT="Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB this classifier is suitable for discrete data. The difference is that while MultinomialNB&#xa0;" ID="ID_297845952" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="Naive Bayes classifier - Wikipedia" FOLDED="true" ID="ID_402590262" CREATED="1557224068603" MODIFIED="1557225479661" LINK="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">
<node TEXT="In the multivariate Bernoulli event model features are  Note that a naive Bayes classifier with a Bernoulli event model is&#xa0;" ID="ID_1128411332" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="The Bernoulli model" FOLDED="true" ID="ID_1599969301" CREATED="1557224068603" MODIFIED="1557225479661" LINK="https://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html">
<node TEXT="Next: Properties of Naive Bayes Up: Text classification and Naive Previous:  An alternative to the multinomial model is the multivariate Bernoulli model or&#xa0;" ID="ID_1843172952" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="1.9. Naive Bayes &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_342136839" CREATED="1557224068603" MODIFIED="1557225479661" LINK="http://scikit-learn.org/stable/modules/naive_bayes.html">
<node TEXT="Bernoulli Naive Bayes; 1.9.5. Out-of-core naive Bayes model fitting  Naive Bayes methods are a set of supervised learning algorithms based on applying&#xa0;" ID="ID_170957012" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="machine learning - When to use Bernoulli Naive Bayes? - Cross " FOLDED="true" ID="ID_224506654" CREATED="1557224068603" MODIFIED="1557225479661" LINK="https://stats.stackexchange.com/questions/246101/when-to-use-bernoulli-naive-bayes">
<node TEXT="Bernoulli Naive Bayes is for binary features only. Similarly multinomial naive Bayes treats features as event probabilities. Your example is&#xa0;" ID="ID_1883786544" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="Bernoulli Naive Bayes Classifier" FOLDED="true" ID="ID_844411536" CREATED="1557224068603" MODIFIED="1557225479661" LINK="https://mattshomepage.com/articles/2016/Jun/07/bernoulli_nb/">
<node TEXT="Jun 7 2016  Covers theory and implementation of a Bernoulli naive Bayes classifier." ID="ID_1451648291" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="Naive Bayes Classifier - Multinomial Bernoulli Gaussian Using " FOLDED="true" ID="ID_1858654552" CREATED="1557224068603" MODIFIED="1557225479661" LINK="https://www.youtube.com/watch?v=99MN-rl8jGY">
<node TEXT="Sep 18 2017  In this Python for Data Science tutorial You will learn about Naive Bayes classifier (Multinomial Bernoulli Gaussian) using scikit learn and Urllib&#xa0;" ID="ID_1935516100" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="Na&#xef;ve Bayes" FOLDED="true" ID="ID_930542072" CREATED="1557224068603" MODIFIED="1557225479661" LINK="https://www.cs.cmu.edu/~mgormley/courses/10701-f16/slides/lecture3.pdf">
<node TEXT="Sep 14 2016  Na&#xef;ve Bayes Assumption. &#x2013; Model 1: Bernoulli Na&#xef;ve Bayes. &#x2013; Model 2: Multinomial Na&#xef;ve Bayes. &#x2013; Model 3: Gaussian Na&#xef;ve Bayes." ID="ID_1054880316" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="CS340 Machine learning Na&#xef;ve Bayes classifiers" FOLDED="true" ID="ID_6203007" CREATED="1557224068603" MODIFIED="1557225479661" LINK="https://www.cs.ubc.ca/~murphyk/Teaching/CS340-Fall07/NB.pdf">
<node TEXT="Bayes rule for classifiers p(y = c|x) =  independently (naive Bayes assumption). &#x2022; E.g. prob of  focus on the multivariate Bernoulli (binary features) model for&#xa0;" ID="ID_1422921041" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="Difference between Bernoulli and Multinomial Naive Bayes - Data " FOLDED="true" ID="ID_1889882197" CREATED="1557224068603" MODIFIED="1557225479661" LINK="https://datascience.stackexchange.com/questions/27624/difference-between-bernoulli-and-multinomial-naive-bayes">
<node TEXT="Bernoulli models the presence/absence of a feature. Multinomial models the number of counts of a feature. Heres a concise explanation." ID="ID_1782765562" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
</node>
</node>
<node TEXT="Multinomial Naive Bayes" ID="ID_827100250" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Multinomial Naive Bayes#$D$#" FOLDED="true" ID="ID_412309271" CREATED="1557224068603" MODIFIED="1557225483438">
<icon BUILTIN="stop-sign"/>
<node TEXT="sklearn.naive_bayes.MultinomialNB &#x2014; scikit-learn 0.20.3 " FOLDED="true" ID="ID_1118584997" CREATED="1557224068603" MODIFIED="1557225479661" LINK="http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html">
<node TEXT="The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g. word counts for text classification). The multinomial distribution&#xa0;" ID="ID_1231097154" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="Naive Bayes classifier - Wikipedia" FOLDED="true" ID="ID_150722619" CREATED="1557224068603" MODIFIED="1557225479661" LINK="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">
<node TEXT="With a multinomial event model samples (feature vectors) represent the frequencies with which certain events have&#xa0;" ID="ID_87468608" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="Multinomial Naive Bayes Classifier for Text Analysis (Python)" FOLDED="true" ID="ID_1698223045" CREATED="1557224068603" MODIFIED="1557225479677" LINK="https://towardsdatascience.com/multinomial-naive-bayes-classifier-for-text-analysis-python-8dd6825ece67">
<node TEXT="Apr 9 2018  In this blog I will cover how you can implement a Multinomial Naive Bayes Classifier for the 20 Newsgroups dataset. The 20 newsgroups&#xa0;" ID="ID_622348314" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="bayesian - Difference between naive Bayes  multinomial naive " FOLDED="true" ID="ID_796238031" CREATED="1557224068603" MODIFIED="1557225479677" LINK="https://stats.stackexchange.com/questions/33185/difference-between-naive-bayes-multinomial-naive-bayes">
<node TEXT="The general term Naive Bayes refers the the strong independence assumptions in the model rather than the particular distribution of each&#xa0;" ID="ID_417631372" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="Applying Multinomial Naive Bayes to NLP Problems - GeeksforGeeks" FOLDED="true" ID="ID_340068158" CREATED="1557224068603" MODIFIED="1557225479677" LINK="https://www.geeksforgeeks.org/applying-multinomial-naive-bayes-to-nlp-problems/">
<node TEXT="Naive Bayes Classifier Algorithm is a family of probabilistic algorithms based on applying Bayes theorem with the &#x201c;naive&#x201d; assumption of conditional&#xa0;" ID="ID_1487348245" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="Document Classification Using Multinomial Naive Bayes Classifier" FOLDED="true" ID="ID_1763701142" CREATED="1557224068603" MODIFIED="1557225479677" LINK="https://www.3pillarglobal.com/insights/document-classification-using-multinomial-naive-bayes-classifier">
<node TEXT="A machine learning technique for using the Multinomial Naive Bayes algorithms to classify certain documents with specific keywords." ID="ID_1926165077" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="6 - 6 - Multinomial Naive Bayes- A Worked Example .mp4 - YouTube" FOLDED="true" ID="ID_623889698" CREATED="1557224068603" MODIFIED="1557225479677" LINK="https://www.youtube.com/watch?v=km2LoOpdB3A">
<node TEXT="Apr 6 2012  6 - 6 - Multinomial Naive Bayes- A Worked Example .mp4. Rafael Merino Garc&#xed;a. Loading Unsubscribe from Rafael Merino Garc&#xed;a? Cancel" ID="ID_1561877858" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="Applying Multinomial Naive Bayes to NLP Problems: A Practical " FOLDED="true" ID="ID_497345406" CREATED="1557224068603" MODIFIED="1557225479677" LINK="https://medium.com/syncedreview/applying-multinomial-naive-bayes-to-nlp-problems-a-practical-explanation-4f5271768ebf">
<node TEXT="Jul 17 2017  1.Introduction Naive Bayes is a family of algorithms based on applying Bayes theorem with a strong(naive) assumption that every feature is&#xa0;" ID="ID_441135984" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="How does multinomial Naive Bayes work? - Quora" FOLDED="true" ID="ID_41198304" CREATED="1557224068603" MODIFIED="1557225479677" LINK="https://www.quora.com/How-does-multinomial-Naive-Bayes-work">
<node TEXT="Check out this Naive Bayes Tutorial. It describes 3 Naive Bayes models (Multinomial Binarized and Benoulli) in the context of Text Classification. Note that&#xa0;" ID="ID_52772442" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="Naive Bayes text classification" FOLDED="true" ID="ID_1015436646" CREATED="1557224068603" MODIFIED="1557225479677" LINK="https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html">
<node TEXT="The first supervised learning method we introduce is the multinomial Naive Bayes or multinomial NB model a probabilistic learning method. The probability of a&#xa0;" ID="ID_1562057620" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
</node>
</node>
<node TEXT="Gaussian Naive Bayes" ID="ID_1007021915" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Gaussian Naive Bayes#$D$#" FOLDED="true" ID="ID_1954517592" CREATED="1557224068603" MODIFIED="1557225483438">
<icon BUILTIN="stop-sign"/>
<node TEXT="sklearn.naive_bayes.GaussianNB &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_1647924165" CREATED="1557224068603" MODIFIED="1557225479677" LINK="http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html">
<node TEXT="Examples using sklearn.naive_bayes.  Gaussian Naive Bayes (GaussianNB)  fit (X y[ sample_weight]) Fit Gaussian Naive Bayes according to X y." ID="ID_1754517328" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="Naive Bayes classifier - Wikipedia" FOLDED="true" ID="ID_218213131" CREATED="1557224068603" MODIFIED="1557225479677" LINK="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">
<node TEXT=" distributed according to a Gaussian distribution. For example suppose the training data contains a continuous attribute&#xa0;" ID="ID_383900606" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="Naive Bayes for Machine Learning" FOLDED="true" ID="ID_700223067" CREATED="1557224068603" MODIFIED="1557225479677" LINK="https://machinelearningmastery.com/naive-bayes-for-machine-learning/">
<node TEXT="Apr 11 2016  Naive Bayes is a simple but surprisingly powerful algorithm for predictive . This extension of naive Bayes is called Gaussian Naive Bayes." ID="ID_1993760459" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="1.9. Naive Bayes &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_154197280" CREATED="1557224068603" MODIFIED="1557225479677" LINK="http://scikit-learn.org/stable/modules/naive_bayes.html">
<node TEXT="Gaussian Naive Bayes; 1.9.2. Multinomial Naive Bayes; 1.9.3. Complement Naive Bayes; 1.9.4. Bernoulli Naive Bayes; 1.9.5. Out-of-core naive Bayes model&#xa0;" ID="ID_1735268797" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="Naive Bayes 3: Gaussian example - YouTube" FOLDED="true" ID="ID_910270372" CREATED="1557224068603" MODIFIED="1557225479677" LINK="https://www.youtube.com/watch?v=r1in0YNetG8">
<node TEXT="Jan 15 2014  http://bit.ly/N-Bayes] How can we use Naive Bayes classifier with continuous (real-valued) attributes? We estimate the priors and the means&#xa0;" ID="ID_11763341" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="Gaussian Na&#xef;ve Bayes and Logistic Regression" FOLDED="true" ID="ID_1158793769" CREATED="1557224068603" MODIFIED="1557225479677" LINK="https://www.cs.cmu.edu/~epxing/Class/10701-10s/Lecture/lecture5.pdf">
<node TEXT="Jan 25 2010  Gaussian Na&#xef;ve Bayes and. Logistic Regression. Machine Learning 10-701. Tom M. Mitchell. Machine Learning Department. Carnegie Mellon&#xa0;" ID="ID_96099623" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="IAML5.9: Gaussian Naive Bayes classifier - YouTube" FOLDED="true" ID="ID_770000882" CREATED="1557224068603" MODIFIED="1557225479677" LINK="https://www.youtube.com/watch?v=TcAQKPgymLE">
<node TEXT="Sep 10 2015  IAML5.9: Gaussian Naive Bayes classifier. Victor Lavrenko. Loading Unsubscribe from Victor Lavrenko? Cancel Unsubscribe. Working." ID="ID_1339439576" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="Implementation of Gaussian Naive Bayes in Python from scratch" FOLDED="true" ID="ID_972609173" CREATED="1557224068603" MODIFIED="1557225479677" LINK="https://hackernoon.com/implementation-of-gaussian-naive-bayes-in-python-from-scratch-c4ea64e3944d">
<node TEXT="Jan 23 2019  Gaussian Naive Bayes is an algorithm having a Probabilistic Approach. It involves prior and posterior probability calculation of the classes in&#xa0;" ID="ID_1848933879" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="Naive Bayes Classifiers - GeeksforGeeks" FOLDED="true" ID="ID_1677495924" CREATED="1557224068603" MODIFIED="1557225479677" LINK="https://www.geeksforgeeks.org/naive-bayes-classifiers/">
<node TEXT="Naive Bayes classifiers are a collection of classification algorithms based on . Now we look at an implementation of Gaussian Naive Bayes classifier using&#xa0;" ID="ID_1536629625" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="Gaussian Naive Bayes | Machine Learning Deep Learning and " FOLDED="true" ID="ID_1787633855" CREATED="1557224068603" MODIFIED="1557225479677" LINK="http://www.ritchieng.com/machine-learning-gaussian-naive-bayes/">
<node TEXT="Bayes Rule: Intuitive Explanation. (Prior probability)(Test evidence) -- (Posterior probability); Example. P(C) = 0.01; 90% it is positive if you have C (Sensitivity)&#xa0;" ID="ID_1943797647" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
</node>
</node>
</node>
<node TEXT="Interactive Visualization " ID="ID_701767515" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
</node>
<node TEXT="Support Vector Machine" ID="ID_1490097736" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Linear Support Vector Machines" FOLDED="true" ID="ID_100618802" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Support Vector Machine#$D$#" FOLDED="true" ID="ID_1915441900" CREATED="1557224068603" MODIFIED="1557225483438">
<icon BUILTIN="stop-sign"/>
<node TEXT="Support-vector machine - Wikipedia" FOLDED="true" ID="ID_1384766945" CREATED="1557224068603" MODIFIED="1557225479677" LINK="https://en.wikipedia.org/wiki/Support-vector_machine">
<node TEXT="In machine learning support-vector machines are supervised learning models with associated learning algorithms that analyze data used for classification and&#xa0;" ID="ID_1757120445" CREATED="1557224068603" MODIFIED="1557224068603"/>
</node>
<node TEXT="Chapter 2 : SVM (Support Vector Machine) &#x2014; Theory &#x2013; Machine " FOLDED="true" ID="ID_835860462" CREATED="1557224068604" MODIFIED="1557225479677" LINK="https://medium.com/machine-learning-101/chapter-2-svm-support-vector-machine-theory-f0812effc72">
<node TEXT="May 3 2017  A Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane. In other words given labeled&#xa0;" ID="ID_570248485" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="1.4. Support Vector Machines &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_1830749926" CREATED="1557224068604" MODIFIED="1557225479677" LINK="http://scikit-learn.org/stable/modules/svm.html">
<node TEXT="The support vector machines in scikit-learn support both dense ( numpy.ndarray and convertible to that by numpy.asarray ) and sparse (any scipy.sparse )&#xa0;" ID="ID_923304381" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Support Vector Machine &#x2014; Introduction to Machine Learning " FOLDED="true" ID="ID_1454122341" CREATED="1557224068604" MODIFIED="1557225479677" LINK="https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47">
<node TEXT="Jun 7 2018  If not I suggest you have a look at them before moving on to support vector machine. Support vector machine is another simple algorithm that&#xa0;" ID="ID_697056918" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="An Idiots guide to Support vector machines (SVMs)" FOLDED="true" ID="ID_233298262" CREATED="1557224068604" MODIFIED="1557225479677" LINK="http://web.mit.edu/6.034/wwwbob/svm.pdf">
<node TEXT="Basic idea of support vector machines: just like 1- layer or multi-layer neural nets. &#x2013; Optimal hyperplane for linearly separable patterns. &#x2013; Extend to patterns that&#xa0;" ID="ID_212234736" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Support Vector Machine (SVM) - Fun and Easy Machine Learning " FOLDED="true" ID="ID_1846758905" CREATED="1557224068604" MODIFIED="1557225479677" LINK="https://www.youtube.com/watch?v=Y6RRHw9uN9o">
<node TEXT="Aug 15 2017  Support Vector Machine (SVM) - Fun and Easy Machine Learning &#x25bb;FREE YOLO GIFT - http://augmentedstartups.info/yolofreegiftsp &#x25bb;KERAS&#xa0;" ID="ID_283747083" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Support Vector Machines" FOLDED="true" ID="ID_135553798" CREATED="1557224068604" MODIFIED="1557225479677" LINK="http://cs229.stanford.edu/notes/cs229-notes3.pdf">
<node TEXT="This set of notes presents the Support Vector Machine (SVM) learning al- gorithm. SVMs are among the best (and many believe are indeed the best)." ID="ID_1265440132" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="How SVM (Support Vector Machine) algorithm works - YouTube" FOLDED="true" ID="ID_248523183" CREATED="1557224068604" MODIFIED="1557225479677" LINK="https://www.youtube.com/watch?v=1NxnPkZM9bc">
<node TEXT="Jan 6 2014  In this video I explain how SVM (Support Vector Machine) algorithm works to classify a linearly separable binary data set. The original&#xa0;" ID="ID_741957882" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Understanding Support Vector Machine algorithm from examples " FOLDED="true" ID="ID_314984385" CREATED="1557224068604" MODIFIED="1557225479677" LINK="https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/">
<node TEXT="Sep 13 2017  This article explains support vector machine a machine learning algorithm and its uses in classification and regression. Its a supervised&#xa0;" ID="ID_333539700" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Introduction to Support Vector Machines &#x2014; OpenCV 2.4.13.7 " FOLDED="true" ID="ID_335745816" CREATED="1557224068604" MODIFIED="1557225479677" LINK="https://docs.opencv.org/2.4/doc/tutorials/ml/introduction_to_svm/introduction_to_svm.html">
<node TEXT="Then the operation of the SVM algorithm is based on finding the hyperplane that gives the largest minimum distance to the training examples. Twice this&#xa0;" ID="ID_1597983378" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
</node>
<node TEXT="Linear Support Vector Machines#$D$#" FOLDED="true" ID="ID_308118577" CREATED="1557224068604" MODIFIED="1557225483438">
<icon BUILTIN="stop-sign"/>
<node TEXT="Support-vector machine - Wikipedia" FOLDED="true" ID="ID_1899203657" CREATED="1557224068604" MODIFIED="1557225479677" LINK="https://en.wikipedia.org/wiki/Support-vector_machine">
<node TEXT="In machine learning support-vector machines are supervised learning models with associated learning algorithms that analyze data&#xa0;" ID="ID_1108674717" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Support Vector Machine &#x2014; Introduction to Machine Learning " FOLDED="true" ID="ID_1693006103" CREATED="1557224068604" MODIFIED="1557225479677" LINK="https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47">
<node TEXT="Jun 7 2018  Introduction. I guess by now you wouldve accustomed yourself with linear regression and logistic regression algorithms. If not I suggest you&#xa0;" ID="ID_1062319955" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Chapter 2 : SVM (Support Vector Machine) &#x2014; Theory &#x2013; Machine " FOLDED="true" ID="ID_731510204" CREATED="1557224068604" MODIFIED="1557225479677" LINK="https://medium.com/machine-learning-101/chapter-2-svm-support-vector-machine-theory-f0812effc72">
<node TEXT="May 3 2017  A Support Vector Machine (SVM) is a discriminative classifier formally defined by a  The learning of the hyperplane in linear SVM is done by&#xa0;" ID="ID_478119494" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="1.4. Support Vector Machines &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_1961701562" CREATED="1557224068604" MODIFIED="1557225479677" LINK="http://scikit-learn.org/stable/modules/svm.html">
<node TEXT="On the other hand LinearSVC is another implementation of Support Vector Classification for the case of a linear kernel. Note that LinearSVC does not accept&#xa0;" ID="ID_1946693875" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="An Idiots guide to Support vector machines (SVMs)" FOLDED="true" ID="ID_1265348689" CREATED="1557224068604" MODIFIED="1557225479677" LINK="http://web.mit.edu/6.034/wwwbob/svm.pdf">
<node TEXT="Basic idea of support vector machines: just like 1- layer or multi-layer neural nets. &#x2013; Optimal hyperplane for linearly separable patterns. &#x2013; Extend to patterns that&#xa0;" ID="ID_1591063788" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Understanding Support Vector Machine algorithm from examples " FOLDED="true" ID="ID_1514746227" CREATED="1557224068604" MODIFIED="1557225479677" LINK="https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/">
<node TEXT="Sep 13 2017  In SVM it is easy to have a linear hyper-plane between these two classes. But another burning question which arises is should we need to&#xa0;" ID="ID_88399608" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="How SVM (Support Vector Machine) algorithm works - YouTube" FOLDED="true" ID="ID_1760309943" CREATED="1557224068604" MODIFIED="1557225479677" LINK="https://www.youtube.com/watch?v=1NxnPkZM9bc">
<node TEXT="Jan 6 2014  In this video I explain how SVM (Support Vector Machine) algorithm works to classify a linearly separable binary data set. The original&#xa0;" ID="ID_77290943" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Support Vector Machines" FOLDED="true" ID="ID_1224090190" CREATED="1557224068604" MODIFIED="1557225479677" LINK="http://cs229.stanford.edu/notes/cs229-notes3.pdf">
<node TEXT="This set of notes presents the Support Vector Machine (SVM) learning al- gorithm.  Also rather than parameterizing our linear classifier with the vector &#x3b8; we." ID="ID_1808721236" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Support Vector Machine (SVM) - Fun and Easy Machine Learning " FOLDED="true" ID="ID_766948950" CREATED="1557224068604" MODIFIED="1557225479677" LINK="https://www.youtube.com/watch?v=Y6RRHw9uN9o">
<node TEXT="Aug 15 2017  Support Vector Machine (SVM) - Fun and Easy Machine Learning &#x25bb;FREE YOLO GIFT - http://augmentedstartups.info/yolofreegiftsp &#x25bb;KERAS&#xa0;" ID="ID_722382818" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Support vector machines: The linearly separable case" FOLDED="true" ID="ID_1350143371" CREATED="1557224068604" MODIFIED="1557225479677" LINK="https://nlp.stanford.edu/IR-book/html/htmledition/support-vector-machines-the-linearly-separable-case-1.html">
<node TEXT="The SVM in particular defines the criterion to be looking for a decision surface that is maximally far away from any data point. This distance from the decision&#xa0;" ID="ID_433231424" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
</node>
<node TEXT="Support Vector Machine Scikit-learn implementation#$D$#" FOLDED="true" ID="ID_1493811305" CREATED="1557224068604" MODIFIED="1557225483438">
<icon BUILTIN="stop-sign"/>
<node TEXT="1.4. Support Vector Machines &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_700847527" CREATED="1557224068604" MODIFIED="1557225479677" LINK="http://scikit-learn.org/stable/modules/svm.html">
<node TEXT="On the other hand LinearSVC is another implementation of Support Vector Classification for the case of a linear kernel. Note that LinearSVC does not accept&#xa0;" ID="ID_1121143303" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Implementing SVM and Kernel SVM with Pythons Scikit-Learn" FOLDED="true" ID="ID_53029191" CREATED="1557224068604" MODIFIED="1557225479677" LINK="https://stackabuse.com/implementing-svm-and-kernel-svm-with-pythons-scikit-learn/">
<node TEXT="Apr 17 2018  We will then move towards an advanced SVM concept known as Kernel SVM and will also implement it with the help of Scikit-Learn." ID="ID_1428258418" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="sklearn.svm.SVC &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_1111988127" CREATED="1557224068604" MODIFIED="1557225479677" LINK="http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html">
<node TEXT="C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes&#xa0;" ID="ID_1167451361" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Support Vector Machines in Scikit-learn (article) - DataCamp" FOLDED="true" ID="ID_341165515" CREATED="1557224068604" MODIFIED="1557225479677" LINK="https://www.datacamp.com/community/tutorials/svm-classification-scikit-learn-python">
<node TEXT="Jul 12 2018  Until now you have learned about the theoretical background of SVM. Now you will learn about its implementation in Python using scikit-learn." ID="ID_737940432" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="sklearn.svm.SVR &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_1823920231" CREATED="1557224068604" MODIFIED="1557225479677" LINK="http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html">
<node TEXT="Epsilon-Support Vector Regression. The free parameters in the model are C and epsilon. The implementation is based on libsvm. Read more in the User Guide." ID="ID_420248456" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="In-Depth: Support Vector Machines | Python Data Science Handbook" FOLDED="true" ID="ID_1469117017" CREATED="1557224068604" MODIFIED="1557225479677" LINK="https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html">
<node TEXT="Support vector machines (SVMs) are a particularly powerful and flexible class of . will use Scikit-Learns support vector classifier to train an SVM model on this data. .. To handle this case the SVM implementation has a bit of a fudge-factor&#xa0;" ID="ID_1914889858" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="(Svm classifier) implemenation in python with Scikit-learn" FOLDED="true" ID="ID_47709568" CREATED="1557224068604" MODIFIED="1557225479677" LINK="http://dataaspirant.com/2017/01/25/svm-classifier-implemenation-python-scikit-learn/">
<node TEXT="Jan 25 2017  Svm classifier implementation in python with scikit-learn. Support vector machine classifier is one of the most popular machine learning&#xa0;" ID="ID_1522538863" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Linear SVC Machine learning SVM example with Python" FOLDED="true" ID="ID_209309512" CREATED="1557224068604" MODIFIED="1557225479677" LINK="https://pythonprogramming.net/linear-svc-example-scikit-learn-svm-python/">
<node TEXT="The most applicable machine learning algorithm for our problem is Linear SVC. Before hopping into Linear SVC with our data were going to show a very simple&#xa0;" ID="ID_1138519987" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="SVM using Scikit-Learn in Python | Learn OpenCV" FOLDED="true" ID="ID_475902281" CREATED="1557224068604" MODIFIED="1557225479677" LINK="https://www.learnopencv.com/svm-using-scikit-learn-in-python/">
<node TEXT="Jul 27 2018  This post explains the implementation of Support Vector Machines (SVMs) using Scikit-Learn library in Python." ID="ID_1050611566" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Understanding Support Vector Machine algorithm from examples " FOLDED="true" ID="ID_1997765846" CREATED="1557224068604" MODIFIED="1557225479677" LINK="https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/">
<node TEXT="Sep 13 2017  This article explains support vector machine a machine learning  How to implement SVM in Python and R? How to tune Parameters of SVM?" ID="ID_1889953780" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
</node>
<node TEXT="Support Vector Machine Classification#$D$#" FOLDED="true" ID="ID_1408156009" CREATED="1557224068604" MODIFIED="1557225483438">
<icon BUILTIN="stop-sign"/>
<node TEXT="Understanding Support Vector Machine algorithm from examples " FOLDED="true" ID="ID_1847239707" CREATED="1557224068604" MODIFIED="1557225479677" LINK="https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/">
<node TEXT="Sep 13 2017  This article explains support vector machine a machine learning algorithm and its uses in classification and regression. Its a supervised&#xa0;" ID="ID_1829462179" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Support-vector machine - Wikipedia" FOLDED="true" ID="ID_909382203" CREATED="1557224068604" MODIFIED="1557225479677" LINK="https://en.wikipedia.org/wiki/Support-vector_machine">
<node TEXT="In machine learning support-vector machines are supervised learning models with associated learning algorithms that&#xa0;" ID="ID_1811190132" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Support Vector Machine &#x2014; Introduction to Machine Learning " FOLDED="true" ID="ID_1907066594" CREATED="1557224068604" MODIFIED="1557225479677" LINK="https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47">
<node TEXT="Jun 7 2018  Support Vector Machine abbreviated as SVM can be used for both regression and classification tasks. But it is widely used in classification&#xa0;" ID="ID_616635521" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="1.4. Support Vector Machines &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_400540857" CREATED="1557224068604" MODIFIED="1557225479692" LINK="http://scikit-learn.org/stable/modules/svm.html">
<node TEXT="Support vector machines (SVMs) are a set of supervised learning methods used  LinearSVC is another implementation of Support Vector Classification for the&#xa0;" ID="ID_1414917076" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Chapter 2 : SVM (Support Vector Machine) &#x2014; Theory &#x2013; Machine " FOLDED="true" ID="ID_1206328890" CREATED="1557224068604" MODIFIED="1557225479692" LINK="https://medium.com/machine-learning-101/chapter-2-svm-support-vector-machine-theory-f0812effc72">
<node TEXT="May 3 2017  A Support Vector Machine (SVM) is a discriminative classifier formally  Varying those we can achive considerable non linear classification line&#xa0;" ID="ID_1679973099" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Support Vector Machines (SVM)" FOLDED="true" ID="ID_1258609802" CREATED="1557224068604" MODIFIED="1557225479692" LINK="http://www.statsoft.com/textbook/support-vector-machines">
<node TEXT="Support Vector Machine (SVM) is primarily a classier method that performs classification tasks by constructing hyperplanes in a multidimensional space that&#xa0;" ID="ID_967256493" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Support Vector Machines for Binary Classification - MATLAB " FOLDED="true" ID="ID_1321971170" CREATED="1557224068604" MODIFIED="1557225479692" LINK="https://www.mathworks.com/help/stats/support-vector-machines-for-binary-classification.html">
<node TEXT="Perform binary classification via SVM using separating hyperplanes and kernel transformations." ID="ID_449054866" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Support vector machine classification and validation of cancer tissue " FOLDED="true" ID="ID_505463998" CREATED="1557224068604" MODIFIED="1557225479692" LINK="https://www.ncbi.nlm.nih.gov/pubmed/11120680">
<node TEXT="Support vector machine classification and validation of cancer tissue samples using microarray expression data. Furey TS(1) Cristianini N Duffy N Bednarski&#xa0;" ID="ID_990565267" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="An Idiots guide to Support vector machines (SVMs)" FOLDED="true" ID="ID_89160582" CREATED="1557224068604" MODIFIED="1557225479692" LINK="http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf">
<node TEXT="3. Support Vectors. &#x2022; Support vectors are the data points that lie closest to the decision surface (or hyperplane). &#x2022; They are the data points most difficult to classify." ID="ID_287212662" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="How SVM (Support Vector Machine) algorithm works - YouTube" FOLDED="true" ID="ID_687616440" CREATED="1557224068604" MODIFIED="1557225479692" LINK="https://www.youtube.com/watch?v=1NxnPkZM9bc">
<node TEXT="Jan 6 2014  In this video I explain how SVM (Support Vector Machine) algorithm works to classify a linearly separable binary data set. The original&#xa0;" ID="ID_1924848489" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
</node>
<node TEXT="Support Vector Machine Linear Classification#$D$#" FOLDED="true" ID="ID_1423472133" CREATED="1557224068604" MODIFIED="1557225483439">
<icon BUILTIN="stop-sign"/>
<node TEXT="Support-vector machine - Wikipedia" FOLDED="true" ID="ID_1750338911" CREATED="1557224068604" MODIFIED="1557225479692" LINK="https://en.wikipedia.org/wiki/Support-vector_machine">
<node TEXT="In machine learning support-vector machines are supervised learning .. Kernel machine. The original maximum-margin hyperplane algorithm proposed by Vapnik in 1963 constructed a linear classifier." ID="ID_762187647" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="sklearn.svm.LinearSVC &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_1237101402" CREATED="1557224068604" MODIFIED="1557225479692" LINK="http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html">
<node TEXT="Linear Support Vector Classification. Similar to SVC with parameter kernel=linear but implemented in terms of liblinear rather than libsvm so it has more&#xa0;" ID="ID_406674866" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Linear Classification and Support Vector Machines" FOLDED="true" ID="ID_812751338" CREATED="1557224068604" MODIFIED="1557225479692" LINK="https://courses.cs.washington.edu/courses/cse473/13au/slides/26-SVMs.pdf">
<node TEXT="Find a line (in general a hyperplane) separating the two sets of data points: g(x) = w&#x2219;x + b = 0 i.e. w. 1 x. 1. + w. 2 x. 2. + b = 0. For any new point x choose:." ID="ID_670948434" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="1.4. Support Vector Machines &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_758198830" CREATED="1557224068604" MODIFIED="1557225479692" LINK="http://scikit-learn.org/stable/modules/svm.html">
<node TEXT="On the other hand LinearSVC is another implementation of Support Vector Classification for the case of a linear kernel. Note that LinearSVC does not accept&#xa0;" ID="ID_757999916" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Chapter 2 : SVM (Support Vector Machine) &#x2014; Theory &#x2013; Machine " FOLDED="true" ID="ID_1670399268" CREATED="1557224068604" MODIFIED="1557225479692" LINK="https://medium.com/machine-learning-101/chapter-2-svm-support-vector-machine-theory-f0812effc72">
<node TEXT="May 3 2017  A Support Vector Machine (SVM) is a discriminative classifier formally  Varying those we can achive considerable non linear classification line&#xa0;" ID="ID_1932788108" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Support Vector Machine vs Logistic Regression &#x2013; Towards Data " FOLDED="true" ID="ID_295639493" CREATED="1557224068604" MODIFIED="1557225479692" LINK="https://towardsdatascience.com/support-vector-machine-vs-logistic-regression-94cc2975433f">
<node TEXT="Aug 12 2018  Support Vector Machine (SVM) is an algorithm used for classification problems similar to Logistic Regression (LR). LR and SVM with linear&#xa0;" ID="ID_943654650" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Lecture 2: The SVM classifier" FOLDED="true" ID="ID_1090586991" CREATED="1557224068604" MODIFIED="1557225479692" LINK="http://www.robots.ox.ac.uk/~az/lectures/ml/lect2.pdf">
<node TEXT="C19 Machine Learning Hilary 2015. A. Zisserman. &#x2022; Review of linear classifiers. &#x2022; Linear separability. &#x2022; Perceptron. &#x2022; Support Vector Machine (SVM) classifier." ID="ID_280794097" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Linear classification: Support Vector Machine Softmax" FOLDED="true" ID="ID_737782395" CREATED="1557224068604" MODIFIED="1557225479692" LINK="http://cs231n.github.io/linear-classify/">
<node TEXT="Table of Contents: Intro to Linear classification; Linear score function; Interpreting a linear classifier; Loss function. Multiclass SVM; Softmax classifier; SVM vs&#xa0;" ID="ID_1120651676" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Understanding Support Vector Machine algorithm from examples " FOLDED="true" ID="ID_132696769" CREATED="1557224068604" MODIFIED="1557225479692" LINK="https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/">
<node TEXT="Sep 13 2017  In SVM it is easy to have a linear hyper-plane between these two classes.  Create SVM classification object model = svm.svc(kernel=linear&#xa0;" ID="ID_937439457" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Support Vector Machines for Binary Classification - MATLAB " FOLDED="true" ID="ID_1581839908" CREATED="1557224068605" MODIFIED="1557225479692" LINK="https://www.mathworks.com/help/stats/support-vector-machines-for-binary-classification.html">
<node TEXT="Create an SVM template that specifies storing the support vectors of the binary learners." ID="ID_351849044" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
</node>
<node TEXT="Support Vector Machine Non-Linear Examples#$D$#" FOLDED="true" ID="ID_1256277284" CREATED="1557224068605" MODIFIED="1557225483439">
<icon BUILTIN="stop-sign"/>
<node TEXT="5.3 Nonlinear SVM | 5 Support Vector Machines | Pattern " FOLDED="true" ID="ID_363734764" CREATED="1557224068605" MODIFIED="1557225479692" LINK="https://www.youtube.com/watch?v=riFEO1pHS7A">
<node TEXT="Nov 22 2012  The Pattern Recognition Class 2012 by Prof. Fred Hamprecht. It took place at the HCI / University of Heidelberg during the summer term of&#xa0;" ID="ID_651051281" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
<node TEXT="An Idiots guide to Support vector machines (SVMs)" FOLDED="true" ID="ID_1043521056" CREATED="1557224068605" MODIFIED="1557225479692" LINK="http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf">
<node TEXT="Efficient learning algorithms for non-linear functions based  Basic idea of support vector machines: just like 1- layer or  subset of training samples the support&#xa0;" ID="ID_1764899145" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
<node TEXT="Support-vector machine - Wikipedia" FOLDED="true" ID="ID_1512685686" CREATED="1557224068605" MODIFIED="1557225479692" LINK="https://en.wikipedia.org/wiki/Support-vector_machine">
<node TEXT="In machine learning support-vector machines are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training examples each marked as belonging to one or the  6 Nonlinear classification; 7 Computing the SVM classifier." ID="ID_1523655324" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
<node TEXT="Nonlinear SVMs" FOLDED="true" ID="ID_1997404075" CREATED="1557224068605" MODIFIED="1557225479692" LINK="https://nlp.stanford.edu/IR-book/html/htmledition/nonlinear-svms-1.html">
<node TEXT="Nonlinear SVMs.  The SVM linear classifier relies on a dot product between data point vectors. Let $K(\vec{x}_i \vec{x}_j .  Worked example. The quadratic&#xa0;" ID="ID_1692104674" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
<node TEXT="1.4. Support Vector Machines &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_476954164" CREATED="1557224068605" MODIFIED="1557225479692" LINK="http://scikit-learn.org/stable/modules/svm.html">
<node TEXT="If the number of features is much greater than the number of samples avoid over-fitting .. Support Vector Regression (SVR) using linear and non-linear kernels&#xa0;" ID="ID_1381337525" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
<node TEXT="Non-linear Support Vector Machines" FOLDED="true" ID="ID_1842919159" CREATED="1557224068605" MODIFIED="1557225479692" LINK="https://disi.unitn.it/~passerini/teaching/2011-2012/MachineLearning/slides/16_nonlinear_svm/talk.pdf">
<node TEXT="Non-linear Support Vector Machines feature map. &#x3a6; : X &#x2192;H. &#x3a6; is a function mapping each example to a higher dimensional space H. Examples x are replaced&#xa0;" ID="ID_1142971733" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
<node TEXT="SVM Example" FOLDED="true" ID="ID_979952515" CREATED="1557224068605" MODIFIED="1557225479692" LINK="http://axon.cs.byu.edu/Dan/678/miscellaneous/SVM.example.pdf">
<node TEXT="We try to give a helpful simple example that demonstrates a linear. SVM and then extend the example to a simple non-linear case to illustrate the use of mapping&#xa0;" ID="ID_76787886" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
<node TEXT="Understanding Support Vector Machine algorithm from examples " FOLDED="true" ID="ID_1856324919" CREATED="1557224068605" MODIFIED="1557225479692" LINK="https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/">
<node TEXT="Sep 13 2017  You can look at support vector machines and a few examples of its working here.  It is mostly useful in non-linear separation problem. Simply&#xa0;" ID="ID_148220490" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
<node TEXT="A Practical Guide to Interpreting and Visualising Support Vector " FOLDED="true" ID="ID_331071191" CREATED="1557224068605" MODIFIED="1557225479692" LINK="https://towardsdatascience.com/a-practical-guide-to-interpreting-and-visualising-support-vector-machines-97d2a5b0564e">
<node TEXT="Jan 12 2019  Introduction to Linear Models SVMs and Kernels; Interpreting high  The second example uses a non linear model (actually a kernel trick&#xa0;" ID="ID_1807423030" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
<node TEXT="FPGA based nonlinear Support Vector Machine training using an " FOLDED="true" ID="ID_1059578803" CREATED="1557224068605" MODIFIED="1557225479692" LINK="https://ieeexplore.ieee.org/document/7293972">
<node TEXT="In this work a complete FPGA-based system for nonlinear SVM training using  whose computational cost scales quadratically with the number of examples." ID="ID_1556089912" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
</node>
<node TEXT="Support Vector Regression#$D$#" FOLDED="true" ID="ID_337508365" CREATED="1557224068605" MODIFIED="1557225512332">
<icon BUILTIN="stop-sign"/>
<node TEXT="Support Vector Regression Or SVR &#x2013; Coinmonks &#x2013; Medium" FOLDED="true" ID="ID_1281914669" CREATED="1557224068605" MODIFIED="1557225479692" LINK="https://medium.com/coinmonks/support-vector-regression-or-svr-8eb3acf6d0ff">
<node TEXT="Jun 28 2018  This post is about SUPPORT VECTOR REGRESSION. Those who are in Machine Learning or Data Science are quite familiar with the term&#xa0;" ID="ID_1934394430" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
<node TEXT="sklearn.svm.SVR &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_634232909" CREATED="1557224068605" MODIFIED="1557225479692" LINK="http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html">
<node TEXT="Epsilon-Support Vector Regression. The free parameters in the model are C and epsilon. The implementation is based on libsvm. Read more in the User Guide." ID="ID_398416816" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
<node TEXT="Support Vector Regression" FOLDED="true" ID="ID_1784986888" CREATED="1557224068605" MODIFIED="1557225479692" LINK="https://www.saedsayad.com/support_vector_machine_reg.htm">
<node TEXT="The Support Vector Regression (SVR) uses the same principles as the SVM for classification with only a few minor differences. First of all because output is a&#xa0;" ID="ID_1618547634" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
<node TEXT="Support-vector machine - Wikipedia" FOLDED="true" ID="ID_1112634199" CREATED="1557224068605" MODIFIED="1557225479692" LINK="https://en.wikipedia.org/wiki/Support-vector_machine">
<node TEXT="In machine learning support-vector machines are supervised learning models with associated learning algorithms that analyze data&#xa0;" ID="ID_1973624221" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
<node TEXT="Understanding Support Vector Machine Regression - MATLAB " FOLDED="true" ID="ID_743505241" CREATED="1557224068605" MODIFIED="1557225479692" LINK="https://www.mathworks.com/help/stats/understanding-support-vector-machine-regression.html">
<node TEXT="Understand the mathematical formulation of linear and nonlinear SVM regression problems and solver algorithms." ID="ID_770686970" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
<node TEXT="A Tutorial on Support Vector Regression&#x2217;" FOLDED="true" ID="ID_851542453" CREATED="1557224068605" MODIFIED="1557225479692" LINK="https://alex.smola.org/papers/2003/SmoSch03b.pdf">
<node TEXT="A Tutorial on Support Vector Regression&#x2217;. Alex J. Smola&#x2020; and Bernhard Sch &#xf6;lkopf&#x2021;. September 30 2003. Abstract. In this tutorial we give an overview of the&#xa0;" ID="ID_1670799165" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
<node TEXT="Support Vector Machine Regression" FOLDED="true" ID="ID_1288768901" CREATED="1557224068605" MODIFIED="1557225479692" LINK="http://kernelsvm.tripod.com/">
<node TEXT="Support Vector Machine Regression. Support Vector Machines are very specific class of algorithms characterized by usage of kernels absence of local minima&#xa0;" ID="ID_1385513422" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
<node TEXT="svm - How does support vector regression work intuitively? - Cross " FOLDED="true" ID="ID_853292462" CREATED="1557224068605" MODIFIED="1557225479692" LINK="https://stats.stackexchange.com/questions/82044/how-does-support-vector-regression-work-intuitively">
<node TEXT="In order to understand how you go from classification to regression it helps to see how both cases one applies the same SVM theory to&#xa0;" ID="ID_1013462957" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
<node TEXT="Support Vector Regression - YouTube" FOLDED="true" ID="ID_954945858" CREATED="1557224068605" MODIFIED="1557225479692" LINK="https://www.youtube.com/watch?v=8qsFI22c5Lk">
<node TEXT="Jul 25 2017  Training on Support Vector Regression by Vamsidhar Ambatipudi." ID="ID_724848989" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
<node TEXT="A tutorial on support vector regression" FOLDED="true" ID="ID_439891194" CREATED="1557224068605" MODIFIED="1557225479692" LINK="https://alex.smola.org/papers/2004/SmoSch04.pdf">
<node TEXT="A tutorial on support vector regression. &#x2217;. ALEX J. SMOLA and BERNHARD SCH &#xa8;OLKOPF. RSISE Australian National University Canberra 0200 Australia." ID="ID_291730816" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
</node>
</node>
<node TEXT="Scikit-learn implementation" ID="ID_984354550" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
<node TEXT="Classification" ID="ID_561899328" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Kernel based classification" FOLDED="true" ID="ID_1116524624" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Polynomial Kernel Visualization" ID="ID_681533924" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Visualization 1 " FOLDED="true" ID="ID_643570974" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Kernel PCA#$D$#" FOLDED="true" ID="ID_973056160" CREATED="1557224068599" MODIFIED="1557225483435">
<icon BUILTIN="stop-sign"/>
<node TEXT="Kernel principal component analysis - Wikipedia" FOLDED="true" ID="ID_374461244" CREATED="1557224068599" MODIFIED="1557225479645" LINK="https://en.wikipedia.org/wiki/Kernel_principal_component_analysis">
<node TEXT="In the field of multivariate statistics kernel principal component analysis (kernel PCA) is an extension of principal component analysis (PCA) using techniques of&#xa0;" ID="ID_1208819959" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Kernel PCA &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_1750752957" CREATED="1557224068599" MODIFIED="1557225479645" LINK="http://scikit-learn.org/stable/auto_examples/decomposition/plot_kernel_pca.html">
<node TEXT="This example shows that Kernel PCA is able to find a projection of the data that  sklearn.decomposition import PCA KernelPCA from sklearn.datasets import&#xa0;" ID="ID_1518390952" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Kernel PCA vs PCA vs ICA in Tensorflow/sklearn &#x2013; Towards Data " FOLDED="true" ID="ID_1989016065" CREATED="1557224068599" MODIFIED="1557225479645" LINK="https://towardsdatascience.com/kernel-pca-vs-pca-vs-ica-in-tensorflow-sklearn-60e17eb15a64">
<node TEXT="Sep 10 2018  Principle Component Analysis performs a linear transformation on a given data however many real-world data are not linearly separable." ID="ID_103838208" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="sklearn.decomposition.KernelPCA &#x2014; scikit-learn 0.20.3 " FOLDED="true" ID="ID_46433729" CREATED="1557224068599" MODIFIED="1557225479645" LINK="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html">
<node TEXT="Kernel Principal component analysis (KPCA). Non-linear dimensionality reduction through the use of kernels (see Pairwise metrics Affinities and Kernels)." ID="ID_1523588729" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Kernel PCA" FOLDED="true" ID="ID_891246857" CREATED="1557224068599" MODIFIED="1557225479645" LINK="http://www.cs.haifa.ac.il/~rita/uml_course/lectures/KPCA.pdf">
<node TEXT="" ID="ID_1970263673" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Dimensionality reduction. PCA. Kernel PCA." FOLDED="true" ID="ID_79481976" CREATED="1557224068599" MODIFIED="1557225479645" LINK="https://www.cs.mcgill.ca/~dprecup/courses/ML/Lectures/ml-lecture13.pdf">
<node TEXT="Kernel PCA. &#x2022; Dimensionality reduction. &#x2022; Principal Component Analysis (PCA). &#x2022; Kernelizing PCA. &#x2022; If we have time: Autoencoders. COMP-652 and ECSE-608&#xa0;" ID="ID_1273757169" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Kernel PCA" FOLDED="true" ID="ID_1608938768" CREATED="1557224068599" MODIFIED="1557225479645" LINK="http://fourier.eng.hmc.edu/e161/lectures/kernelPCA/node4.html">
<node TEXT="Kernel PCA. First consider nonlinearly mapping all data points ${\bf x}$ to $f({\bf x})$ in a higher dimensional feature space ${\cal F}$  where the covariance&#xa0;" ID="ID_606611264" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Kernel Principal Component Analysis and its Applications in Face " FOLDED="true" ID="ID_1293230505" CREATED="1557224068599" MODIFIED="1557225479645" LINK="https://arxiv.org/pdf/1207.3538">
<node TEXT="Aug 31 2014  Principal component analysis (PCA) is a popular tool for linear dimensionality reduc- tion and feature extraction. Kernel PCA is the nonlinear&#xa0;" ID="ID_1480259461" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Kernel tricks and nonlinear dimensionality reduction via RBF kernel " FOLDED="true" ID="ID_137008003" CREATED="1557224068599" MODIFIED="1557225479645" LINK="https://sebastianraschka.com/Articles/2014_kernel_pca.html">
<node TEXT="In the linear PCA approach we are interested in  (Kernel Principal Component Analysis [2])&#xa0;" ID="ID_1370812760" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
<node TEXT="Kernel principal component analysis" FOLDED="true" ID="ID_397650947" CREATED="1557224068599" MODIFIED="1557225479645" LINK="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.128.7613">
<node TEXT="CiteSeerX - Document Details (Isaac Councill Lee Giles Pradeep Teregowda): A new method for performing a nonlinear form of Principal Component Analysis&#xa0;" ID="ID_191466594" CREATED="1557224068599" MODIFIED="1557224068599"/>
</node>
</node>
<node TEXT="Support Vector Machine Kernel based classification#$D$#" FOLDED="true" ID="ID_270329902" CREATED="1557224068604" MODIFIED="1557225483438">
<icon BUILTIN="stop-sign"/>
<node TEXT="Kernel method - Wikipedia" ID="ID_1504114368" CREATED="1557224068604" MODIFIED="1557225479692" LINK="https://en.wikipedia.org/wiki/Kernel_method">
<node TEXT="In machine learning kernel methods are a class of algorithms for pattern analysis whose best known member is the support vector machine (SVM). The general task of pattern analysis is to find and study general types of  Most kernel algorithms are based on convex optimization or eigenproblems and are statistically&#xa0;" ID="ID_921653002" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Support Vector Machine and Bhattacharrya kernel function for " ID="ID_499957924" CREATED="1557224068604" MODIFIED="1557225479692" LINK="https://ieeexplore.ieee.org/document/6352380/">
<node TEXT="This study presents a new approach for region based classification that consists in use the Support Vector Machine (SVM) method with Bhattacharyya kernel&#xa0;" ID="ID_1648796477" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Support-vector machine - Wikipedia" FOLDED="true" ID="ID_902956791" CREATED="1557224068604" MODIFIED="1557225479692" LINK="https://en.wikipedia.org/wiki/Support-vector_machine">
<node TEXT="In machine learning support-vector machines are supervised learning models with associated . Permutation tests based on SVM weights have been suggested as a  classifiers by applying the kernel trick to maximum-margin hyperplanes." ID="ID_1814197948" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="SVM kernel functions for classification - IEEE Conference Publication" FOLDED="true" ID="ID_1062824510" CREATED="1557224068604" MODIFIED="1557225479692" LINK="http://ieeexplore.ieee.org/document/6524743/">
<node TEXT="SVM kernel functions for classification. Abstract: A new generation learning system based on recent advances in statistical learning theory deliver state-of-the-art&#xa0;" ID="ID_221147381" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Minimum classification error-based weighted support vector " FOLDED="true" ID="ID_1464010967" CREATED="1557224068604" MODIFIED="1557225479692" LINK="https://www.ncbi.nlm.nih.gov/pubmed/23556696">
<node TEXT="J Acoust Soc Am. 2013 Apr;133(4):EL307-13. doi: 10.1121/1.4794350. Minimum classification error-based weighted support vector machine kernels for speaker&#xa0;" ID="ID_238207775" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="1.4. Support Vector Machines &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_589791858" CREATED="1557224068604" MODIFIED="1557225479692" LINK="http://scikit-learn.org/stable/modules/svm.html">
<node TEXT="Support vector machines (SVMs) are a set of supervised learning methods used for  of Support Vector Classification for the case of a linear kernel. .. liblinear implementation is much more efficient than its libsvm-based SVC counterpart and&#xa0;" ID="ID_1236134206" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Support Vector Machines for Binary Classification - MATLAB " FOLDED="true" ID="ID_589512639" CREATED="1557224068604" MODIFIED="1557225479692" LINK="https://www.mathworks.com/help/stats/support-vector-machines-for-binary-classification.html">
<node TEXT="Then generates a classifier based on the data with the  Train an SVM classifier with&#xa0;" ID="ID_847688560" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="A Multiple Kernel Support Vector Machine Scheme for Simultaneous " FOLDED="true" ID="ID_1667206446" CREATED="1557224068604" MODIFIED="1557225479692" LINK="https://link.springer.com/chapter/10.1007/978-3-540-71701-0_44">
<node TEXT="A Multiple Kernel Support Vector Machine Scheme for Simultaneous Feature Selection and Rule-Based Classification. Authors; Authors and affiliations." ID="ID_655469943" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Object Classification Using Support Vector Machines with Kernel " FOLDED="true" ID="ID_1763382630" CREATED="1557224068604" MODIFIED="1557225479692" LINK="https://content.sciendo.com/view/journals/ipc/21/3/article-p45.xml">
<node TEXT="Object Classification Using Support Vector Machines with Kernel-based Data Preprocessing. Krzysztof Adamiakkrzysztof.adam.adamiak@gmail.com Piotr Duch&#xa0;" ID="ID_1857696810" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Classification of EEG Signals Using a Multiple Kernel Learning " FOLDED="true" ID="ID_991994240" CREATED="1557224068604" MODIFIED="1557225479692" LINK="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4168520/">
<node TEXT="Experimental results showed that the proposed method provided better classification performance compared with the SVM based on a single kernel. For mental&#xa0;" ID="ID_641412930" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
</node>
<node TEXT="Support Vector Machine Polynomial Kernel Visualization#$D$#" FOLDED="true" ID="ID_1080428239" CREATED="1557224068604" MODIFIED="1557225483439">
<icon BUILTIN="stop-sign"/>
<node TEXT="SVM with polynomial kernel visualization - YouTube" FOLDED="true" ID="ID_1573652002" CREATED="1557224068604" MODIFIED="1557225479692" LINK="https://www.youtube.com/watch?v=3liCbRZPrZA">
<node TEXT="Feb 5 2007  A visual demonstration of the kernel trick in SVM. This short video demonstrates how vectors of two classes that cannot be linearly separated in&#xa0;" ID="ID_1030617" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="3. Visualising how different kernels in SVMs work. | Kaggle" FOLDED="true" ID="ID_754447312" CREATED="1557224068604" MODIFIED="1557225479692" LINK="https://www.kaggle.com/joparga3/3-visualising-how-different-kernels-in-svms-work">
<node TEXT="This workbook will provide an understanding of how SVM (Support vector machines)  This is done for visualisation purposes which will enable us to better . SVMs using linear kernel have one important parameter that can be tuned and this&#xa0;" ID="ID_1095336535" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="How to Select Support Vector Machine Kernels" FOLDED="true" ID="ID_436477162" CREATED="1557224068604" MODIFIED="1557225479692" LINK="https://www.kdnuggets.com/2016/06/select-support-vector-machine-kernels.html">
<node TEXT="For simplicity (and visualization purposes) lets assume our dataset consists of 2 dimensions only. Below I plotted the decision regions of a linear SVM on 2&#xa0;" ID="ID_1781774780" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Visualization of the SVM model with polynomial kernel on the " FOLDED="true" ID="ID_563991635" CREATED="1557224068604" MODIFIED="1557225479692" LINK="https://www.researchgate.net/figure/Visualization-of-the-SVM-model-with-polynomial-kernel-on-the-checkerboard-example-It-can_fig16_309004959">
<node TEXT="Visualization of the SVM model with polynomial kernel on the checkerboard example. It can be seen that all contributions involving x(3) do not contribute in a&#xa0;" ID="ID_500752751" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="machine learning - How to intuitively explain what a kernel is " FOLDED="true" ID="ID_1001675097" CREATED="1557224068604" MODIFIED="1557225479692" LINK="https://stats.stackexchange.com/questions/152897/how-to-intuitively-explain-what-a-kernel-is">
<node TEXT="For example consider a simple polynomial kernel k(xy)=(1+xTy)2 with xy&#x2208;R2. . Specifically lectures Support Vector Machines Kernel Methods and Radial Basis Functions . Visualizing the feature map and the resulting boundary line." ID="ID_1092289424" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Comprehensive Support Vector Machines Guide - Using Illusion to " FOLDED="true" ID="ID_991886477" CREATED="1557224068604" MODIFIED="1557225479692" LINK="https://medium.com/analytics-vidhya/comprehensive-support-vector-machines-guide-using-illusion-to-solve-reality-ad3136d8f877">
<node TEXT="Sep 29 2018  Support Vector machine is a linear classifier that separates the various  SVM uses kernel trick as can be seen on the left to transform the  can then visualize our imaginary separating/classification boundary which is linear." ID="ID_276352296" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Support Vector Machines in R | DataCamp" FOLDED="true" ID="ID_478577033" CREATED="1557224068604" MODIFIED="1557225479692" LINK="https://www.datacamp.com/courses/support-vector-machines-in-r">
<node TEXT="Support Vector Classifiers - Linear Kernels. Introduces students to the basic concepts of support vector machines by applying the svm algorithm to a dataset that&#xa0;" ID="ID_1915719871" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Understanding Support Vector Machine algorithm from examples " FOLDED="true" ID="ID_1041012483" CREATED="1557224068604" MODIFIED="1557225479692" LINK="https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/">
<node TEXT="Sep 13 2017  We do not scale our # data since we want to plot the support vectors C = 1.0 # SVM regularization parameter svc = svm.SVC(kernel=linear&#xa0;" ID="ID_492912372" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Support Vector Machines for Binary Classification - MATLAB " FOLDED="true" ID="ID_528396634" CREATED="1557224068604" MODIFIED="1557225479692" LINK="https://www.mathworks.com/help/stats/support-vector-machines-for-binary-classification.html">
<node TEXT="Perform binary classification via SVM using separating hyperplanes and kernel transformations.  KernelFunction &#x2014; The default value is linear for two-class learning which separates the data by a .. Visualize the optimized classifier." ID="ID_1799543574" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
<node TEXT="Support Vector Machines Tutorial &#x2013; Stats and Bots" FOLDED="true" ID="ID_1044094123" CREATED="1557224068604" MODIFIED="1557225479692" LINK="https://blog.statsbot.co/support-vector-machines-tutorial-c1618e635e93">
<node TEXT="Aug 15 2017  Its time to introduce you to support vector machines (SVM) without hard  For the 3D projection above I had used a polynomial kernel with c=0&#xa0;" ID="ID_592997444" CREATED="1557224068604" MODIFIED="1557224068604"/>
</node>
</node>
</node>
<node TEXT="Visualization 2 " ID="ID_1310350169" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
</node>
</node>
<node TEXT="Linear Classification" FOLDED="true" ID="ID_1645563476" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Linear Classification Implementation#$D$#" FOLDED="true" ID="ID_1973717366" CREATED="1557224068601" MODIFIED="1557225483437">
<icon BUILTIN="stop-sign"/>
<node TEXT="An intro to linear classification with Python - PyImageSearch" FOLDED="true" ID="ID_1577088297" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://www.pyimagesearch.com/2016/08/22/an-intro-to-linear-classification-with-python/">
<node TEXT="Aug 22 2016  From there I provide an actual linear classification implementation and example using the scikit-learn library that can be used to classify the&#xa0;" ID="ID_1423329533" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="sklearn.svm.LinearSVC &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_373653444" CREATED="1557224068601" MODIFIED="1557225479661" LINK="http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html">
<node TEXT="Linear Support Vector Classification. Similar to SVC with parameter kernel=linear but implemented in terms of liblinear rather than libsvm so it has more&#xa0;" ID="ID_1734759018" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="GitHub - williamd4112/simple-linear-classification: A python " FOLDED="true" ID="ID_1099416030" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://github.com/williamd4112/simple-linear-classification">
<node TEXT="A python implementation of linear classification algorithm (including Probabilistic Generative Model Probabilistic Discriminative Model). (See Pattern&#xa0;" ID="ID_344626217" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="LIBLINEAR -- A Library for Large Linear Classification" FOLDED="true" ID="ID_851339364" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://www.csie.ntu.edu.tw/~cjlin/liblinear/">
<node TEXT="logistic regression support vector machines linear classification document  The appendices of this paper give all implementation details of LIBLINEAR." ID="ID_59076276" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="sklearn.linear_model.LogisticRegression &#x2014; scikit-learn 0.20.3 " FOLDED="true" ID="ID_1268220830" CREATED="1557224068601" MODIFIED="1557225479661" LINK="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">
<node TEXT="In the multiclass case the training algorithm uses the one-vs-rest (OvR)  This class implements regularized logistic regression using the liblinear library&#xa0;" ID="ID_1547851589" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="LIBLINEAR: A Library for Large Linear Classification" FOLDED="true" ID="ID_459783586" CREATED="1557224068601" MODIFIED="1557225479661" LINK="https://www.csie.ntu.edu.tw/~cjlin/papers/liblinear.pdf">
<node TEXT="Dec 8 2017  Keywords: large-scale linear classification logistic regression support vector machines . Library calls are implemented in the file linear.cpp." ID="ID_1926482587" CREATED="1557224068601" MODIFIED="1557224068601"/>
</node>
<node TEXT="What I Learned Implementing a Classifier from Scratch in Python " FOLDED="true" ID="ID_1061932017" CREATED="1557224068602" MODIFIED="1557225479661" LINK="http://www.jeannicholashould.com/what-i-learned-implementing-a-classifier-from-scratch.html">
<node TEXT="Jan 4 2017  The classifier algorithm falls under the supervised learning category. .. the capabilities of the Perceptron algorithm are attributable to linear&#xa0;" ID="ID_45805076" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="Linear Regression (Python Implementation) - GeeksforGeeks" FOLDED="true" ID="ID_955614147" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://www.geeksforgeeks.org/linear-regression-python-implementation/">
<node TEXT="This article discusses the basics of linear regression and its implementation in Python programming language. Linear regression is a statistical approach for&#xa0;" ID="ID_460815013" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="Perceptron - Wikipedia" FOLDED="true" ID="ID_1657607490" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://en.wikipedia.org/wiki/Perceptron">
<node TEXT="In machine learning the perceptron is an algorithm for supervised learning of binary classifiers. A binary classifier is a function which can decide whether or not an input represented by a vector of numbers belongs to some specific class. It is a type of linear classifier i.e. a classification algorithm that makes its&#xa0;" ID="ID_290870457" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="Predict labels for linear classification models - MATLAB" FOLDED="true" ID="ID_196358265" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://www.mathworks.com/help/stats/classificationlinear.predict.html">
<node TEXT="Mdl &#x2014; Binary linear classification model ClassificationLinear model object. Binary linear .. Generate C and C++ code using MATLAB&#xae; Coder&#x2122;. Usage notes&#xa0;" ID="ID_465735078" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
</node>
<node TEXT="Linear Classification Optimizations#$D$#" FOLDED="true" ID="ID_258292171" CREATED="1557224068602" MODIFIED="1557225483437">
<icon BUILTIN="stop-sign"/>
<node TEXT="Convex Optimization in Classification Problems" FOLDED="true" ID="ID_1754567857" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://web.stanford.edu/class/ee392o/mit022702.pdf">
<node TEXT="outline.  convex optimization. &#x2022; SVMs and robust linear programming. &#x2022; minimax probability machine. &#x2022; learning the kernel matrix. 3&#xa0;" ID="ID_867922972" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="A Comparison of Optimization Methods and Software for Large " FOLDED="true" ID="ID_190568664" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf">
<node TEXT="Keywords: L1 regularization linear classification optimization methods logistic . Comparing Methods and Software for L1-regularized Linear Classification." ID="ID_914211203" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="Linear classifier - Wikipedia" FOLDED="true" ID="ID_1191649462" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://en.wikipedia.org/wiki/Linear_classifier">
<node TEXT="In the field of machine learning the goal of statistical classification is to use an objects . the discrepancy between the classifiers outputs and the desired outputs. Thus the learning algorithm solves an optimization problem of the form." ID="ID_811321816" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="Online DC Optimization for Online Binary Linear Classification " FOLDED="true" ID="ID_1492856749" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://link.springer.com/chapter/10.1007/978-3-662-49390-8_64">
<node TEXT="This paper concerns online algorithms for online binary linear classification (OBLC) problems in Machine learning. In a sense of &#x201c;online&#x201d; classification&#xa0;" ID="ID_1177748938" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="CS231n Winter 2016: Lecture 3: Linear Classification 2 " FOLDED="true" ID="ID_899358282" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://www.youtube.com/watch?v=qlLChbHhbg4">
<node TEXT="Jan 11 2016  Stanford Winter Quarter 2016 class: CS231n: Convolutional Neural Networks for Visual Recognition. Lecture 3. Get in touch on Twitter&#xa0;" ID="ID_847536979" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="Multiclass SVM optimization demo" FOLDED="true" ID="ID_40302065" CREATED="1557224068602" MODIFIED="1557225479661" LINK="http://vision.stanford.edu/teaching/cs231n/linear-classify-demo/">
<node TEXT="The class scores for linear classifiers are computed as f(xi;Wb)=Wxi+b where the  Each classifier is visualized by a line that indicates its zero score level set." ID="ID_1734518535" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="CS231n Lecture 3 - Linear Classification 2 Optimization - YouTube" FOLDED="true" ID="ID_639556911" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://www.youtube.com/watch?v=q3TZVNGtOug">
<node TEXT="Jun 14 2016  Linear classification II Higher-level representations image features Optimization stochastic gradient descent." ID="ID_615369632" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="Fit linear classification model to high-dimensional data - MATLAB " FOLDED="true" ID="ID_1424002655" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://www.mathworks.com/help/stats/fitclinear.html">
<node TEXT="This example shows how to minimize the cross-validation error in a linear classifier using fitclinear . The example uses&#xa0;" ID="ID_1347537110" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="A hybrid optimization method for acceleration of building linear " FOLDED="true" ID="ID_134828506" CREATED="1557224068602" MODIFIED="1557225479661" LINK="http://ieeexplore.ieee.org/document/6707017/">
<node TEXT="Linear classification is an important technique in machine learning and data mining and development of fast optimization methods for training linear class." ID="ID_178075711" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
<node TEXT="ABC inventory classification with multiple-criteria using weighted " FOLDED="true" ID="ID_202128438" CREATED="1557224068602" MODIFIED="1557225479661" LINK="https://www.sciencedirect.com/science/article/pii/S0305054804001790">
<node TEXT="Inventory classification using ABC analysis is one of the most widely employed  In this paper we propose a simple weighted linear optimization model to&#xa0;" ID="ID_330847719" CREATED="1557224068602" MODIFIED="1557224068602"/>
</node>
</node>
</node>
</node>
<node TEXT="Non-Linear Examples" ID="ID_573761442" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
<node TEXT="Controlled Support Vector Machines" FOLDED="true" ID="ID_1779924908" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Controlled Support Vector Machines#$D$#" FOLDED="true" ID="ID_1442143798" CREATED="1557224068605" MODIFIED="1557225483439">
<icon BUILTIN="stop-sign"/>
<node TEXT="Controlled support vector machines - Machine Learning Algorithms" FOLDED="true" ID="ID_1390296332" CREATED="1557224068605" MODIFIED="1557225479692" LINK="https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781785889622/7/ch07lvl1sec54/controlled-support-vector-machines">
<node TEXT="Controlled support vector machinesWith real datasets SVM can extract a very large number of support vectors to increase accur" ID="ID_1561353601" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
<node TEXT="Support Vector Machines &#x2014; A Brief Overview &#x2013; Towards Data Science" FOLDED="true" ID="ID_1207540671" CREATED="1557224068605" MODIFIED="1557225479692" LINK="https://towardsdatascience.com/support-vector-machines-a-brief-overview-37e018ae310f">
<node TEXT="Aug 2 2017  Support vector machines have become a great tool for the data scientist.  These close points are the support vectors because they control the&#xa0;" ID="ID_598537553" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
<node TEXT="Support Vector Machine-Based Classification Scheme for " FOLDED="true" ID="ID_1565038178" CREATED="1557224068605" MODIFIED="1557225479692" LINK="https://ieeexplore.ieee.org/iel5/10/4567608/04463647.pdf">
<node TEXT="support vector machine (SVM) to classify upper limb motions using myoelectric signals. It explores the optimum configuration of SVM- based myoelectric control&#xa0;" ID="ID_1457917998" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
<node TEXT="(PDF) Support Vector Machines: A Nonlinear Modelling and Control " FOLDED="true" ID="ID_421946943" CREATED="1557224068605" MODIFIED="1557225479692" LINK="https://www.researchgate.net/publication/245441278_Support_Vector_Machines_A_Nonlinear_Modelling_and_Control_Perspective">
<node TEXT="We discuss a method of least squares support vector machines (LS-SVM) which has been extended to recurrent models and use in optimal control problems." ID="ID_365900001" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
<node TEXT="Controlled spectral unmixing using extended Support Vector Machines" FOLDED="true" ID="ID_1679825150" CREATED="1557224068605" MODIFIED="1557225479692" LINK="https://ieeexplore.ieee.org/document/5594843">
<node TEXT="Controlled spectral unmixing using extended Support Vector Machines. Abstract: This paper presents an improved spectral unmixing framework for remote&#xa0;" ID="ID_280846074" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
<node TEXT="Least Squares Support Vector Machines" FOLDED="true" ID="ID_1106958315" CREATED="1557224068605" MODIFIED="1557225479692" LINK="https://www.sciencedirect.com/science/article/pii/S1537511017311194">
<node TEXT="Least Squares Support Vector Machines (LS-SVM) used to predict nutrient release  Keywords. Controlled release fertiliser. Nutrient release model. LS-SVM." ID="ID_306952593" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
<node TEXT="Optimal control by least squares support vector machines" FOLDED="true" ID="ID_701721324" CREATED="1557224068605" MODIFIED="1557225479692" LINK="ftp://ftp.esat.kuleuven.be/sista/ida/reports/98-86.pdf">
<node TEXT="method for imposing local stability in the LS-SVM control scheme. The results are discussed for support vector machines with radial basis function kernel." ID="ID_1426427855" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
<node TEXT="Support vector machines&#x2010;based generalized predictive control " FOLDED="true" ID="ID_1479488983" CREATED="1557224068605" MODIFIED="1557225479692" LINK="https://onlinelibrary.wiley.com/doi/abs/10.1002/rnc.1094">
<node TEXT="Aug 22 2006  In this study we propose a novel control methodology that introduces the use of support vector machines (SVMs) in the generalized predictive&#xa0;" ID="ID_50335265" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
<node TEXT="SVM-Based Control System for a Robot Manipulator - Foudil " FOLDED="true" ID="ID_1471999162" CREATED="1557224068605" MODIFIED="1557225479692" LINK="https://journals.sagepub.com/doi/full/10.5772/51192">
<node TEXT="Real systems are usually non-linear ill-defined have variable parameters and are subject to external disturbances. Modelling these systems is often an approxi." ID="ID_1918017536" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
<node TEXT="1.4. Support Vector Machines &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_1402554540" CREATED="1557224068605" MODIFIED="1557225479692" LINK="http://scikit-learn.org/stable/modules/svm.html">
<node TEXT="Support vector machines (SVMs) are a set of supervised learning methods used for .. This randomness can be controlled with the random_state parameter." ID="ID_452715217" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
</node>
</node>
<node TEXT="Support Vector Regression" ID="ID_1718088884" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
<node TEXT="Blog" ID="ID_59721775" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Analytics Vidya " ID="ID_261488283" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
</node>
</node>
</node>
<node TEXT="Decision Trees and Ensemble Learning" POSITION="left" ID="ID_935460987" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<edge COLOR="#00cc33"/>
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Decision Trees" FOLDED="true" ID="ID_1307049531" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Impurity Measures" ID="ID_1981315952" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Decision Trees Impurity Measures#$D$#" FOLDED="true" ID="ID_63069496" CREATED="1557224068606" MODIFIED="1557225520011">
<icon BUILTIN="stop-sign"/>
<node TEXT="Decision tree learning - Wikipedia" FOLDED="true" ID="ID_1901347475" CREATED="1557224068606" MODIFIED="1557225479692" LINK="https://en.wikipedia.org/wiki/Decision_tree_learning">
<node TEXT="Used by the CART (classification and regression tree) algorithm for classification trees Gini impurity is a measure of how often a&#xa0;" ID="ID_278652681" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
<node TEXT="Tutorial on Decision Tree: measure impurity" FOLDED="true" ID="ID_1322256713" CREATED="1557224068606" MODIFIED="1557225479692" LINK="https://people.revoledu.com/kardi/tutorial/DecisionTree/how-to-measure-impurity.htm">
<node TEXT="Most well known indices to measure degree of impurity are entropy gini index and classification error. The formulas are given below. Entropy. Gini Index." ID="ID_303019602" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
<node TEXT="Decision trees" FOLDED="true" ID="ID_1283537082" CREATED="1557224068606" MODIFIED="1557225479692" LINK="https://people.cs.pitt.edu/~milos/courses/cs2750-Spring03/lectures/class19.pdf">
<node TEXT="How to construct the decision tree? &#x2022; Top-bottom algorithm: &#x2013; Find the best split condition (quantified based on the impurity measure). &#x2013; Stops when no&#xa0;" ID="ID_1533922489" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
<node TEXT="Decision Trees - RDD-based API - Spark 2.2.0 Documentation" FOLDED="true" ID="ID_1729966287" CREATED="1557224068606" MODIFIED="1557225479692" LINK="https://spark.apache.org/docs/2.2.0/mllib-decision-tree.html">
<node TEXT="The current implementation provides two impurity measures for classification (Gini impurity and entropy) and&#xa0;" ID="ID_1483559422" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
<node TEXT="Chapter 3 : Decision Tree Classifier &#x2014; Theory &#x2013; Machine Learning " FOLDED="true" ID="ID_1802973162" CREATED="1557224068606" MODIFIED="1557225479692" LINK="https://medium.com/machine-learning-101/chapter-3-decision-trees-theory-e7398adac567">
<node TEXT="May 11 2017  In second part we modify spam classification code for decision tree  of randomness of elements or in other words it is measure of impurity." ID="ID_1897131528" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
<node TEXT="decision-tree-binary.." FOLDED="true" ID="ID_839165807" CREATED="1557224068606" MODIFIED="1557225479692" LINK="https://sebastianraschka.com/faq/docs/decision-tree-binary.html">
<node TEXT="Why are implementations of decision tree algorithms usually binary and what  Now the two impurity measures or splitting criteria that are commonly used in&#xa0;" ID="ID_716565497" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
<node TEXT="machine learning - Gini Impurity vs Entropy - Data Science Stack " FOLDED="true" ID="ID_1024125170" CREATED="1557224068606" MODIFIED="1557225479692" LINK="https://datascience.stackexchange.com/questions/10228/gini-impurity-vs-entropy">
<node TEXT="Gini impurity and Information Gain Entropy are pretty much the same. . measure has little effect on the performance of single decision tree&#xa0;" ID="ID_707631055" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
<node TEXT="Robust impurity measures in decision trees | SpringerLink" FOLDED="true" ID="ID_570463170" CREATED="1557224068606" MODIFIED="1557225479692" LINK="https://link.springer.com/chapter/10.1007/978-4-431-65950-1_21">
<node TEXT="Tree-based methods are a statistical procedure for automatic learning from data their main characteristic being the simplicity of the results obtained. Their virtue&#xa0;" ID="ID_1603459830" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
<node TEXT="Gini Impurity (With Examples) - Bambiellis Blog" FOLDED="true" ID="ID_1839481767" CREATED="1557224068606" MODIFIED="1557225479692" LINK="https://bambielli.com/til/2017-10-29-gini-impurity/">
<node TEXT="Oct 29 2017   about Gini Impurity: another metric that is used when training decision trees.  Gini Impurity is a measurement of the likelihood of an incorrect&#xa0;" ID="ID_1890918493" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
<node TEXT="Classification and Regression Trees" FOLDED="true" ID="ID_1071674589" CREATED="1557224068606" MODIFIED="1557225479692" LINK="http://mason.gmu.edu/~jgentle/csi772/16s/L10_CART_16s.pdf">
<node TEXT="Nodes are split based on their &#x201c;impurity&#x201d;. Impurity is a measure of how badly the observations at a given node fit the model. In a regression tree for example the&#xa0;" ID="ID_819647952" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
</node>
</node>
<node TEXT="Feature Importance" ID="ID_224344853" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Decision Trees Feature Importance#$D$#" FOLDED="true" ID="ID_460895492" CREATED="1557224068606" MODIFIED="1557225483440">
<icon BUILTIN="stop-sign"/>
<node TEXT="The Mathematics of Decision Trees Random Forest and Feature " FOLDED="true" ID="ID_1586708443" CREATED="1557224068606" MODIFIED="1557225479692" LINK="https://medium.com/@srnghn/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3">
<node TEXT="May 11 2018  The Mathematics of Decision Trees Random Forest and Feature Importance in Scikit-learn and Spark. Go to the profile of Stacey Ronaghan." ID="ID_1200358761" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
<node TEXT="sklearn.tree.DecisionTreeClassifier &#x2014; scikit-learn 0.20.3 " FOLDED="true" ID="ID_964831953" CREATED="1557224068606" MODIFIED="1557225479692" LINK="http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html">
<node TEXT="For the default settings of a decision tree on large datasets setting this to true . The importance of a feature is computed as the (normalized) total reduction of&#xa0;" ID="ID_1205236181" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
<node TEXT="Feature Importance and Feature Selection With XGBoost in Python" FOLDED="true" ID="ID_963741685" CREATED="1557224068606" MODIFIED="1557225479692" LINK="https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/">
<node TEXT="Aug 31 2016  A benefit of using ensembles of decision tree methods like gradient boosting is that they can automatically provide estimates of feature&#xa0;" ID="ID_675036867" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
<node TEXT="scikit learn - feature importance calculation in decision trees - Stack " FOLDED="true" ID="ID_1535880502" CREATED="1557224068606" MODIFIED="1557225479692" LINK="https://stackoverflow.com/questions/49170296/scikit-learn-feature-importance-calculation-in-decision-trees">
<node TEXT="Mar 8 2018  I think feature importance depends on the implementation so we need to look at the documentation of scikit-learn. The feature importances." ID="ID_1548201535" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
<node TEXT="machine learning - Interpreting Decision Tree in context of feature " FOLDED="true" ID="ID_308528929" CREATED="1557224068606" MODIFIED="1557225479692" LINK="https://datascience.stackexchange.com/questions/16693/interpreting-decision-tree-in-context-of-feature-importances">
<node TEXT="It is not necessary that the more important a feature is then the higher its node is at the decision tree. This is simply because different criteria&#xa0;" ID="ID_1113959848" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
<node TEXT="StatQuest: Decision Trees Part 2 - Feature Selection and Missing Data" FOLDED="true" ID="ID_1768579772" CREATED="1557224068606" MODIFIED="1557225479692" LINK="https://www.youtube.com/watch?v=wpNl-JwwplA">
<node TEXT="Jan 29 2018  This is just a short follow up to last weeks StatQuest where we introduced decision trees. Here we show how decision trees deal with variables&#xa0;" ID="ID_919075038" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
<node TEXT="Variable Importance using Decision Trees" FOLDED="true" ID="ID_142217685" CREATED="1557224068606" MODIFIED="1557225479692" LINK="https://www.cs.cmu.edu/~atalwalk/dstump_nips17.pdf">
<node TEXT="Decision trees and random forests are well established models that not only offer good predictive performance but also provide rich feature importance&#xa0;" ID="ID_17550896" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
<node TEXT="Variable Importance Using Decision Trees" FOLDED="true" ID="ID_858348512" CREATED="1557224068606" MODIFIED="1557225479692" LINK="https://papers.nips.cc/paper/6646-variable-importance-using-decision-trees">
<node TEXT="Abstract. Decision trees and random forests are well established models that not only offer good predictive performance but also provide rich feature importance&#xa0;" ID="ID_1226398400" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
<node TEXT="Feature Importance Measures for Tree Models &#x2014; Part II" FOLDED="true" ID="ID_1876819646" CREATED="1557224068606" MODIFIED="1557225479692" LINK="https://becominghuman.ai/feature-importance-measures-for-tree-models-part-ii-20c9ff4329b">
<node TEXT="Oct 29 2017  In part II were going to apply the algorithms introduced in part I and explore the features in the Mushroom Classification dataset. Honestly&#xa0;" ID="ID_297131636" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
<node TEXT="Feature Selection Using Random forest &#x2013; Towards Data Science" FOLDED="true" ID="ID_1529820231" CREATED="1557224068606" MODIFIED="1557225479692" LINK="https://towardsdatascience.com/feature-selection-using-random-forest-26d7b747597f">
<node TEXT="Dec 14 2018  This interpretability is given by the fact that it is straightforward to derive the importance of each variable on the tree decision. In other words it is&#xa0;" ID="ID_158305892" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
</node>
</node>
<node TEXT="DT Classification using Scikit-learn" ID="ID_1876294969" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Decision Trees Classification using Scikit-learn#$D$#" FOLDED="true" ID="ID_1898764742" CREATED="1557224068606" MODIFIED="1557225483440">
<icon BUILTIN="stop-sign"/>
<node TEXT="1.10. Decision Trees &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_99229823" CREATED="1557224068606" MODIFIED="1557225479692" LINK="http://scikit-learn.org/stable/modules/tree.html">
<node TEXT="Classification; 1.10.2. Regression; 1.10.3. Multi-output problems; 1.10.4. Complexity; 1.10.5. Tips on practical use; 1.10.6. Tree algorithms: ID3 C4.5 C5.0 and&#xa0;" ID="ID_1663172826" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
<node TEXT="Decision Tree Classification in Python (article) - DataCamp" FOLDED="true" ID="ID_156738063" CREATED="1557224068606" MODIFIED="1557225479692" LINK="https://www.datacamp.com/community/tutorials/decision-tree-classification-python">
<node TEXT="Dec 28 2018  In this tutorial learn Decision Tree Classification attribute selection measures and how to build and optimize Decision Tree Classifier using&#xa0;" ID="ID_923372119" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
<node TEXT="sklearn.tree.DecisionTreeClassifier &#x2014; scikit-learn 0.20.3 " FOLDED="true" ID="ID_1941437647" CREATED="1557224068606" MODIFIED="1557225479692" LINK="http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html">
<node TEXT="Examples using sklearn.tree.  A decision tree classifier. . For example for four-class multilabel classification weights should be [{0: 1 1: 1} {0: 1 1: 5} {0: 1&#xa0;" ID="ID_1187571866" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
<node TEXT="Building Decision Tree Algorithm in Python with scikit learn" FOLDED="true" ID="ID_1803460846" CREATED="1557224068606" MODIFIED="1557225479692" LINK="http://dataaspirant.com/2017/02/01/decision-tree-algorithm-python-with-scikit-learn/">
<node TEXT="Feb 1 2017  Learn how to build one of the cutest and lovable supervised algorithms Decision Tree classifier in Python using the scikit-learn package." ID="ID_1670664944" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
<node TEXT="Decision Tree Classifier in Python using Scikit-learn &#x2013; Ben Alex Keen" FOLDED="true" ID="ID_1655361686" CREATED="1557224068606" MODIFIED="1557225479692" LINK="http://benalexkeen.com/decision-tree-classifier-in-python-using-scikit-learn/">
<node TEXT="May 31 2017  Decision Tree Classifier in Python using Scikit-learn. Decision Trees can be used as classifier or regression models. A tree structure is&#xa0;" ID="ID_519709242" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
<node TEXT="Multiclass classification using scikit-learn - GeeksforGeeks" FOLDED="true" ID="ID_517322704" CREATED="1557224068606" MODIFIED="1557225479692" LINK="https://www.geeksforgeeks.org/multiclass-classification-using-scikit-learn/">
<node TEXT="In a multiclass classification we train a classifier using our training data and use  In the following code snippet we train a decision tree classifier in scikit-learn." ID="ID_1354850442" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
<node TEXT="Decision Trees in Python with Scikit-Learn" FOLDED="true" ID="ID_152990814" CREATED="1557224068606" MODIFIED="1557225479692" LINK="https://stackabuse.com/decision-trees-in-python-with-scikit-learn/">
<node TEXT="Feb 28 2018  Implementing Decision Trees with Python Scikit Learn  well solve both classification as well as regression problems using the decision tree." ID="ID_1998621450" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
<node TEXT="Machine Learning with Scikit-Learn - The Cancer Dataset - 11 " FOLDED="true" ID="ID_1068458864" CREATED="1557224068606" MODIFIED="1557225479692" LINK="https://www.youtube.com/watch?v=9YcMzsFvfxU">
<node TEXT="Apr 5 2017  In this machine learning series I will work on the Wisconsin Breast Cancer  with Scikit-Learn - The Cancer Dataset - 11 - Decision Trees 1." ID="ID_439286280" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
<node TEXT="Decision Trees in scikit-learn - A Data Analyst" FOLDED="true" ID="ID_717564183" CREATED="1557224068606" MODIFIED="1557225479692" LINK="https://adataanalyst.com/scikit-learn/decision-trees-scikit-learn/">
<node TEXT="Decision Trees in scikit-learn  Sometimes feature selection may be done by using automatic tools to evaluate and  The preprocessing module of scikit-learn includes a LabelEncoder class whose fit . Training a decision tree classifier&#xa0;" ID="ID_1209872036" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
<node TEXT="Decision trees in python with scikit-learn and pandas &#x2014; chris sandbox" FOLDED="true" ID="ID_388316299" CREATED="1557224068606" MODIFIED="1557225479692" LINK="http://chrisstrelioff.ws/sandbox/2015/06/08/decision_trees_in_python_with_scikit_learn_and_pandas.html">
<node TEXT="Jun 8 2015  Decision trees in python with scikit-learn and pandas. In this post I will cover decision trees (for classification) in python using scikit-learn and&#xa0;" ID="ID_1139467910" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
</node>
</node>
<node TEXT="Interactive Visualization " ID="ID_1721520381" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
<node TEXT="Decision Tree Visualization Blog(T2) " ID="ID_84705570" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Decision Trees#$D$#" FOLDED="true" ID="ID_893483303" CREATED="1557224068605" MODIFIED="1557225513316">
<icon BUILTIN="stop-sign"/>
<node TEXT="Decision tree - Wikipedia" FOLDED="true" ID="ID_1033305211" CREATED="1557224068605" MODIFIED="1557225479692" LINK="https://en.wikipedia.org/wiki/Decision_tree">
<node TEXT="A decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences including chance event outcomes resource&#xa0;" ID="ID_1203388976" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
<node TEXT="Decision Tree - GeeksforGeeks" FOLDED="true" ID="ID_211298486" CREATED="1557224068605" MODIFIED="1557225479692" LINK="https://www.geeksforgeeks.org/decision-tree/">
<node TEXT="A Decision tree is a flowchart like tree structure where each internal node  Decision trees classify instances by sorting them down the tree from the root to&#xa0;" ID="ID_359389851" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
<node TEXT="Decision tree learning - Wikipedia" FOLDED="true" ID="ID_1894028188" CREATED="1557224068605" MODIFIED="1557225479692" LINK="https://en.wikipedia.org/wiki/Decision_tree_learning">
<node TEXT="In computer science Decision tree learning uses a decision tree (as a predictive model) to go from observations about an item (represented in the branches) to&#xa0;" ID="ID_1150337269" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
<node TEXT="1.10. Decision Trees &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_1048356266" CREATED="1557224068605" MODIFIED="1557225479692" LINK="http://scikit-learn.org/stable/modules/tree.html">
<node TEXT="Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the&#xa0;" ID="ID_491540004" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
<node TEXT="Decision Tree Analysis - Decision Skills from MindTools.com" FOLDED="true" ID="ID_1629534620" CREATED="1557224068605" MODIFIED="1557225479692" LINK="https://www.mindtools.com/dectree.html">
<node TEXT="Decision Trees are excellent tools for helping you to choose between several courses of action. They provide a highly effective structure within which you can lay&#xa0;" ID="ID_536197979" CREATED="1557224068605" MODIFIED="1557224068605"/>
</node>
<node TEXT="What is a Decision Tree Diagram | Lucidchart" FOLDED="true" ID="ID_1103367643" CREATED="1557224068605" MODIFIED="1557225479692" LINK="https://www.lucidchart.com/pages/decision-tree">
<node TEXT="Need to break down a complex decision? Try using a decision tree maker. Read on to find out all about decision trees including what they are how theyre used&#xa0;" ID="ID_330352671" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
<node TEXT="Decision Trees in Machine Learning &#x2013; Towards Data Science" FOLDED="true" ID="ID_1880441549" CREATED="1557224068606" MODIFIED="1557225479692" LINK="https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052">
<node TEXT="May 17 2017  A decision tree is drawn upside down with its root at the top. In the image on the left the bold text in black represents a condition/internal node&#xa0;" ID="ID_1808883553" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
<node TEXT="Decision Tree" FOLDED="true" ID="ID_917176974" CREATED="1557224068606" MODIFIED="1557225479692" LINK="https://www.investopedia.com/terms/d/decision-tree.asp">
<node TEXT="Jul 23 2018  A decision tree os a schematic tree-shaped diagram used to determine a course of action or show a statistical probability." ID="ID_853773909" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
<node TEXT="Chapter 4: Decision Trees Algorithms &#x2013; Deep Math Machine " FOLDED="true" ID="ID_72737617" CREATED="1557224068606" MODIFIED="1557225479692" LINK="https://medium.com/deep-math-machine-learning-ai/chapter-4-decision-trees-algorithms-b93975f7a1f1">
<node TEXT="Oct 6 2017  Decision tree is one of the most popular machine learning algorithms used all along This story I wanna talk about it so lets get started!" ID="ID_1184366064" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
<node TEXT="Decision Tree 1: how it works - YouTube" FOLDED="true" ID="ID_206692042" CREATED="1557224068606" MODIFIED="1557225479692" LINK="https://www.youtube.com/watch?v=eKD5gxPPeY0">
<node TEXT="Jan 19 2014  Full lecture: http://bit.ly/D-Tree A Decision Tree recursively splits training data into subsets based on the value of a single attribute. Each split&#xa0;" ID="ID_1249102854" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
</node>
</node>
</node>
<node TEXT=" Ensemble Learning" FOLDED="true" ID="ID_531884128" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Random Forests" ID="ID_385062271" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Ensemble Learning#$D$#" FOLDED="true" ID="ID_1981785204" CREATED="1557224068606" MODIFIED="1557225483440">
<icon BUILTIN="stop-sign"/>
<node TEXT="Ensemble learning - Wikipedia" FOLDED="true" ID="ID_146625453" CREATED="1557224068606" MODIFIED="1557225479692" LINK="https://en.wikipedia.org/wiki/Ensemble_learning">
<node TEXT="In statistics and machine learning ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any&#xa0;" ID="ID_78141601" CREATED="1557224068606" MODIFIED="1557224068606"/>
</node>
<node TEXT="Ensemble Learning" FOLDED="true" ID="ID_531902139" CREATED="1557224068607" MODIFIED="1557225479692" LINK="https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/springerEBR09.pdf">
<node TEXT="Ensemble Learning. Zhi-Hua Zhou. National Key Laboratory for Novel Software Technology Nanjing University Nanjing 210093 China zhouzh@nju.edu.cn." ID="ID_1354227912" CREATED="1557224068607" MODIFIED="1557224068607"/>
</node>
<node TEXT="Ensemble Learning to Improve Machine Learning Results" FOLDED="true" ID="ID_1107515149" CREATED="1557224068607" MODIFIED="1557225479692" LINK="https://blog.statsbot.co/ensemble-learning-d1dcd548e936">
<node TEXT="Aug 22 2017  Ensemble learning helps improve machine learning results by combining several models. Ensemble methods allow the production of better&#xa0;" ID="ID_1635488830" CREATED="1557224068607" MODIFIED="1557224068607"/>
</node>
<node TEXT="Ensemble learning - Scholarpedia" FOLDED="true" ID="ID_1665422056" CREATED="1557224068607" MODIFIED="1557225479692" LINK="http://www.scholarpedia.org/article/Ensemble_learning">
<node TEXT="Dec 22 2008  Ensemble learning is the process by which multiple models such as classifiers or experts are strategically generated and combined to solve a&#xa0;" ID="ID_606861169" CREATED="1557224068607" MODIFIED="1557224068607"/>
</node>
<node TEXT="Simple guide for ensemble learning methods &#x2013; Towards Data Science" FOLDED="true" ID="ID_1088482545" CREATED="1557224068607" MODIFIED="1557225479692" LINK="https://towardsdatascience.com/simple-guide-for-ensemble-learning-methods-d87cc68705a2">
<node TEXT="Feb 26 2019  Ensemble models in machine learning combine the decisions from multiple models to improve the overall performance. They operate on the&#xa0;" ID="ID_1860197126" CREATED="1557224068607" MODIFIED="1557224068607"/>
</node>
<node TEXT="A Comprehensive Guide to Ensemble Learning (with Python codes)" FOLDED="true" ID="ID_1249769872" CREATED="1557224068607" MODIFIED="1557225479692" LINK="https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/">
<node TEXT="Jun 18 2018  Ensemble models in machine learning operate on a similar idea. They combine the decisions from multiple models to improve the overall&#xa0;" ID="ID_1345727211" CREATED="1557224068607" MODIFIED="1557224068607"/>
</node>
<node TEXT="Ensemble Learning &#x2013; Together we grow strong schools" FOLDED="true" ID="ID_1875279123" CREATED="1557224068607" MODIFIED="1557225479692" LINK="http://www.ensemblelearning.org/">
<node TEXT="Ensemble Learning formerly The College-Ready Promise is a national organization committed to fixing inequities in the education system by improving how&#xa0;" ID="ID_1833060407" CREATED="1557224068607" MODIFIED="1557224068607"/>
</node>
<node TEXT="Basics of Ensemble Learning Explained in Simple English" FOLDED="true" ID="ID_1380459009" CREATED="1557224068607" MODIFIED="1557225479692" LINK="https://www.analyticsvidhya.com/blog/2015/08/introduction-ensemble-learning/">
<node TEXT="Aug 2 2015  Introduction. Ensemble modeling is a powerful way to improve the performance of your model. It usually pays off to apply ensemble learning&#xa0;" ID="ID_1453064985" CREATED="1557224068607" MODIFIED="1557224068607"/>
</node>
<node TEXT="Ensemble learners - YouTube" FOLDED="true" ID="ID_1734992671" CREATED="1557224068607" MODIFIED="1557225479692" LINK="https://www.youtube.com/watch?v=Un9zObFjBH0">
<node TEXT="Jun 6 2016  This video is part of the Udacity course Machine Learning for Trading. Watch the full course at https://www.udacity.com/course/ud501." ID="ID_1691979733" CREATED="1557224068607" MODIFIED="1557224068607"/>
</node>
<node TEXT="Ensemble Learning &#x2014; Bagging and Boosting &#x2013; Becoming Human " FOLDED="true" ID="ID_154028552" CREATED="1557224068607" MODIFIED="1557225479692" LINK="https://becominghuman.ai/ensemble-learning-bagging-and-boosting-d20f38be9b1e">
<node TEXT="Jul 3 2018  Bagging and Boosting are similar in that they are both ensemble techniques where a set of weak learners are combined to create a strong&#xa0;" ID="ID_1463893649" CREATED="1557224068607" MODIFIED="1557224068607"/>
</node>
</node>
<node TEXT="Random Forests    #$D$#" FOLDED="true" ID="ID_186862183" CREATED="1557224068607" MODIFIED="1557225483440">
<icon BUILTIN="stop-sign"/>
<node TEXT="Random forest - Wikipedia" FOLDED="true" ID="ID_280319701" CREATED="1557224068607" MODIFIED="1557225479692" LINK="https://en.wikipedia.org/wiki/Random_forest">
<node TEXT="Random forests or random decision forests are an ensemble learning method for classification regression and other tasks that operates by constructing a&#xa0;" ID="ID_399435530" CREATED="1557224068607" MODIFIED="1557224068607"/>
</node>
<node TEXT="The Random Forest Algorithm &#x2013; Towards Data Science" FOLDED="true" ID="ID_67794332" CREATED="1557224068607" MODIFIED="1557225479692" LINK="https://towardsdatascience.com/the-random-forest-algorithm-d457d499ffcd">
<node TEXT="Feb 22 2018  Random Forest is a flexible easy to use machine learning algorithm that produces even without hyper-parameter tuning a great result most of&#xa0;" ID="ID_642552941" CREATED="1557224068607" MODIFIED="1557224068607"/>
</node>
<node TEXT="Random forests - classification description" FOLDED="true" ID="ID_1078527336" CREATED="1557224068607" MODIFIED="1557225479692" LINK="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm">
<node TEXT="Random Forests(tm) is a trademark of Leo Breiman and Adele Cutler and is  In the original paper on random forests it was shown that the forest error rate&#xa0;" ID="ID_1590899500" CREATED="1557224068607" MODIFIED="1557224068607"/>
</node>
<node TEXT="3.2.4.3.1. sklearn.ensemble.RandomForestClassifier &#x2014; scikit-learn " FOLDED="true" ID="ID_535129743" CREATED="1557224068607" MODIFIED="1557225479692" LINK="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">
<node TEXT="A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the&#xa0;" ID="ID_604302479" CREATED="1557224068607" MODIFIED="1557224068607"/>
</node>
<node TEXT="Random Forest Simple Explanation &#x2013; Will Koehrsen &#x2013; Medium" FOLDED="true" ID="ID_511104111" CREATED="1557224068607" MODIFIED="1557225479692" LINK="https://medium.com/@williamkoehrsen/random-forest-simple-explanation-377895a60d2d">
<node TEXT="Dec 27 2017  Understanding the Random Forest with an intuitive example. When learning a technical concept I find its better to start with a high-level&#xa0;" ID="ID_1612640433" CREATED="1557224068607" MODIFIED="1557224068607"/>
</node>
<node TEXT="Random Forests | SpringerLink" FOLDED="true" ID="ID_1294514012" CREATED="1557224068607" MODIFIED="1557225479692" LINK="https://link.springer.com/article/10.1023/A:1010933404324">
<node TEXT="Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same&#xa0;" ID="ID_79016390" CREATED="1557224068607" MODIFIED="1557224068607"/>
</node>
<node TEXT="Random Forests for Complete Beginners - victorzhou.com" FOLDED="true" ID="ID_130697376" CREATED="1557224068607" MODIFIED="1557225479692" LINK="https://victorzhou.com/blog/intro-to-random-forests/">
<node TEXT="Apr 10 2019  The definitive guide to Random Forests and Decision Trees." ID="ID_81114341" CREATED="1557224068607" MODIFIED="1557224068607"/>
</node>
<node TEXT="Introduction to Random Forests" FOLDED="true" ID="ID_417773122" CREATED="1557224068607" MODIFIED="1557225479692" LINK="https://www.datascience.com/resources/notebooks/random-forest-intro">
<node TEXT="Dec 8 2017  Random forests are a popular ensemble method for building predictive models. This tutorial explains how to fit train and validate a random&#xa0;" ID="ID_891547321" CREATED="1557224068607" MODIFIED="1557224068607"/>
</node>
<node TEXT="StatQuest: Random Forests Part 1 - Building Using and Evaluating " FOLDED="true" ID="ID_842099539" CREATED="1557224068607" MODIFIED="1557225479692" LINK="https://www.youtube.com/watch?v=J4Wdy0Wc_xQ">
<node TEXT="Feb 5 2018  NOTE: At 9:28 I say square when I meant to say square root. Oops! I would correct this minor error if it were easy to do but YouTube doesnt&#xa0;" ID="ID_682345385" CREATED="1557224068607" MODIFIED="1557224068607"/>
</node>
<node TEXT="Ensembles - RDD-based API - Spark 2.4.2 Documentation" FOLDED="true" ID="ID_894581814" CREATED="1557224068607" MODIFIED="1557225479692" LINK="https://spark.apache.org/docs/latest/mllib-ensembles.html">
<node TEXT="Random forests are ensembles of decision trees. Random forests are one of the most successful machine learning models for classification and regression." ID="ID_1189010311" CREATED="1557224068607" MODIFIED="1557224068607"/>
</node>
</node>
<node TEXT="Ensemble methods#$D$#" FOLDED="true" ID="ID_765800242" CREATED="1557224068608" MODIFIED="1557225483441">
<icon BUILTIN="stop-sign"/>
<node TEXT="Ensemble learning - Wikipedia" FOLDED="true" ID="ID_999951715" CREATED="1557224068608" MODIFIED="1557225479708" LINK="https://en.wikipedia.org/wiki/Ensemble_learning">
<node TEXT="In statistics and machine learning ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any&#xa0;" ID="ID_1275689641" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="Ensemble Methods in Machine Learning: What are They and Why " FOLDED="true" ID="ID_306753962" CREATED="1557224068608" MODIFIED="1557225479708" LINK="https://towardsdatascience.com/ensemble-methods-in-machine-learning-what-are-they-and-why-use-them-68ec3f9fef5f">
<node TEXT="Aug 1 2017  Ensemble Methods what are they? Ensemble methods is a machine learning technique that combines several base models in order to&#xa0;" ID="ID_1459089914" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="1.11. Ensemble methods &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_30887187" CREATED="1557224068608" MODIFIED="1557225479708" LINK="http://scikit-learn.org/stable/modules/ensemble.html">
<node TEXT="The goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability&#xa0;" ID="ID_1837757601" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="Ensemble Learning to Improve Machine Learning Results" FOLDED="true" ID="ID_1063102278" CREATED="1557224068608" MODIFIED="1557225479708" LINK="https://blog.statsbot.co/ensemble-learning-d1dcd548e936">
<node TEXT="Aug 22 2017  Ensemble learning helps improve machine learning results by combining several models. Ensemble methods allow the production of better&#xa0;" ID="ID_619706812" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="Ensemble Methods in Machine Learning | Toptal" FOLDED="true" ID="ID_629952067" CREATED="1557224068608" MODIFIED="1557225479708" LINK="https://www.toptal.com/machine-learning/ensemble-methods-machine-learning">
<node TEXT="Ensemble methods are techniques that create multiple models and then combine them to produce improved results. Ensemble methods usually produces more&#xa0;" ID="ID_604464241" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="Ensemble Methods in Machine Learning" FOLDED="true" ID="ID_1569296988" CREATED="1557224068608" MODIFIED="1557225479708" LINK="http://web.engr.oregonstate.edu/~tgd/publications/mcs-ensembles.pdf">
<node TEXT="Abstract. Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a (weighted) vote of their&#xa0;" ID="ID_855531748" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="Ensemble Learning Methods for Deep Learning Neural Networks" FOLDED="true" ID="ID_1813374760" CREATED="1557224068608" MODIFIED="1557225479708" LINK="https://machinelearningmastery.com/ensemble-methods-for-deep-learning-neural-networks/">
<node TEXT="Dec 19 2018  Ensemble Methods to Reduce Variance and Improve Performance of Deep Learning Neural Networks Photo by University of San Franciscos&#xa0;" ID="ID_1367761570" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="Ensemble Methods" FOLDED="true" ID="ID_1050270478" CREATED="1557224068608" MODIFIED="1557225479708" LINK="https://www3.nd.edu/~rjohns15/cse40647.sp14/www/content/lectures/31%20-%20Decision%20Tree%20Ensembles.pdf">
<node TEXT="Ensemble Methods. &#x2022; An ensemble is a set of classifiers that learn a target function and their individual predictions are combined to classify new examples." ID="ID_1106609525" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="13 | ENSEMBLE METHODS" FOLDED="true" ID="ID_1514885250" CREATED="1557224068608" MODIFIED="1557225479708" LINK="http://ciml.info/dl/v0_99/ciml-v0_99-ch13.pdf">
<node TEXT="individuals especially when group members each come in with their own biases. The same is true in machine learning. Ensemble methods are learning models&#xa0;" ID="ID_1282085447" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="Random Forests Decision Trees and Ensemble Methods Explained" FOLDED="true" ID="ID_393429935" CREATED="1557224068608" MODIFIED="1557225479708" LINK="https://www.datascience.com/blog/random-forests-decision-trees-ensemble-methods">
<node TEXT="Dec 4 2018  The random forest first described by Breimen et al (2001) is an ensemble approach for building predictive models. The &#x201c;forest&#x201d; in this&#xa0;" ID="ID_1426526163" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
</node>
</node>
<node TEXT="AdaBoost" ID="ID_268982315" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
<node TEXT="Gradient Tree Boosting" ID="ID_1523033159" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Gradient Tree Boosting#$D$#" FOLDED="true" ID="ID_1472373422" CREATED="1557224068607" MODIFIED="1557225483440">
<icon BUILTIN="stop-sign"/>
<node TEXT="Gradient boosting - Wikipedia" FOLDED="true" ID="ID_1923464127" CREATED="1557224068607" MODIFIED="1557225479692" LINK="https://en.wikipedia.org/wiki/Gradient_boosting">
<node TEXT="Gradient boosting is typically used with decision trees (especially CART trees) of a fixed size as base learners. For this&#xa0;" ID="ID_1295200195" CREATED="1557224068607" MODIFIED="1557224068607"/>
</node>
<node TEXT="Introduction to Boosted Trees &#x2014; xgboost 0.83.dev0 documentation" FOLDED="true" ID="ID_1292391059" CREATED="1557224068607" MODIFIED="1557225479692" LINK="https://xgboost.readthedocs.io/en/latest/tutorials/model.html">
<node TEXT="The gradient boosted trees has been around for a while and there are a lot of materials on the topic. This tutorial will explain boosted trees in a self-contained&#xa0;" ID="ID_1191603098" CREATED="1557224068607" MODIFIED="1557224068607"/>
</node>
<node TEXT="A Gentle Introduction to the Gradient Boosting Algorithm for Machine " FOLDED="true" ID="ID_1198766274" CREATED="1557224068607" MODIFIED="1557225479692" LINK="https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/">
<node TEXT="Sep 9 2016  This framework was further developed by Friedman and called Gradient Boosting Machines. Later called just gradient boosting or gradient tree&#xa0;" ID="ID_1223856890" CREATED="1557224068607" MODIFIED="1557224068607"/>
</node>
<node TEXT="Gradient Boosting from scratch &#x2013; ML Review &#x2013; Medium" FOLDED="true" ID="ID_1909552198" CREATED="1557224068607" MODIFIED="1557225479692" LINK="https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d">
<node TEXT="Dec 8 2017  Although tree-based models (considering decision tree as base models for our gradient boosting here) are not based on such assumptions&#xa0;" ID="ID_187912087" CREATED="1557224068607" MODIFIED="1557224068607"/>
</node>
<node TEXT="Introduction to Boosted Trees" FOLDED="true" ID="ID_1108572760" CREATED="1557224068607" MODIFIED="1557225479692" LINK="https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf">
<node TEXT="Oct 22 2014  Outline. &#x2022; Review of key concepts of supervised learning. &#x2022; Regression Tree and Ensemble (What are we Learning). &#x2022; Gradient Boosting (How&#xa0;" ID="ID_1881013114" CREATED="1557224068607" MODIFIED="1557224068607"/>
</node>
<node TEXT="A Kaggle Master Explains Gradient Boosting | No Free Hunch" FOLDED="true" ID="ID_793171977" CREATED="1557224068607" MODIFIED="1557225479692" LINK="http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/">
<node TEXT="Jan 23 2017  A particular implementation of gradient boosting XGBoost  Here in lies the drawback to using a single decision/regression tree &#x2013; it fails to&#xa0;" ID="ID_1536968124" CREATED="1557224068607" MODIFIED="1557224068607"/>
</node>
<node TEXT="Understanding Gradient Boosting Machines &#x2013; Towards Data Science" FOLDED="true" ID="ID_1830385268" CREATED="1557224068607" MODIFIED="1557225479692" LINK="https://towardsdatascience.com/understanding-gradient-boosting-machines-9be756fe76ab">
<node TEXT="Nov 3 2018  In boosting each new tree is a fit on a modified version of the original data set. The gradient boosting algorithm (gbm) can be most easily&#xa0;" ID="ID_1982376115" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="3.2.4.3.5. sklearn.ensemble.GradientBoostingClassifier &#x2014; scikit " FOLDED="true" ID="ID_31238248" CREATED="1557224068608" MODIFIED="1557225479692" LINK="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html">
<node TEXT="In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special&#xa0;" ID="ID_1051358788" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="Introduction Boosting Trees for Regression and Classification" FOLDED="true" ID="ID_365586483" CREATED="1557224068608" MODIFIED="1557225479692" LINK="http://www.statsoft.com/Textbook/Boosting-Trees-Regression-Classification">
<node TEXT="Boosting Trees for Regression and Classification Introductory Overview. The general computational approach of stochastic gradient boosting is also known by&#xa0;" ID="ID_45959481" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="3.2.4.3.6. sklearn.ensemble.GradientBoostingRegressor &#x2014; scikit " FOLDED="true" ID="ID_945121328" CREATED="1557224068608" MODIFIED="1557225479692" LINK="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html">
<node TEXT="GradientBoostingRegressor (loss=ls learning_rate=0.1 n_estimators=100  In each stage a regression tree is fit on the negative gradient of the given loss&#xa0;" ID="ID_1354851386" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
</node>
</node>
<node TEXT="Voting Classifier" ID="ID_375887774" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Voting Classifier#$D$#" FOLDED="true" ID="ID_532094056" CREATED="1557224068608" MODIFIED="1557225483441">
<icon BUILTIN="stop-sign"/>
<node TEXT="sklearn.ensemble.VotingClassifier &#x2014; scikit-learn 0.20.3 " FOLDED="true" ID="ID_1456377817" CREATED="1557224068608" MODIFIED="1557225479692" LINK="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html">
<node TEXT="Parameters: estimators : list of (string estimator) tuples. Invoking the fit method on the VotingClassifier will fit clones of those original estimators that will be stored&#xa0;" ID="ID_1418404870" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="EnsembleVoteClassifier - mlxtend" FOLDED="true" ID="ID_754319798" CREATED="1557224068608" MODIFIED="1557225479692" LINK="http://rasbt.github.io/mlxtend/user_guide/classifier/EnsembleVoteClassifier/">
<node TEXT="Soft Voting/Majority Rule classifier for  an ensemble of well-calibrated classifiers." ID="ID_1917613622" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="1.11. Ensemble methods &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_524373980" CREATED="1557224068608" MODIFIED="1557225479692" LINK="http://scikit-learn.org/stable/modules/ensemble.html">
<node TEXT="Weighted Average Probabilities (Soft Voting); 1.11.5.3.  means a diverse set of classifiers is created by introducing randomness in the classifier construction." ID="ID_1744549542" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="Better predictions: stacking with VotingClassifier | Kaggle" FOLDED="true" ID="ID_305478643" CREATED="1557224068608" MODIFIED="1557225479708" LINK="https://www.kaggle.com/den3b81/better-predictions-stacking-with-votingclassifier">
<node TEXT="Feb 23 2017  As you may well know in most Kaggle competitions the winners usually resort to stacking or meta-ensembling which is a technique involving&#xa0;" ID="ID_52299668" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="Implementing a Weighted Majority Rule Ensemble Classifier" FOLDED="true" ID="ID_1728679027" CREATED="1557224068608" MODIFIED="1557225479708" LINK="https://sebastianraschka.com/Articles/2014_ensemble_classifier.html">
<node TEXT="classifier 1 - class 1; classifier 2  the majority rule voting will be applied to&#xa0;" ID="ID_768423764" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="Use Voting Classifiers &#x2014; dask-ml 0.12.1 documentation" FOLDED="true" ID="ID_1540151470" CREATED="1557224068608" MODIFIED="1557225479708" LINK="https://ml.dask.org/examples/voting-classifier.html">
<node TEXT="A Voting classifier model combines multiple different models (i.e.  What follows is an example of how one would deploy a voting classifier model in dask (using&#xa0;" ID="ID_471703458" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="How does voting between two classifiers work in sklearn? - Stack " FOLDED="true" ID="ID_672857320" CREATED="1557224068608" MODIFIED="1557225479708" LINK="https://stackoverflow.com/questions/48528933/how-does-voting-between-two-classifiers-work-in-sklearn">
<node TEXT="Jan 30 2018  Make sure that if you set voting=soft then the classifiers you provide can also calculate  To see the confidence of each classifier you can do:" ID="ID_1177256187" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="Voting classifier - Machine Learning Algorithms - Second Edition " FOLDED="true" ID="ID_1793742209" CREATED="1557224068608" MODIFIED="1557225479708" LINK="https://www.oreilly.com/library/view/machine-learning-algorithms/9781789347999/e1a7f25d-f21b-4194-befd-771d708b25aa.xhtml">
<node TEXT="Voting classifier A very interesting ensemble solution (which can be considered as part of the stacking subset) is offered by the VotingClassifier class which isnt&#xa0;" ID="ID_1210415007" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="Ensemble Machine Learning Algorithms in Python with scikit-learn" FOLDED="true" ID="ID_589502561" CREATED="1557224068608" MODIFIED="1557225479708" LINK="https://machinelearningmastery.com/ensemble-machine-learning-algorithms-python-scikit-learn/">
<node TEXT="Jun 3 2016  Voting. Building multiple models (typically of differing types) and simple .. A Voting Classifier can then be used to wrap your models and&#xa0;" ID="ID_1040879095" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="sklearn.ensemble.VotingClassifier Python Example" FOLDED="true" ID="ID_821860061" CREATED="1557224068608" MODIFIED="1557225479708" LINK="https://www.programcreek.com/python/example/102435/sklearn.ensemble.VotingClassifier">
<node TEXT="This page provides Python code examples for sklearn.ensemble.VotingClassifier." ID="ID_1176571648" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
</node>
</node>
</node>
<node TEXT="Ensemble methods" FOLDED="true" ID="ID_1926968862" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Bagging" ID="ID_1434499741" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
<node TEXT="Boosting" ID="ID_496409952" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
<node TEXT="Random Forests" ID="ID_645132124" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
<node TEXT="Blog" ID="ID_1134223840" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Analytics Vidya " ID="ID_223995945" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
</node>
<node TEXT="Code " ID="ID_1315002497" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
</node>
<node TEXT=" Introduction to Meta Classifier:" FOLDED="true" ID="ID_1527896511" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Concept of Weak Learner" ID="ID_946810075" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Introduction to Meta Classifier:#$D$#" FOLDED="true" ID="ID_1506215767" CREATED="1557224068608" MODIFIED="1557225483441">
<icon BUILTIN="stop-sign"/>
<node TEXT="Classifiers that do more &#x2013; Meta Classifiers &#x2014; PyMVPA 2.6.5.dev1 " FOLDED="true" ID="ID_304549556" CREATED="1557224068608" MODIFIED="1557225479708" LINK="http://www.pymvpa.org/tutorial_meta_classifiers.html">
<node TEXT="Of course there is a solution to this problem &#x2013; a meta-classifier. This is a classifier that . In this tutorial part we took a look at classifiers. We have seen that&#xa0;" ID="ID_1484192625" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="From zero to research &#x2014; An introduction to Meta-learning" FOLDED="true" ID="ID_834453325" CREATED="1557224068608" MODIFIED="1557225479708" LINK="https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a">
<node TEXT="Apr 3 2018  Our introduction to meta-learning goes from zero to current research  and its recent developments as well as the Reptile algorithm of OpenAI." ID="ID_1379663701" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="Meta Learning" FOLDED="true" ID="ID_1816922276" CREATED="1557224068608" MODIFIED="1557225479708" LINK="http://www.cs.ubc.ca/labs/beta/Courses/CPSC532H-13/Slides/content-session-4-slides.pdf">
<node TEXT="Definition. Meta Learning affects the hypothesis space for the learning algorithm by either: Changing the hypothesis space of the learning algorithms&#xa0;" ID="ID_1150456924" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="An Introduction to Meta-Learning &#x2013; WalmartLabs &#x2013; Medium" FOLDED="true" ID="ID_1759086633" CREATED="1557224068608" MODIFIED="1557225479708" LINK="https://medium.com/walmartlabs/an-introduction-to-meta-learning-ced7072b80e7">
<node TEXT="Mar 14 2019  At Walmart Labs we utilize meta-learning every day &#x2014; whether its in our  our classifier f&#x3b8; with parameter &#x3b8; outputs a log probability of a data&#xa0;" ID="ID_1796048671" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="Fuzzy-belief K-nearest neighbor classifier for uncertain data - IEEE " FOLDED="true" ID="ID_279593376" CREATED="1557224068608" MODIFIED="1557225479708" LINK="https://ieeexplore.ieee.org/document/6916242/">
<node TEXT="The introduction of meta-classes in the classification procedure reduces the misclassification errors. The ignorant class is employed for outliers detections." ID="ID_433827956" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="Is Combining Classifiers with Stacking Better than Selecting the Best " FOLDED="true" ID="ID_1414986698" CREATED="1557224068608" MODIFIED="1557225479708" LINK="https://link.springer.com/content/pdf/10.1023/B:MACH.0000015881.36452.6e.pdf">
<node TEXT="Keywords: multi-response model trees stacking combining classifiers ensembles of classifiers meta-learning. 1. Introduction. An ensemble of classifiers is a&#xa0;" ID="ID_1507093863" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="Pdf here" FOLDED="true" ID="ID_130278849" CREATED="1557224068608" MODIFIED="1557225479708" LINK="http://cgi.di.uoa.gr/~takis/atem03.pdf">
<node TEXT="a meta-level classifier based on the output of base-level information  Wolpert [18] introduced an approach for constructing ensembles of classifiers known." ID="ID_1417457310" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="Introduction to Python Ensembles &#x2013; Dataquest" FOLDED="true" ID="ID_173163914" CREATED="1557224068608" MODIFIED="1557225479708" LINK="https://www.dataquest.io/blog/introduction-to-ensembles/">
<node TEXT="Jan 11 2018  Data is fed to a set of models and a meta learner combine model . For that reason an ensemble that averages classifier predictions is known&#xa0;" ID="ID_1475050967" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="Meta-Learning: Learning to Learn Fast" FOLDED="true" ID="ID_1602897834" CREATED="1557224068608" MODIFIED="1557225479708" LINK="https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html">
<node TEXT="Nov 30 2018  Few-shot classification is an instantiation of meta-learning in the field . All the models introduced below learn embedding vectors of input data&#xa0;" ID="ID_1576244665" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="Rough set Based Ensemble Classifier for Web Page Classification 1 " FOLDED="true" ID="ID_362937990" CREATED="1557224068608" MODIFIED="1557225479708" LINK="https://www.isical.ac.in/~sankar/paper/SAHA-FI-2007.pdf">
<node TEXT="introduced a rough set based meta classifier to classify web pages. The proposed method  used on the decision table to construct a meta classifier. It has been&#xa0;" ID="ID_898849103" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
</node>
<node TEXT="Concept of Weak Learner#$D$#" FOLDED="true" ID="ID_620234773" CREATED="1557224068608" MODIFIED="1557225483441">
<icon BUILTIN="stop-sign"/>
<node TEXT="Boosting (machine learning) - Wikipedia" FOLDED="true" ID="ID_624935703" CREATED="1557224068608" MODIFIED="1557225479708" LINK="https://en.wikipedia.org/wiki/Boosting_(machine_learning)">
<node TEXT="Boosting is a machine learning ensemble meta-algorithm for primarily reducing bias and also variance in supervised learning and a family of machine learning algorithms that convert weak learners to strong ones.  A weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can&#xa0;" ID="ID_1423934869" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="classification - What is meant by weak learner? - Cross Validated" FOLDED="true" ID="ID_838464132" CREATED="1557224068608" MODIFIED="1557225479708" LINK="https://stats.stackexchange.com/questions/82049/what-is-meant-by-weak-learner">
<node TEXT="Weak learner also suggests that many instances of the algorithm are . The idea is that you use a classifier that is well not that good but at&#xa0;" ID="ID_119280415" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="Machine Learning Theory Lecture 13: Weak vs. Strong Learning and " FOLDED="true" ID="ID_719175745" CREATED="1557224068608" MODIFIED="1557225479708" LINK="http://www.jennwv.com/courses/F11/lecture13.pdf">
<node TEXT="Nov 7 2011  whether so-called &#x201c;weak&#x201d; learning implies &#x201c;strong&#x201d; learning (which is simply  A concept class C is strongly PAC learnable using a hypothesis." ID="ID_863237388" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="What is a formal definition of &#x201c;weak learner&#x201d; and an intuitive " FOLDED="true" ID="ID_1576991678" CREATED="1557224068608" MODIFIED="1557225479708" LINK="https://www.quora.com/What-is-a-formal-definition-of-%E2%80%9Cweak-learner%E2%80%9D-and-an-intuitive-explanation-of-it">
<node TEXT="Mar 8 2017  Assume that youre observing pairs where the values are iid observations from distribution and for some function . Assume youre also given&#xa0;" ID="ID_1393183672" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="Weak Classifier - an overview | ScienceDirect Topics" FOLDED="true" ID="ID_1185767407" CREATED="1557224068608" MODIFIED="1557225479708" LINK="https://www.sciencedirect.com/topics/computer-science/weak-classifier">
<node TEXT="Because the weak classifier is less powerful perhaps all training samples cannot be correctly classified. The basic idea of boosting is to assign large weight to&#xa0;" ID="ID_808128030" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="Lecture 6: November 21 2010 6.1 Weak and Strong Learners" FOLDED="true" ID="ID_1836332967" CREATED="1557224068608" MODIFIED="1557225479708" LINK="http://www.cs.tau.ac.il/~mansour/ml-course-10/scribe6.pdf">
<node TEXT="Nov 21 2010  Is it possible to drive those weak algorithms to be strong learners ? . concept class has a weak learning algorithm then there is a PAC&#xa0;" ID="ID_1003598969" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="What is a weak learner? - Stack Overflow" FOLDED="true" ID="ID_1656914656" CREATED="1557224068608" MODIFIED="1557225479708" LINK="https://stackoverflow.com/questions/20435717/what-is-a-weak-learner">
<node TEXT="Dec 7 2013  So my question is what are a few choices for a simple easy to process weak learner? Or do I understand the concept incorrectly and is a&#xa0;" ID="ID_32744153" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="The strength of weak learnability" FOLDED="true" ID="ID_524174236" CREATED="1557224068608" MODIFIED="1557225479708" LINK="http://rob.schapire.net/papers/strengthofweak.pdf">
<node TEXT="given access to a Source of examples of the unknown concept the learner with  The notion of weak learnability was introduced by Kearns and Valiant (1988;&#xa0;" ID="ID_1400767813" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="Weak Learning Boosting and the AdaBoost algorithm &#x2013; Math " FOLDED="true" ID="ID_1868797135" CREATED="1557224068608" MODIFIED="1557225479708" LINK="https://jeremykun.com/2015/05/18/boosting-census/">
<node TEXT="May 18 2015  Well review the model of PAC-learning define what it means to be a weak learner &#x201c;organically&#x201d; come up with the AdaBoost algorithm from&#xa0;" ID="ID_83102063" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
<node TEXT="Text classification by boosting weak learners based on terms and " FOLDED="true" ID="ID_1282552390" CREATED="1557224068608" MODIFIED="1557225479708" LINK="https://ieeexplore.ieee.org/document/1410303/">
<node TEXT="Text classification by boosting weak learners based on terms and concepts. Abstract: Document representations for text classification are typically based on the&#xa0;" ID="ID_755975247" CREATED="1557224068608" MODIFIED="1557224068608"/>
</node>
</node>
</node>
<node TEXT="Concept of Eager Learner" ID="ID_1487735355" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Concept of Eager Learner#$D$#" FOLDED="true" ID="ID_1081849711" CREATED="1557224068608" MODIFIED="1557225483441">
<icon BUILTIN="stop-sign"/>
<node TEXT="Eager learning - Wikipedia" FOLDED="true" ID="ID_1878462121" CREATED="1557224068608" MODIFIED="1557225479708" LINK="https://en.wikipedia.org/wiki/Eager_learning">
<node TEXT="In artificial intelligence eager learning is a learning method in which the system tries to construct a general input-independent target function during training of&#xa0;" ID="ID_651677808" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="Eager - Definition for English-Language Learners from Merriam " FOLDED="true" ID="ID_1858862273" CREATED="1557224068609" MODIFIED="1557225479708" LINK="http://www.learnersdictionary.com/definition/eager">
<node TEXT="Definition of eager written for English Language Learners from the Merriam-Webster Learners Dictionary with audio pronunciations usage examples and&#xa0;" ID="ID_710213226" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="eager | meaning in the Cambridge Learners Dictionary" FOLDED="true" ID="ID_678797515" CREATED="1557224068609" MODIFIED="1557225479708" LINK="https://dictionary.cambridge.org/dictionary/learner-english/eager">
<node TEXT="eager definition: wanting to do or have something very much: . Learn more." ID="ID_1596241796" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="Machine Learning Classifiers &#x2013; Towards Data Science" FOLDED="true" ID="ID_1337457858" CREATED="1557224068609" MODIFIED="1557225479708" LINK="https://towardsdatascience.com/machine-learning-classifiers-a5cc4e1b0623">
<node TEXT="Jun 11 2018  Eager learners construct a classification model based on the given training data  and they are identified using the information gain concept." ID="ID_1937732101" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="eager adjective - Definition pictures pronunciation and usage notes " FOLDED="true" ID="ID_103569805" CREATED="1557224068609" MODIFIED="1557225479708" LINK="https://www.oxfordlearnersdictionaries.com/us/definition/english/eager">
<node TEXT="Definition of eager adjective in Oxford Advanced Learners Dictionary. Meaning pronunciation picture example sentences grammar usage notes synonyms&#xa0;" ID="ID_17848562" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="Is a neural network a lazy or eager learning method? - Stack Overflow" FOLDED="true" ID="ID_361811057" CREATED="1557224068609" MODIFIED="1557225479708" LINK="https://stackoverflow.com/questions/5749867/is-a-neural-network-a-lazy-or-eager-learning-method">
<node TEXT="Looking at the definition of the terms lazy and eager learning and knowing how a neural network works I believe that it is clear that it is eager. A trained network&#xa0;" ID="ID_447558236" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="Want to Raise an Eager Learner? | Highlights" FOLDED="true" ID="ID_500289811" CREATED="1557224068609" MODIFIED="1557225479708" LINK="https://www.highlights.com/parents/articles/want-to-raise-eager-learner">
<node TEXT="Jan 10 2018  Promote the Real Skills Preschoolers Need for Learning.  Make the connection between the concept of &#x201c;six&#x201d; the sound of the word &#x201c;six&#x201d; and&#xa0;" ID="ID_54750134" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="What is the difference between eager learning and lazy learning " FOLDED="true" ID="ID_983083868" CREATED="1557224068609" MODIFIED="1557225479708" LINK="https://www.quora.com/What-is-the-difference-between-eager-learning-and-lazy-learning">
<node TEXT="Eager learning is when you open your mind to receive new information and new  This translates to: Think not that all wisdom is in your school meaning that&#xa0;" ID="ID_654463264" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="What is Learner Engagement? A Modern Definition for 2018 " FOLDED="true" ID="ID_1674541422" CREATED="1557224068609" MODIFIED="1557225479708" LINK="https://www.talentlms.com/ebook/learner-engagement/definition">
<node TEXT="What does an engaged learner look like? An engaged learner looks: &#x25cb; Active in their learning &#x25cb; Eager to participate &#x25cb; Willing to expend effort &#x25cb; Motivated" ID="ID_1212763363" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="Eager Beaver | Definition of Eager Beaver by Merriam-Webster" FOLDED="true" ID="ID_412915985" CREATED="1557224068609" MODIFIED="1557225479708" LINK="https://www.merriam-webster.com/dictionary/eager%20beaver">
<node TEXT="Eager beaver definition is - a person who is extremely zealous about  See the full definition for eager beaver in the English Language Learners Dictionary." ID="ID_1685804052" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
</node>
</node>
</node>
<node TEXT="Clustering" FOLDED="true" ID="ID_299282937" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Clustering Fundamentals" ID="ID_1366986485" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Basics" ID="ID_1314403634" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
</node>
<node TEXT="Kmeans" ID="ID_735363804" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Finding Optimal number of Clusters" ID="ID_1532942946" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Finding Optimal number of Clusters Kmeans#$D$#" FOLDED="true" ID="ID_1771234191" CREATED="1557224068609" MODIFIED="1557225483442">
<icon BUILTIN="stop-sign"/>
<node TEXT="Determining The Optimal Number Of Clusters: 3 Must Know " FOLDED="true" ID="ID_41004585" CREATED="1557224068609" MODIFIED="1557225479708" LINK="https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/">
<node TEXT="Determining the optimal number of clusters in a data set is a fundamental issue in partitioning clustering such as k-means clustering which requires the user to&#xa0;" ID="ID_103857916" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="Tutorial: How to determine the optimal number of clusters for k " FOLDED="true" ID="ID_1537021761" CREATED="1557224068609" MODIFIED="1557225479708" LINK="https://blog.cambridgespark.com/how-to-determine-the-optimal-number-of-clusters-for-k-means-clustering-14f27070048f">
<node TEXT="May 27 2018  Introduction K-means is a type of unsupervised learning and one of the popular methods of clustering unlabelled data into k clusters. One of the&#xa0;" ID="ID_1251188129" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="Finding Optimal Number of Clusters | DataScience+" FOLDED="true" ID="ID_594086774" CREATED="1557224068609" MODIFIED="1557225479723" LINK="https://datascienceplus.com/finding-optimal-number-of-clusters/">
<node TEXT="Feb 9 2017  In this post we are going to have a look at one of the problems while applying clustering algorithms such as k-means and expectation&#xa0;" ID="ID_721072395" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="Determining the number of clusters in a data set - Wikipedia" FOLDED="true" ID="ID_1262369206" CREATED="1557224068609" MODIFIED="1557225479723" LINK="https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set">
<node TEXT="Determining the number of clusters in a data set a quantity often labelled k as in the k-means  Intuitively then the optimal choice of k will strike a balance between maximum compression of the data using a single cluster and maximum&#xa0;" ID="ID_833744472" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="Selecting the number of clusters with silhouette analysis on KMeans " FOLDED="true" ID="ID_777106988" CREATED="1557224068609" MODIFIED="1557225479723" LINK="http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html">
<node TEXT="In this example the silhouette analysis is used to choose an optimal value for n_clusters . The silhouette plot shows that the n_clusters value of 3 5 and 6 are a&#xa0;" ID="ID_319246451" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="Cluster analysis in R: determine the optimal number of clusters " FOLDED="true" ID="ID_1327039931" CREATED="1557224068609" MODIFIED="1557225479723" LINK="https://stackoverflow.com/questions/15376075/cluster-analysis-in-r-determine-the-optimal-number-of-clusters">
<node TEXT="If your question is how can I determine how many clusters are appropriate for a kmeans analysis of my data?  then here are some options. The wikipedia article&#xa0;" ID="ID_48989930" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="Finding Optimal Number Of Clusters for Kmeans - MATLAB Answers " FOLDED="true" ID="ID_72346785" CREATED="1557224068609" MODIFIED="1557225479723" LINK="https://www.mathworks.com/matlabcentral/answers/152409-finding-optimal-number-of-clusters-for-kmeans">
<node TEXT="I want to find the number of clusters for my data for which the correlation is above .9. I know you can use a sum of squared error (SSE) scree plot but I am not&#xa0;" ID="ID_1158040850" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="10 Tips for Choosing the Optimal Number of Clusters" FOLDED="true" ID="ID_443472752" CREATED="1557224068609" MODIFIED="1557225479723" LINK="https://towardsdatascience.com/10-tips-for-choosing-the-optimal-number-of-clusters-277e93d72d92">
<node TEXT="Jan 27 2019  Clustering is one of the most common unsupervised machine learning  and methods for identifying the optimal number of clusters so Ill just&#xa0;" ID="ID_1904582753" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="K-Means Clustering &#x2014; Deciding How Many Clusters to Build" FOLDED="true" ID="ID_1240620238" CREATED="1557224068609" MODIFIED="1557225479723" LINK="https://blog.exploratory.io/k-means-clustering-deciding-how-many-clusters-to-build-d33fd9c68088">
<node TEXT="Nov 1 2018  This is a follow-up post for Visualizing K-Means Clustering Results to Understand  Finding How Many Clusters to Create with Elbow Curve." ID="ID_1630425813" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="KMeans Clustering Part 1 - Determining The Optimal Number Of " FOLDED="true" ID="ID_1918263304" CREATED="1557224068609" MODIFIED="1557225479723" LINK="https://www.youtube.com/watch?v=ePfX6a7rsis">
<node TEXT="Aug 18 2018  In this video Im going to walk you through how to determine the optimal number of clusters in a data set for a KMeans cluster analysis in R with&#xa0;" ID="ID_1091920659" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
</node>
</node>
</node>
<node TEXT="DBSCAN" ID="ID_1081340850" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
<node TEXT="Spectral Clustering" ID="ID_928132396" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Spectral Clustering#$D$#" FOLDED="true" ID="ID_1041956216" CREATED="1557224068609" MODIFIED="1557225483442">
<icon BUILTIN="stop-sign"/>
<node TEXT="Spectral clustering - Wikipedia" FOLDED="true" ID="ID_375893770" CREATED="1557224068609" MODIFIED="1557225479723" LINK="https://en.wikipedia.org/wiki/Spectral_clustering">
<node TEXT="In multivariate statistics and the clustering of data spectral clustering techniques make use of the spectrum (eigenvalues) of the similarity matrix of the data to&#xa0;" ID="ID_1143408374" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="Spectral clustering &#x2013; Towards Data Science" FOLDED="true" ID="ID_1869976812" CREATED="1557224068609" MODIFIED="1557225479723" LINK="https://towardsdatascience.com/spectral-clustering-82d3cff3d3b7">
<node TEXT="Feb 4 2019  Clustering is a widely used unsupervised learning method. The grouping is such that points in a cluster are similar to each other and less&#xa0;" ID="ID_1456084530" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="A Tutorial on Spectral Clustering" FOLDED="true" ID="ID_1143811769" CREATED="1557224068609" MODIFIED="1557225479723" LINK="https://www.cs.cmu.edu/~aarti/Class/10701/readings/Luxburg06_TR.pdf">
<node TEXT="Aug 1 2006  Abstract. In recent years spectral clustering has become one of the most popular modern  Nevertheless on the first glance spectral clustering." ID="ID_1217939459" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="Spectral Clustering for beginners &#x2013; Towards Data Science" FOLDED="true" ID="ID_1629946833" CREATED="1557224068609" MODIFIED="1557225479723" LINK="https://towardsdatascience.com/spectral-clustering-for-beginners-d08b7d25b4d8">
<node TEXT="May 7 2018  Spectral clustering has become increasingly popular due to its simple implementation and promising performance in many graph-based&#xa0;" ID="ID_581526475" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="On Spectral Clustering: Analysis and an algorithm Andrew Y. Ng CS " FOLDED="true" ID="ID_261572791" CREATED="1557224068609" MODIFIED="1557225479723" LINK="https://ai.stanford.edu/~ang/papers/nips01-spectral.pdf">
<node TEXT="Abstract. Despite many empirical successes of spectral clustering methods&#x2014; algorithms that cluster points using eigenvectors of matrices de- rived from the&#xa0;" ID="ID_663264157" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="Introduction to spectral clustering" FOLDED="true" ID="ID_776359070" CREATED="1557224068609" MODIFIED="1557225479723" LINK="http://www.cvl.isy.liu.se:82/education/graduate/spectral-clustering/SC_course_part1.pdf">
<node TEXT="Basic introduction into the core ideas of spectral clustering. &#x2022; Sufficient to get a basic understanding of how the method works. &#x2022; Application mainly to computer&#xa0;" ID="ID_1499956154" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="Lecture 34 &#x2014; Spectral Clustering Three Steps (Advanced) | Stanford " FOLDED="true" ID="ID_1932662031" CREATED="1557224068609" MODIFIED="1557225479723" LINK="https://www.youtube.com/watch?v=uxsDKhZHDcc">
<node TEXT="Apr 12 2016  Copyright Disclaimer Under Section 107 of the Copyright Act 1976 allowance is made for FAIR USE for purposes such as criticism comment&#xa0;" ID="ID_1111627646" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="sklearn.cluster.SpectralClustering &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_54064306" CREATED="1557224068609" MODIFIED="1557225479723" LINK="http://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html">
<node TEXT="In practice Spectral Clustering is very useful when the structure of the individual clusters is highly non-convex or more generally when a measure of the center&#xa0;" ID="ID_845962247" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="Spectral Clustering Lecture 16" FOLDED="true" ID="ID_1109745038" CREATED="1557224068609" MODIFIED="1557225479723" LINK="http://people.csail.mit.edu/dsontag/courses/ml13/slides/lecture16.pdf">
<node TEXT="Spectral Clustering. Lecture 16. David Sontag. New York University. Slides adapted from James Hays Alan Fern and Tommi Jaakkola&#xa0;" ID="ID_644116516" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="Unit 6 7b Spectral Clustering Algorithm - YouTube" FOLDED="true" ID="ID_461689390" CREATED="1557224068609" MODIFIED="1557225479723" LINK="https://www.youtube.com/watch?v=P-LEH-AFovE">
<node TEXT="Oct 22 2011  Unit 6 7b Spectral Clustering Algorithm.  Ali Ghodsi Lec 6: Spectral Clustering Laplacian Eigenmap MVU - Duration: 1:17:28. Data Science&#xa0;" ID="ID_622599448" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
</node>
</node>
<node TEXT="Evaluation Methods based on Ground Truth" ID="ID_1479096788" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Homogeneity" ID="ID_444935184" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Evaluation Methods based on Ground Truth#$D$#" FOLDED="true" ID="ID_534326687" CREATED="1557224068609" MODIFIED="1557225483443">
<icon BUILTIN="stop-sign"/>
<node TEXT="Defining and Evaluating Network Communities Based on Ground " FOLDED="true" ID="ID_666898635" CREATED="1557224068609" MODIFIED="1557225479723" LINK="https://ieeexplore.ieee.org/document/6413740">
<node TEXT="Defining and Evaluating Network Communities Based on Ground-Truth  detection method that easily scales to networks with more than hundred million nodes." ID="ID_292148415" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="Evaluation methods based on the ground truth - Machine Learning " FOLDED="true" ID="ID_1923844850" CREATED="1557224068609" MODIFIED="1557225479723" LINK="https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781785889622/9/ch09lvl1sec64/evaluation-methods-based-on-the-ground-truth">
<node TEXT="Evaluation methods based on the ground truthIn this section we present some evaluation methods that require the knowledge of " ID="ID_373109508" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="Defining and Evaluating Network Communities based on Ground-truth" FOLDED="true" ID="ID_1059480770" CREATED="1557224068609" MODIFIED="1557225479723" LINK="https://cs.stanford.edu/~jure/pubs/comscore-icdm12.pdf">
<node TEXT="of network communities correspond to ground-truth communi- ties. We choose 13  evaluation of community detection methods based on their performance to&#xa0;" ID="ID_1365985919" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="Reference-free ground truth metric for metal artifact evaluation in CT " FOLDED="true" ID="ID_143821576" CREATED="1557224068609" MODIFIED="1557225479723" LINK="https://www.ncbi.nlm.nih.gov/pubmed/21859033">
<node TEXT="Reference-free ground truth metric for metal artifact evaluation in CT images.  METHODS: The proposed metric is based on an inherent ground truth for metal&#xa0;" ID="ID_548465430" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="Evaluation of Event-Based Algorithms for Optical Flow with Ground " FOLDED="true" ID="ID_1476478198" CREATED="1557224068609" MODIFIED="1557225479723" LINK="https://www.frontiersin.org/articles/10.3389/fnins.2016.00176">
<node TEXT="In frame-based methods this property is typically  The consequence is that the ground truth from the IMU is not&#xa0;" ID="ID_232256972" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="Intersection-Validation: A Method for Evaluating Structure Learning " FOLDED="true" ID="ID_881550587" CREATED="1557224068609" MODIFIED="1557225479723" LINK="http://proceedings.mlr.press/v84/viinikka18a/viinikka18a.pdf">
<node TEXT="Structure Learning without Ground Truth  model the so-called ground truth then evaluating the  formance of the method in various scenarios based on." ID="ID_1841357282" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="Performance evaluation of inverse methods for identification and " FOLDED="true" ID="ID_929486910" CREATED="1557224068609" MODIFIED="1557225479723" LINK="https://www.biorxiv.org/content/10.1101/395780v1.full">
<node TEXT="Aug 20 2018  Most of these techniques are based on fitting single/ multiple dipolar cortical . to test a method provides mechanism for ground truth validation." ID="ID_1506832090" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="On Evaluating Brain Tissue Classifiers without a Ground Truth" FOLDED="true" ID="ID_1813353903" CREATED="1557224068609" MODIFIED="1557225479723" LINK="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2702211/">
<node TEXT="We apply these different evaluation methodologies to a set eleven different  We then validate our evaluation pipeline by building a ground truth based on&#xa0;" ID="ID_1504035409" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="Ground Truth Data Content Metrics and Analysis" FOLDED="true" ID="ID_711506975" CREATED="1557224068609" MODIFIED="1557225479723" LINK="https://www.embedded-vision.com/sites/default/files/apress/computervisionmetrics/chapter7/9781430259299_Ch07.pdf">
<node TEXT="propose a method and corresponding ground truth dataset for measuring  either by a human or automatically by image analysis depending on the . case study to generate both the ground truth dataset and the metric basis for evaluating." ID="ID_1276603596" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="On the construction of a ground truth framework for evaluating voxel " FOLDED="true" ID="ID_1269537963" CREATED="1557224068609" MODIFIED="1557225479723" LINK="https://www.ncbi.nlm.nih.gov/pubmed/19268708">
<node TEXT="Mar 5 2009  On the construction of a ground truth framework for evaluating voxel-based diffusion tensor MRI analysis methods. Van Hecke W(1) Sijbers J&#xa0;" ID="ID_1668475562" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
</node>
<node TEXT="Clustering Evaluation Methods Homogeneity#$D$#" FOLDED="true" ID="ID_65526068" CREATED="1557224068609" MODIFIED="1557225483443">
<icon BUILTIN="stop-sign"/>
<node TEXT="2.3. Clustering &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_332097344" CREATED="1557224068609" MODIFIED="1557225479723" LINK="http://scikit-learn.org/stable/modules/clustering.html">
<node TEXT="Gaussian mixture models useful for clustering are described in another chapter of the  Evaluating the performance of a clustering algorithm is not as trivial as .. Homogeneity completeness and V-measure can be computed at once using&#xa0;" ID="ID_1239220905" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="On evaluation of clustering using homogeneity analysis - IEEE " FOLDED="true" ID="ID_1182173193" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://ieeexplore.ieee.org/document/886566/">
<node TEXT="Abstract: Proposes a new technique to evaluate the results of fuzzy clustering. Fuzzy clustering is a method to obtain natural groups from given observations by&#xa0;" ID="ID_1077865082" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="sklearn.metrics.homogeneity_score &#x2014; scikit-learn 0.20.3 " FOLDED="true" ID="ID_1076868300" CREATED="1557224068610" MODIFIED="1557225479723" LINK="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_score.html">
<node TEXT="Homogeneity metric of a cluster labeling given a ground truth. A clustering result  V-Measure: A conditional entropy-based external cluster evaluation measure&#xa0;" ID="ID_986681621" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="K-means Clustering: Algorithm Applications Evaluation Methods " FOLDED="true" ID="ID_537228617" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a">
<node TEXT="Sep 17 2018  In other words we try to find homogeneous subgroups within the data such that data points in each cluster are as similar as possible according&#xa0;" ID="ID_920294471" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="A Conditional Entropy-Based External Cluster Evaluation Measure" FOLDED="true" ID="ID_1025886130" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://www.aclweb.org/anthology/D07-1043">
<node TEXT="Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and .. to evaluate the homogeneity of a clustering solution." ID="ID_23357789" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Methods for evaluating clustering algorithms for gene expression " FOLDED="true" ID="ID_422132807" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1590054/">
<node TEXT="Methods for evaluating clustering algorithms for gene expression data using a reference set of  The first measure is a biological homogeneity index (BHI)." ID="ID_502032687" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="A comparison of Extrinsic Clustering Evaluation Metrics based on " FOLDED="true" ID="ID_1953179377" CREATED="1557224068610" MODIFIED="1557225479723" LINK="http://nlp.uned.es/docs/amigo2007a.pdf">
<node TEXT="May 11 2009  following methodology: each formal restriction consists of a pattern (D1D2) . Figure 5: Example of test to validate the Cluster Homogeneity&#xa0;" ID="ID_1638348124" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Clustering Validity Indices Evaluation with Regard to Semantic " FOLDED="true" ID="ID_462345930" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://annals-csis.org/Volume_9/drp/371.html">
<node TEXT="Clustering Validity Indices Evaluation with Regard to Semantic Homogeneity  Clustering validity indices are a methods for examining and assessing quality of&#xa0;" ID="ID_604110842" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Evaluation methods for a clustering techniques ?" FOLDED="true" ID="ID_1721914956" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://www.researchgate.net/post/Evaluation_methods_for_a_clustering_techniques2">
<node TEXT="Jan 1 2017  If you want to evaluate the clustering techniques you should confirm . Entropy Homogeneity Completeness and V-measure are my favorite." ID="ID_785644593" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="User Centric Homogeneity-Based Clustering Approach for " FOLDED="true" ID="ID_1483030985" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://link.springer.com/chapter/10.1007/978-3-642-22185-9_31">
<node TEXT="We evaluate the effectiveness of our proposal by modifying two most widely used clustering methods K-means and hierarchical according to the homogeneity&#xa0;" ID="ID_1504910892" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
</node>
<node TEXT="Clustering Evaluation Methods Completeness#$D$#" FOLDED="true" ID="ID_849841593" CREATED="1557224068610" MODIFIED="1557225483443">
<icon BUILTIN="stop-sign"/>
<node TEXT="2.3. Clustering &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_1913853326" CREATED="1557224068610" MODIFIED="1557225479723" LINK="http://scikit-learn.org/stable/modules/clustering.html">
<node TEXT="Gaussian mixture models useful for clustering are described in another chapter of the .. Connectivity constraints and single complete or average linkage can enhance . Evaluating the performance of a clustering algorithm is not as trivial as&#xa0;" ID="ID_1042443133" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Cluster analysis - Wikipedia" FOLDED="true" ID="ID_802842333" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://en.wikipedia.org/wiki/Cluster_analysis">
<node TEXT="Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects  Relaxations of the complete connectivity requirement (a fraction of the edges can be missing) are known as .. One drawback of using internal criteria in cluster evaluation is that high scores on an internal measure do not&#xa0;" ID="ID_1542060695" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="An Evaluation Method for Cluster Analyses of Ambiguous Data" FOLDED="true" ID="ID_1940557071" CREATED="1557224068610" MODIFIED="1557225479723" LINK="http://www.lrec-conf.org/proceedings/lrec2014/pdf/829_Paper.pdf">
<node TEXT="Fuzzy V-Measure &#x2013; An Evaluation Method for  Keywords: clustering evaluation ambiguous data. 1. . This represents a maximally complete clustering in that." ID="ID_557796533" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Evaluation methods for a clustering techniques ?" FOLDED="true" ID="ID_273532793" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://www.researchgate.net/post/Evaluation_methods_for_a_clustering_techniques2">
<node TEXT="Jan 1 2017  If you want to evaluate the clustering techniques you should confirm . Entropy Homogeneity Completeness and V-measure are my favorite." ID="ID_1023883545" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="A Conditional Entropy-Based External Cluster Evaluation Measure" FOLDED="true" ID="ID_1229237276" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://www.aclweb.org/anthology/D07-1043">
<node TEXT="cluster kj. To discuss cluster evaluation measures we intro- duce two criteria for a clustering solution: homo- geneity and completeness. A clustering result sat-." ID="ID_1821153688" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="GO functional similarity clustering depends on similarity measure " FOLDED="true" ID="ID_920138019" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-2752-2">
<node TEXT="Mar 27 2019  Finally we show that for semantic similarity-based clustering the  on similarity measure clustering method and annotation completeness." ID="ID_1641084705" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="A comparison of Extrinsic Clustering Evaluation Metrics based on " FOLDED="true" ID="ID_914030432" CREATED="1557224068610" MODIFIED="1557225479723" LINK="http://nlp.uned.es/docs/amigo2007a.pdf">
<node TEXT="May 11 2009  following methodology: each formal restriction consists of a pattern (D1D2) . Figure 6: Example of test to validate the Cluster Completeness&#xa0;" ID="ID_1210777291" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Completeness" FOLDED="true" ID="ID_844259300" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://webis.de/research/aitools/javadoc/de/aitools/dm/clustering/validation/external/Completeness.html">
<node TEXT="V-Measure: A conditional entropy-based external cluster evaluation measure. Version: $Id:  Methods inherited from class de.aitools.dm.clustering.validation." ID="ID_1456247747" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="model evaluation - What are the most common metrics for " FOLDED="true" ID="ID_890836274" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://stats.stackexchange.com/questions/95782/what-are-the-most-common-metrics-for-comparing-two-clustering-algorithms-especi">
<node TEXT="Here are two standard approaches (there may be more). The first  This is still not straightforward -- a clustering can be consistent with a a gold&#xa0;" ID="ID_1570057542" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Is there a function in matlab to evaluate homogeneity and " FOLDED="true" ID="ID_1215892150" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://stackoverflow.com/questions/38980575/is-there-a-function-in-matlab-to-evaluate-homogeneity-and-completeness-of-a-clus">
<node TEXT="Aug 16 2016  Yes. Almost every external cluster evaluation method is based on these concepts. Theynare maybe not exactly what you described - because&#xa0;" ID="ID_1669728725" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
</node>
</node>
<node TEXT="Completeness" ID="ID_390083744" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
<node TEXT="Adjusted Rand Index" ID="ID_1168621958" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Clustering Evaluation Methods Adjusted Rand Index#$D$#" FOLDED="true" ID="ID_461340816" CREATED="1557224068610" MODIFIED="1557225483443">
<icon BUILTIN="stop-sign"/>
<node TEXT="Rand index - Wikipedia" FOLDED="true" ID="ID_919870174" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://en.wikipedia.org/wiki/Rand_index">
<node TEXT="The Rand index or Rand measure in statistics and in particular in data clustering is a measure of the similarity between two data clusterings. A form of the Rand index may be defined that is adjusted for the chance .. Variations of the adjusted Rand Index account for different models of random clusterings. Though the Rand&#xa0;" ID="ID_1493932849" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="sklearn.metrics.adjusted_rand_score &#x2014; scikit-learn 0.20.3 " FOLDED="true" ID="ID_4039017" CREATED="1557224068610" MODIFIED="1557225479723" LINK="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html">
<node TEXT="The adjusted Rand index is thus ensured to have a value close to 0.0 for random labeling independently of the  Cluster labels to evaluate  from sklearn.metrics.cluster import adjusted_rand_score  adjusted_rand_score([0 0 1 1] [0 0&#xa0;" ID="ID_1352592510" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="On the Use of the Adjusted Rand Index as a Metric for Evaluating " FOLDED="true" ID="ID_32455108" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://link.springer.com/chapter/10.1007/978-3-642-04277-5_18">
<node TEXT="The Adjusted Rand Index (ARI) is frequently used in cluster validation since it is a  Rand W.M.: Objective criteria for the evaluation of clustering methods." ID="ID_1344007973" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="2.3. Clustering &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_469627835" CREATED="1557224068610" MODIFIED="1557225479723" LINK="http://scikit-learn.org/stable/modules/clustering.html">
<node TEXT="Gaussian mixture models useful for clustering are described in another chapter of the  Evaluating the performance of a clustering algorithm is not as trivial as  the adjusted Rand index is a function that measures the similarity of the two&#xa0;" ID="ID_1946857306" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Details of the Adjusted Rand index and Clustering algorithms " FOLDED="true" ID="ID_310313674" CREATED="1557224068610" MODIFIED="1557225479723" LINK="http://faculty.washington.edu/kayee/pca/supp.pdf">
<node TEXT="May 3 2001  In order to compare clustering results against external criteria  The adjusted Rand index proposed by [Hubert and Arabie 1985] .. [Rand 1971] Rand W. M. (1971) Objective criteria for the evaluation of clustering methods." ID="ID_1335663665" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="(PDF) ARImp: A Generalized Adjusted Rand Index for Cluster " FOLDED="true" ID="ID_875622190" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://www.researchgate.net/publication/216543682_ARImp_A_Generalized_Adjusted_Rand_Index_for_Cluster_Ensembles">
<node TEXT="Adjusted Rand Index (ARI) is one of the most popular measure to evaluate the consistency between two partitions  the less effective cluster ensemble methods." ID="ID_1244421553" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="AORS: Affinity-based outlier ranking score - IEEE Conference " FOLDED="true" ID="ID_1107018985" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://ieeexplore.ieee.org/document/6889551/">
<node TEXT="Outlier ranking methods can provide a quantitative measure to evaluate the outlierness of data instances in data clustering and attract great interest in p.  Consistent with the improvement of Adjusted Rand Index (ARI) over RAND we find that&#xa0;" ID="ID_523160676" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Chapter 4 Clustering Algorithms and Evaluations" FOLDED="true" ID="ID_1306342599" CREATED="1557224068610" MODIFIED="1557225479723" LINK="http://www.ims.uni-stuttgart.de/institut/mitarbeiter/schulte/theses/phd/algorithm.pdf">
<node TEXT="Clustering methods are applied in many domains such as medical research psychology  Table 4.6: Example evaluation for adjusted Rand index. Exp tij. 2." ID="ID_625679120" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="A Note on Using the Adjusted Rand Index for Link Prediction in " FOLDED="true" ID="ID_1051405051" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6191196/">
<node TEXT="3(Number of Clusters) &#xd7; 3(Density Within) &#xd7; 3(Density Between)  This can evaluate how much better than chance this method is&#xa0;" ID="ID_760001487" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Ranked Adjusted Rand: integrating distance and partition " FOLDED="true" ID="ID_1104433148" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-8-44">
<node TEXT="Feb 7 2007  Biological information is commonly used to cluster or classify entities of interest  RAR differs from existing methods by evaluating the extent of  In the first case RAR is equal to its predecessor Adjusted Rand (HA) index." ID="ID_1542107506" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
</node>
</node>
</node>
</node>
</node>
<node TEXT="Clustering Techniques" POSITION="left" ID="ID_250522630" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<edge COLOR="#00cc33"/>
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Hierarchical Clustering" ID="ID_429291585" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Hierarchical Clustering#$D$#" FOLDED="true" ID="ID_1911415328" CREATED="1557224068610" MODIFIED="1557225483444">
<icon BUILTIN="stop-sign"/>
<node TEXT="Hierarchical clustering - Wikipedia" FOLDED="true" ID="ID_1326933476" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://en.wikipedia.org/wiki/Hierarchical_clustering">
<node TEXT="In data mining and statistics hierarchical clustering is a method of cluster analysis which seeks to build a hierarchy of clusters. Strategies for hierarchical&#xa0;" ID="ID_1738790063" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="What is Hierarchical Clustering? | Displayr.com" FOLDED="true" ID="ID_1405118746" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://www.displayr.com/what-is-hierarchical-clustering/">
<node TEXT="Hierarchical clustering also known as hierarchical cluster analysis is an algorithm that groups similar objects into groups called clusters. Learn more." ID="ID_13926038" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Understanding the concept of Hierarchical clustering Technique" FOLDED="true" ID="ID_1535135285" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://towardsdatascience.com/understanding-the-concept-of-hierarchical-clustering-technique-c6e8243758ec">
<node TEXT="Dec 10 2018  Hierarchical clustering Technique is one of the popular Clustering techniques in Machine Learning. Before we try to understand the concept of&#xa0;" ID="ID_1455056864" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Hierarchical Clustering" FOLDED="true" ID="ID_1157194518" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://www.saedsayad.com/clustering_hierarchical.htm">
<node TEXT="Hierarchical clustering involves creating clusters that have a predetermined ordering from top to bottom. For example all files and folders on the hard disk are&#xa0;" ID="ID_976012781" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="StatQuest: Hierarchical Clustering - YouTube" FOLDED="true" ID="ID_784526903" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://www.youtube.com/watch?v=7xHsRkOdVwo">
<node TEXT="Jun 20 2017  Hierarchical clustering is often used with heatmaps and with machine learning type stuff. Its no big deal though and based on just a few&#xa0;" ID="ID_877684824" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Hierarchical clustering (scipy.cluster.hierarchy) &#x2014; SciPy v1.2.1 " FOLDED="true" ID="ID_328986039" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html">
<node TEXT="These functions cut hierarchical clusterings into flat clusterings or find the roots of the forest formed by a cut by providing the flat cluster ids of each observation." ID="ID_1076311700" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="sklearn.cluster.AgglomerativeClustering &#x2014; scikit-learn 0.20.3 " FOLDED="true" ID="ID_207862388" CREATED="1557224068610" MODIFIED="1557225479723" LINK="http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html">
<node TEXT="Recursively merges the pair of clusters that minimally increases a given linkage  Default is None i.e the hierarchical clustering algorithm is unstructured." ID="ID_1197305306" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Lecture 59 &#x2014; Hierarchical Clustering | Stanford University - YouTube" FOLDED="true" ID="ID_1070342691" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://www.youtube.com/watch?v=rg2cjfMsCk4">
<node TEXT="Apr 13 2016  Copyright Disclaimer Under Section 107 of the Copyright Act 1976 allowance is made for FAIR USE for purposes such as criticism comment&#xa0;" ID="ID_806243825" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Chapter 7 Hierarchical cluster analysis" FOLDED="true" ID="ID_816647311" CREATED="1557224068610" MODIFIED="1557225479723" LINK="http://www.econ.upf.edu/~michael/stanford/maeb7.pdf">
<node TEXT="The method of hierarchical cluster analysis is best explained by  this chapter we demonstrate hierarchical clustering on a small example and then list the." ID="ID_626222613" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Hierarchical agglomerative clustering" FOLDED="true" ID="ID_330827083" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://nlp.stanford.edu/IR-book/html/htmledition/hierarchical-agglomerative-clustering-1.html">
<node TEXT="Hierarchical clustering algorithms are either top-down or bottom-up. Bottom-up algorithms treat each document as a singleton cluster at the outset and then&#xa0;" ID="ID_1784784582" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
</node>
</node>
<node TEXT="Expectation Maximization Clustering" ID="ID_1413218592" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Expectation Maximization Clustering#$D$#" FOLDED="true" ID="ID_1421581972" CREATED="1557224068610" MODIFIED="1557225483444">
<icon BUILTIN="stop-sign"/>
<node TEXT="Expectation&#x2013;maximization algorithm - Wikipedia" FOLDED="true" ID="ID_1180617994" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">
<node TEXT="In statistics an expectation&#x2013;maximization (EM) algorithm is an iterative method to find .. EM is frequently used for data clustering in machine learning and computer vision. In natural language processing two prominent instances of the&#xa0;" ID="ID_1181428488" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Expectation Maximization Clustering - RapidMiner Documentation" FOLDED="true" ID="ID_1502711848" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://docs.rapidminer.com/latest/studio/operators/modeling/segmentation/expectation_maximization_clustering.html">
<node TEXT="This operator performs clustering using the Expectation Maximization algorithm. Clustering is concerned with grouping objects together that are similar to each&#xa0;" ID="ID_790913593" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="EM algorithm: how it works - YouTube" FOLDED="true" ID="ID_600471794" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://www.youtube.com/watch?v=REypj2sy_5U">
<node TEXT="Jan 19 2014  Full lecture: http://bit.ly/EM-alg Mixture models are a probabilistically-sound way to do soft clustering. We assume our data is sampled from K&#xa0;" ID="ID_1943962397" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="EM (Annotated)" FOLDED="true" ID="ID_204750622" CREATED="1557224068610" MODIFIED="1557225479723" LINK="http://www.cs.cmu.edu/~aarti/Class/10701_Spring14/slides/EM_annotatedonclass.pdf">
<node TEXT="Algorithm. Input. &#x2013; Data + Desired number of clusters K. Initialize. &#x2013; the K cluster centers (randomly if necessary). Iterate. 1. Decide the class memberships of the&#xa0;" ID="ID_1126397577" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Unsupervised clustering with E.M." FOLDED="true" ID="ID_1691787134" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://www.math.univ-toulouse.fr/~besse/Wikistat/pdf/st-m-datSc4-EMmixt.pdf">
<node TEXT="Unsupervised clustering with E.M.. Summary. We describe here the important framework of mixture models. These mixture models are rich flexible easy to&#xa0;" ID="ID_264948227" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="2.1. Gaussian mixture models &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_1213666998" CREATED="1557224068610" MODIFIED="1557225479723" LINK="http://scikit-learn.org/stable/modules/mixture.html">
<node TEXT="One can think of mixture models as generalizing k-means clustering to  The GaussianMixture object implements the expectation-maximization (EM) algorithm&#xa0;" ID="ID_472503510" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Expectation-Maximization Algorithm for Clustering Multidimensional " FOLDED="true" ID="ID_138514176" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://engineering.purdue.edu/kak/Tutorials/ExpectationMaximization.pdf">
<node TEXT="Jan 28 2017  Expectation Maximization Tutorial by Avi Kak. Expectation-Maximization Algorithm for. Clustering Multidimensional Numerical. Data. Avinash&#xa0;" ID="ID_1272259080" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="A Tutorial on the Expectation Maximization (EM) Algorithm" FOLDED="true" ID="ID_865700617" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://www.kdnuggets.com/2016/08/tutorial-expectation-maximization-algorithm.html">
<node TEXT="This is a short tutorial on the Expectation Maximization algorithm and how it can  By looking at the spread of each cluster we can estimate that the variance of&#xa0;" ID="ID_196172390" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Clustering performance comparison using K-means and expectation " FOLDED="true" ID="ID_1858891375" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4433949/">
<node TEXT="Two representatives of the clustering algorithms are the K-means and the expectation maximization (EM) algorithm. Linear regression analysis was extended to&#xa0;" ID="ID_765685487" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="machine learning - Clustering with K-Means and EM: how are they " FOLDED="true" ID="ID_433341725" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://stats.stackexchange.com/questions/76866/clustering-with-k-means-and-em-how-are-they-related">
<node TEXT="Hard assign a data point to one particular cluster on convergence.  The most popular variant of EM is also known as Gaussian Mixture&#xa0;" ID="ID_838438067" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
</node>
</node>
<node TEXT="Agglomerative Clustering" ID="ID_1143516169" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Dendograms" ID="ID_968512496" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
<node TEXT="Agglomerative Clustering in Scikit-learn" ID="ID_164548568" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Clustering Fundamentals#$D$#" FOLDED="true" ID="ID_1750380235" CREATED="1557224068609" MODIFIED="1557225483442">
<icon BUILTIN="stop-sign"/>
<node TEXT="Clustering fundamentals" FOLDED="true" ID="ID_882502229" CREATED="1557224068609" MODIFIED="1557225479708" LINK="http://dbdmg.polito.it/wordpress/wp-content/uploads/2017/02/24e-DMClustering_6x.pdf">
<node TEXT="Clustering fundamentals. DataBase and Data Mining Group. 1. Clustering fundamentals. Elena Baralis Tania Cerquitelli. Politecnico di Torino. 2. DB. M. G." ID="ID_786113720" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="Windows Failover Clustering Fundamentals | Pluralsight" FOLDED="true" ID="ID_70382876" CREATED="1557224068609" MODIFIED="1557225479708" LINK="https://www.pluralsight.com/courses/windows-failover-clustering-fundamentals">
<node TEXT="Windows failover clusters are becoming increasingly prevalent in todays data centers. This course will teach you what you need to know about configuring&#xa0;" ID="ID_1569473522" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="An Introduction to Clustering  different methods of clustering" FOLDED="true" ID="ID_339497018" CREATED="1557224068609" MODIFIED="1557225479708" LINK="https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-clustering-and-different-methods-of-clustering/">
<node TEXT="Nov 3 2016  This article is an introduction to clustering and its types. K-means clustering  Hierarchical clustering have been explained in details." ID="ID_803501363" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="Cluster Analysis: Basic Concepts and Algorithms" FOLDED="true" ID="ID_1620133418" CREATED="1557224068609" MODIFIED="1557225479708" LINK="https://www-users.cs.umn.edu/~kumar/dmbook/ch8.pdf">
<node TEXT="Cluster analysis divides data into groups (clusters) that are meaningful useful  dividing objects into groups (clustering) and assigning particular objects to." ID="ID_793616207" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="Clustering: A basic 101 tutorial" FOLDED="true" ID="ID_1296962724" CREATED="1557224068609" MODIFIED="1557225479708" LINK="https://www.ibm.com/developerworks/aix/tutorials/clustering/clustering.html">
<node TEXT="Apr 3 2002  Confused by clusters? Were not talking grapes. Heres a sweet tutorial -- now updated -- on clustering high availability redundancy and&#xa0;" ID="ID_1701413764" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="Hierarchical Clustering Basics" FOLDED="true" ID="ID_478382635" CREATED="1557224068609" MODIFIED="1557225479708" LINK="http://research.med.helsinki.fi/corefacilities/proteinchem/hierarchical_clustering_basics.pdf">
<node TEXT="Basics. Please read the introduction to principal component analysis first. Please read the  Important parameters in hierarchical clustering are: The distance&#xa0;" ID="ID_1569782168" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="Segmentation and Clustering | Udacity" FOLDED="true" ID="ID_1775703324" CREATED="1557224068609" MODIFIED="1557225479708" LINK="https://www.udacity.com/course/segmentation-and-clustering--ud981">
<node TEXT="Segmentation and Clustering Fundamentals. Learn the difference between standardization and localization. Learn about the concept of distance in clustering&#xa0;" ID="ID_1770342935" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="Fundamental Clustering Problems Suite" FOLDED="true" ID="ID_1033626888" CREATED="1557224068609" MODIFIED="1557225479708" LINK="https://www.uni-marburg.de/fb12/arbeitsgruppen/datenbionik/data">
<node TEXT="Dec 16 2013  The Fundamental Clustering Problems Suite (FCPS) offers a variety of clustering problems any algorithm shall be able to handle when facing&#xa0;" ID="ID_447740937" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="Introduction to K-means Clustering" FOLDED="true" ID="ID_132163496" CREATED="1557224068609" MODIFIED="1557225479708" LINK="https://www.datascience.com/blog/k-means-clustering">
<node TEXT="Dec 6 2016  Learn data science with data scientist Dr. Andrea Trevinos step-by-step tutorial on the K-means clustering unsupervised machine learning&#xa0;" ID="ID_1044571056" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
<node TEXT="Clustering fundamentals &#x2013; IBM Developer" FOLDED="true" ID="ID_1366656947" CREATED="1557224068609" MODIFIED="1557225479708" LINK="https://developer.ibm.com/articles/l-cluster1/">
<node TEXT="Sep 27 2005  This part introduces the different types of clusters uses of clusters some fundamentals of HPC the role of Linux and the reasons for the growth&#xa0;" ID="ID_461495402" CREATED="1557224068609" MODIFIED="1557224068609"/>
</node>
</node>
<node TEXT="Clustering Techniques#$D$#" FOLDED="true" ID="ID_624791820" CREATED="1557224068610" MODIFIED="1557225483444">
<icon BUILTIN="stop-sign"/>
<node TEXT="The 5 Clustering Algorithms Data Scientists Need to Know" FOLDED="true" ID="ID_1693792071" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68">
<node TEXT="Feb 5 2018  Clustering is a Machine Learning technique that involves the grouping of data points. Given a set of data points we can use a clustering&#xa0;" ID="ID_230463635" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="5 Amazing Types of Clustering Methods You Should Know - Datanovia" FOLDED="true" ID="ID_1275590096" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://www.datanovia.com/en/blog/types-of-clustering-methods-overview-and-quick-start-r-code/">
<node TEXT="We provide an overview of clustering methods and quick start R codes. You will also learn how to assess the quality of clustering analysis." ID="ID_1321483810" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Cluster analysis - Wikipedia" FOLDED="true" ID="ID_1600527571" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://en.wikipedia.org/wiki/Cluster_analysis">
<node TEXT="Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data mining and a common technique for&#xa0;" ID="ID_1152047955" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="An Introduction to Clustering  different methods of clustering" FOLDED="true" ID="ID_1802257977" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-clustering-and-different-methods-of-clustering/">
<node TEXT="Nov 3 2016  The method of identifying similar groups of data in a dataset is called clustering. It is one of the most popular techniques in data science." ID="ID_252878190" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Three Popular Clustering Methods and When to Use Each" FOLDED="true" ID="ID_1458814955" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://medium.com/predict/three-popular-clustering-methods-and-when-to-use-each-4227c80ba2b6">
<node TEXT="Sep 21 2018  If youre interested in learning more about these techniques look up single link clustering complete link clustering clique margins and Wards&#xa0;" ID="ID_120904822" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Chapter 15 CLUSTERING METHODS" FOLDED="true" ID="ID_1710684991" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://www.cs.swarthmore.edu/~meeden/cs63/s16/reading/Clustering.pdf">
<node TEXT="and the mathematics underlying clustering techniques. The chapter begins by providing measures and criteria that are used for determining whether two ob-." ID="ID_226120859" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Data Mining Cluster Analysis" FOLDED="true" ID="ID_823107839" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://www.tutorialspoint.com/data_mining/dm_cluster_analysis.htm">
<node TEXT="Data Mining Cluster Analysis - Learn Data Mining in simple and easy steps starting  Miscellaneous Classification Methods Cluster Analysis Mining Text Data&#xa0;" ID="ID_1461867708" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Clustering Techniques Every Data Science Beginner Should Swear By" FOLDED="true" ID="ID_1310491271" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://www.analyticsindiamag.com/clustering-techniques-every-data-science-beginner-should-swear-by/">
<node TEXT="Dec 20 2018  Cluster analysis is the statistical method of grouping data into subsets that have application in the context of a selective problem. This technique&#xa0;" ID="ID_538008877" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Different Techniques of Data Clustering" FOLDED="true" ID="ID_1748025375" CREATED="1557224068610" MODIFIED="1557225479723" LINK="http://members.tripod.com/asim_saeed/paper.htm">
<node TEXT="Precisely Data Clustering is a technique in which the information that is logically similar is physically stored together. In order to increase the efficiency in the&#xa0;" ID="ID_1399363435" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Introduction to clustering techniques" FOLDED="true" ID="ID_53993631" CREATED="1557224068610" MODIFIED="1557225479723" LINK="http://www.iula.upf.edu/materials/040701wanner.pdf">
<node TEXT="Jul 1 2004  Introduction to Clustering Techniques. Definition 1 (Clustering) Clustering is a division of data into groups of similar ob- jects. Each group (= a&#xa0;" ID="ID_1278674985" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
</node>
<node TEXT="Agglomerative Clustering#$D$#" FOLDED="true" ID="ID_790620560" CREATED="1557224068610" MODIFIED="1557225483444">
<icon BUILTIN="stop-sign"/>
<node TEXT="sklearn.cluster.AgglomerativeClustering &#x2014; scikit-learn 0.20.3 " FOLDED="true" ID="ID_1090149327" CREATED="1557224068610" MODIFIED="1557225479723" LINK="http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html">
<node TEXT="class sklearn.cluster. AgglomerativeClustering (n_clusters=2 affinity=euclidean memory=None connectivity=None compute_full_tree=auto linkage=ward&#xa0;" ID="ID_925324945" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Hierarchical clustering - Wikipedia" FOLDED="true" ID="ID_400729258" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://en.wikipedia.org/wiki/Hierarchical_clustering">
<node TEXT="In data mining and statistics hierarchical clustering is a method of cluster analysis which seeks to build a&#xa0;" ID="ID_1316190035" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Hierarchical agglomerative clustering" FOLDED="true" ID="ID_801587538" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://nlp.stanford.edu/IR-book/html/htmledition/hierarchical-agglomerative-clustering-1.html">
<node TEXT="Bottom-up hierarchical clustering is therefore called hierarchical agglomerative clustering or HAC . Top-down clustering requires a method for splitting a cluster." ID="ID_1974659006" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Agglomerative Clustering: how it works - YouTube" FOLDED="true" ID="ID_1263889556" CREATED="1557224068610" MODIFIED="1557225479723" LINK="https://www.youtube.com/watch?v=XJ3194AmH40">
<node TEXT="Jan 19 2014  http://bit.ly/s-link] Agglomerative clustering guarantees that similar instances end up in the same cluster. We start by having each instance being&#xa0;" ID="ID_56977364" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Agglomerative Hierarchical Clustering - Datanovia" FOLDED="true" ID="ID_1941798902" CREATED="1557224068610" MODIFIED="1557225479739" LINK="https://www.datanovia.com/en/lessons/agglomerative-hierarchical-clustering/">
<node TEXT="The agglomerative clustering is the most common type of hierarchical clustering used to group objects in clusters based on their similarity. Its also known as&#xa0;" ID="ID_410144148" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Hierarchical Clustering and its Applications &#x2013; Towards Data Science" FOLDED="true" ID="ID_1507743628" CREATED="1557224068610" MODIFIED="1557225479739" LINK="https://towardsdatascience.com/hierarchical-clustering-and-its-applications-41c1ad4441a6">
<node TEXT="Oct 26 2018  Agglomerative clustering uses a bottom-up approach wherein each data point starts in its own cluster. These clusters are then joined greedily&#xa0;" ID="ID_804878311" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Agglomerative Clustering - RapidMiner Documentation" FOLDED="true" ID="ID_932304253" CREATED="1557224068610" MODIFIED="1557225479739" LINK="https://docs.rapidminer.com/latest/studio/operators/modeling/segmentation/agglomerative_clustering.html">
<node TEXT="Agglomerative clustering is a strategy of hierarchical clustering. Hierarchical clustering (also known as Connectivity based clustering) is a method of cluster&#xa0;" ID="ID_190306294" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Agglomerative Hierarchical Clustering Overview" FOLDED="true" ID="ID_836974939" CREATED="1557224068610" MODIFIED="1557225479739" LINK="http://www.improvedoutcomes.com/docs/WebSiteDocs/Clustering/Agglomerative_Hierarchical_Clustering_Overview.htm">
<node TEXT="Agglomerative hierarchical clustering is a bottom-up clustering method where clusters have sub-clusters which in turn have sub-clusters etc. The classic&#xa0;" ID="ID_1116135419" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Understanding the concept of Hierarchical clustering Technique" FOLDED="true" ID="ID_697668136" CREATED="1557224068610" MODIFIED="1557225479739" LINK="https://towardsdatascience.com/understanding-the-concept-of-hierarchical-clustering-technique-c6e8243758ec">
<node TEXT="Dec 10 2018  To understand better lets see a pictorial representation of Agglomerative Hierarchical clustering Technique. Lets say we have six data points {A&#xa0;" ID="ID_581253306" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Modern hierarchical agglomerative clustering algorithms" FOLDED="true" ID="ID_1002208916" CREATED="1557224068610" MODIFIED="1557225479739" LINK="https://arxiv.org/pdf/1109.2378">
<node TEXT="Sep 12 2011  This paper presents algorithms for hierarchical agglomerative clustering which perform most efficiently in the general-purpose setup that is&#xa0;" ID="ID_686996847" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
</node>
<node TEXT="Clustering Dendograms#$D$#" FOLDED="true" ID="ID_1551945484" CREATED="1557224068610" MODIFIED="1557225483444">
<icon BUILTIN="stop-sign"/>
<node TEXT="Dendrogram - Wikipedia" FOLDED="true" ID="ID_1558877376" CREATED="1557224068610" MODIFIED="1557225479739" LINK="https://en.wikipedia.org/wiki/Dendrogram">
<node TEXT="have been clustered by UPGMA based on a matrix of genetic distances. The hierarchical clustering dendrogram would show&#xa0;" ID="ID_1676178884" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Hierarchical Clustering / Dendrogram: Simple Definition Examples " FOLDED="true" ID="ID_1138456508" CREATED="1557224068610" MODIFIED="1557225479739" LINK="https://www.statisticshowto.datasciencecentral.com/hierarchical-clustering/">
<node TEXT="Nov 15 2016  What is hierarchical clustering (a dendrogram)? Definition and overview of clustering algorithms. Different linkage types and basic clustering&#xa0;" ID="ID_1339384815" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="Hierarchical Clustering / Dendrograms" FOLDED="true" ID="ID_6044723" CREATED="1557224068610" MODIFIED="1557225479739" LINK="https://ncss-wpengine.netdna-ssl.com/wp-content/themes/ncss/pdf/Procedures/NCSS/Hierarchical_Clustering-Dendrograms.pdf">
<node TEXT="The agglomerative hierarchical clustering algorithms available in this program  Looking at this dendrogram you can see the three clusters as three branches&#xa0;" ID="ID_719811931" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="What does the dendrogram show or what is correlation analysis?" FOLDED="true" ID="ID_1252752059" CREATED="1557224068610" MODIFIED="1557225479739" LINK="http://www.nonlinear.com/support/progenesis/comet/faq/v2.0/dendrogram.aspx">
<node TEXT="Draw a dendrogram showing clusters of compounds according to how strongly correlated the compounds are. This correlation can be seen in the abundance&#xa0;" ID="ID_1377729076" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="scipy.cluster.hierarchy.dendrogram &#x2014; SciPy v1.2.1 Reference Guide" FOLDED="true" ID="ID_1761490746" CREATED="1557224068610" MODIFIED="1557225479739" LINK="https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.dendrogram.html">
<node TEXT="The dendrogram illustrates how each cluster is composed by drawing a U-shaped link between a non-singleton cluster and its children. The top of the U-link&#xa0;" ID="ID_1694016684" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="What is a Dendrogram? How to use Dendrograms | Displayr" FOLDED="true" ID="ID_67231055" CREATED="1557224068610" MODIFIED="1557225479739" LINK="https://www.displayr.com/what-is-dendrogram/">
<node TEXT="A dendrogram is a diagram that shows the hierarchical relationship between objects. It is commonly created as an output from hierarchical clustering." ID="ID_508992214" CREATED="1557224068610" MODIFIED="1557224068610"/>
</node>
<node TEXT="scipy.cluster.hierarchy.dendrogram &#x2014; SciPy v0.14.0 Reference Guide" FOLDED="true" ID="ID_1904701855" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.cluster.hierarchy.dendrogram.html">
<node TEXT="The dendrogram illustrates how each cluster is composed by drawing a U-shaped link between a non-singleton cluster and its children. The height of the top of&#xa0;" ID="ID_1098272441" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Clustering with dendograms on interpretation variables | Request PDF" FOLDED="true" ID="ID_139520251" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://www.researchgate.net/publication/244104182_Clustering_with_dendograms_on_interpretation_variables">
<node TEXT="Mar 15 2019  Request PDF on ResearchGate | Clustering with dendograms on interpretation variables | Clustering techniques are used frequently in&#xa0;" ID="ID_789601810" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Hierarchical Clustering Tutorial: What is dendogram" FOLDED="true" ID="ID_553014922" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://people.revoledu.com/kardi/tutorial/Clustering/dendogram.htm">
<node TEXT="The standard output of hierarchical clustering is a dendogram. A dendogram is a cluster tree diagram where the distance of split or merge is recorded." ID="ID_22058049" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Hierarchical Clustering with Python and Scikit-Learn" FOLDED="true" ID="ID_394393913" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://stackabuse.com/hierarchical-clustering-with-python-and-scikit-learn/">
<node TEXT="Jul 12 2018  In the last section we said that once one large cluster is formed by the combination of small clusters dendrograms of the cluster are used to&#xa0;" ID="ID_271251322" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
</node>
<node TEXT="Agglomerative Clustering in Scikit-learn#$D$#" FOLDED="true" ID="ID_483280401" CREATED="1557224068611" MODIFIED="1557225483445">
<icon BUILTIN="stop-sign"/>
<node TEXT="sklearn.cluster.AgglomerativeClustering &#x2014; scikit-learn 0.20.3 " FOLDED="true" ID="ID_403968870" CREATED="1557224068611" MODIFIED="1557225479739" LINK="http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html">
<node TEXT="class sklearn.cluster. AgglomerativeClustering (n_clusters=2 affinity=euclidean memory=None connectivity=None compute_full_tree=auto linkage=ward&#xa0;" ID="ID_1276589697" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Hierarchical Clustering with Python and Scikit-Learn" FOLDED="true" ID="ID_1968657471" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://stackabuse.com/hierarchical-clustering-with-python-and-scikit-learn/">
<node TEXT="Jul 12 2018  In some cases the result of hierarchical and K-Means clustering can be similar. Before implementing hierarchical clustering using Scikit-Learn&#xa0;" ID="ID_153861619" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="2.3. Clustering &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_742917976" CREATED="1557224068611" MODIFIED="1557225479739" LINK="http://scikit-learn.org/stable/modules/clustering.html">
<node TEXT="Agglomerative clustering number of clusters linkage type distance Large n_samples and n_clusters Many clusters possibly connectivity constraints non&#xa0;" ID="ID_1338464728" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Agglomerative Clustering" FOLDED="true" ID="ID_1434761721" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://chrisalbon.com/machine_learning/clustering/agglomerative_clustering/">
<node TEXT="Dec 20 2017  Conduct Agglomerative Clustering. In scikit-learn AgglomerativeClustering uses the linkage parameter to determine the merging strategy to&#xa0;" ID="ID_368172728" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Hierarchical clustering: structured vs unstructured ward &#x2014; scikit " FOLDED="true" ID="ID_413202654" CREATED="1557224068611" MODIFIED="1557225479739" LINK="http://scikit-learn.org/stable/auto_examples/cluster/plot_ward_structured_vs_unstructured.html">
<node TEXT="In a first step the hierarchical clustering is performed without connectivity constraints on the structure and is solely based on distance whereas in a second step&#xa0;" ID="ID_486179302" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Unsupervised Machine Learning - Hierarchical Clustering with " FOLDED="true" ID="ID_1128144806" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://www.youtube.com/watch?v=EQZaSuK-PHs">
<node TEXT="Feb 2 2015  This machine learning tutorial covers unsupervised learning with  Learning - Hierarchical Clustering with Mean Shift Scikit-learn and Python." ID="ID_1434721829" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Agglomerative clustering with different metrics &#x2014; scikit-learn 0.20.3 " FOLDED="true" ID="ID_1456529357" CREATED="1557224068611" MODIFIED="1557225479739" LINK="http://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_clustering_metrics.html">
<node TEXT="Demonstrates the effect of different metrics on the hierarchical clustering.  np from sklearn.cluster import AgglomerativeClustering from sklearn.metrics import&#xa0;" ID="ID_78810685" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="An Introduction to Clustering Algorithms in Python &#x2013; Towards Data " FOLDED="true" ID="ID_835126546" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://towardsdatascience.com/an-introduction-to-clustering-algorithms-in-python-123438574097">
<node TEXT="May 29 2018  This would be an example of &#x201c;unsupervised learning&#x201d; since were not  friend blobby; i.e. the make_blobs function in Pythons sci-kit learn library.  Agglomerative hierarchical clustering differs from k-means in a key way." ID="ID_528160860" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Hierarchical Clustering" FOLDED="true" ID="ID_1147777961" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://pythonprogramming.net/hierarchical-clustering-machine-learning-python-scikit-learn/">
<node TEXT="Unsupervised Machine Learning: Hierarchical Clustering. Mean Shift cluster analysis example with Python and Scikit-learn&#xa0;" ID="ID_98720908" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Plot dendrogram using sklearn.AgglomerativeClustering - Stack " FOLDED="true" ID="ID_504242807" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://stackoverflow.com/questions/29127013/plot-dendrogram-using-sklearn-agglomerativeclustering">
<node TEXT="Here is a simple function for taking a hierarchical clustering model from sklearn and plotting it using the scipy dendrogram function. Seems like graphing&#xa0;" ID="ID_310211061" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
</node>
<node TEXT="Clustering Connectivity Constraints#$D$#" FOLDED="true" ID="ID_1304364571" CREATED="1557224068611" MODIFIED="1557225483445">
<icon BUILTIN="stop-sign"/>
<node TEXT="2.3. Clustering &#x2014; scikit-learn 0.20.3 documentation" FOLDED="true" ID="ID_713507825" CREATED="1557224068611" MODIFIED="1557225479739" LINK="http://scikit-learn.org/stable/modules/clustering.html">
<node TEXT="Ward hierarchical clustering number of clusters Large n_samples and n_clusters Many clusters possibly connectivity constraints Distances between points." ID="ID_1022436656" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Constrained clustering - Wikipedia" FOLDED="true" ID="ID_1612342524" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://en.wikipedia.org/wiki/Constrained_clustering">
<node TEXT="In computer science constrained clustering is a class of semi-supervised learning algorithms. Typically constrained clustering incorporates either a set of&#xa0;" ID="ID_1568407486" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Hierarchical clustering: structured vs unstructured ward &#x2014; scikit " FOLDED="true" ID="ID_48652573" CREATED="1557224068611" MODIFIED="1557225479739" LINK="http://scikit-learn.org/stable/auto_examples/cluster/plot_ward_structured_vs_unstructured.html">
<node TEXT="In a first step the hierarchical clustering is performed without connectivity constraints on the structure and is solely based on distance whereas in a second step&#xa0;" ID="ID_839322762" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Scikit-learn Agglomerative Clustering Connectivity Matrix - Stack " FOLDED="true" ID="ID_1583734784" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://stackoverflow.com/questions/42821622/scikit-learn-agglomerative-clustering-connectivity-matrix">
<node TEXT="Mar 21 2017  When passing a connectivity matrix to sklearn.cluster.  You can try reassigning centers after clustering to respect your constraints or youll&#xa0;" ID="ID_1297956023" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Maximum Split Clustering Under Connectivity Constraints " FOLDED="true" ID="ID_1245628582" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://link.springer.com/article/10.1007/s00357-003-0011-7">
<node TEXT="Maximum Split Clustering Under Connectivity Constraints. Authors; Authors and affiliations. Pierre Hansen; Brigitte Jaumard; Christophe Meyer; Bruno Simeone&#xa0;" ID="ID_72021398" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="(Hierarchical cluster analaysis) in R with connectivity constraints " FOLDED="true" ID="ID_930483591" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://www.quora.com/How-can-I-modify-the-hclust-Hierarchical-cluster-analaysis-in-R-with-connectivity-constraints-matrix-so-that-it-only-cluster-adjacent-nodes-as-described-in-the-constraints-matrix">
<node TEXT="How can I modify the hclust (Hierarchical cluster analaysis) in R with connectivity constraints matrix so that it only cluster adjacent nodes as described in the&#xa0;" ID="ID_1359422209" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Agglomerative connectivity constrained clustering for image " FOLDED="true" ID="ID_926742924" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://onlinelibrary.wiley.com/doi/pdf/10.1002/sam.10109">
<node TEXT="Jan 18 2011  Keywords: connectivity constrained clustering; agglomerative clustering; image . maximum split clustering with connectivity constraints." ID="ID_1236375169" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Connectivity Constraints - Fundamentals of Machine Learning with " FOLDED="true" ID="ID_1710343323" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://subscription.packtpub.com/video/big_data_and_business_intelligence/9781789134377/55308/55311/connectivity-constraints">
<node TEXT="Mar 28 2018  scikit-learn also allows specifying a connectivity matrix which can be used as a constraint when finding the clusters to merge. - Start with&#xa0;" ID="ID_467613673" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Maximum Split Clustering Under Connectivity Constraints " FOLDED="true" ID="ID_683042665" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://www.semanticscholar.org/paper/Maximum-Split-Clustering-Under-Connectivity/8ea04b083ca44b1e7af4c400ccb8384e7ec00945">
<node TEXT="The split of a cluster is the smallest dissimilarity between an entity of this cluster and an entity outside of it. The single-linkage algorithm (ignoring contiguity&#xa0;" ID="ID_893408959" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Maximum Split Clustering Under Connectivity Constraints" FOLDED="true" ID="ID_133845806" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://dl.acm.org/citation.cfm?id=1062349">
<node TEXT="Maximum Split Clustering Under Connectivity Constraints 2003 Article. Bibliometrics Data Bibliometrics. &#xb7; Citation Count: 3 &#xb7; Downloads (cumulative): n/a" ID="ID_255081541" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
</node>
</node>
<node TEXT="Connectivity Constraints" ID="ID_1155880132" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
</node>
<node TEXT="Introduction to Recommendation Systems" ID="ID_1042819677" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Naive User based systems" ID="ID_849009274" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Introduction to Recommendation Systems#$D$#" FOLDED="true" ID="ID_491844402" CREATED="1557224068611" MODIFIED="1557225483445">
<icon BUILTIN="stop-sign"/>
<node TEXT="Recommender Systems | Coursera" FOLDED="true" ID="ID_1612210236" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://www.coursera.org/specializations/recommender-systems">
<node TEXT="Learn Recommender Systems from University of Minnesota.  Introduction to Recommender Systems: Non-Personalized and Content-Based. 4.5. 388 ratings." ID="ID_1352161732" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Introduction to Recommender Systems in 2019 | Tryolabs Blog" FOLDED="true" ID="ID_15281996" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://tryolabs.com/blog/introduction-to-recommender-systems/">
<node TEXT="Many e-commerce and retail companies are leveraging the power of data and boosting sales by implementing recommender systems on their websites. In short&#xa0;" ID="ID_992594683" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Introduction to Recommender Systems: Non-Personalized and " FOLDED="true" ID="ID_548220121" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://www.coursera.org/learn/recommender-systems-introduction">
<node TEXT="Learn Introduction to Recommender Systems: Non-Personalized and Content-Based from University of Minnesota. This course which is designed to serve as&#xa0;" ID="ID_1597561684" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Introduction to Recommender System. Part 1 (Collaborative Filtering " FOLDED="true" ID="ID_1476628932" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://hackernoon.com/introduction-to-recommender-system-part-1-collaborative-filtering-singular-value-decomposition-44c9659c5e75">
<node TEXT="Jan 28 2018  Introduction. A recommender system refers to a system that is capable of predicting the future preference of a set of items for a user and&#xa0;" ID="ID_811029239" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Introduction to Recommender Systems Handbook" FOLDED="true" ID="ID_1243738603" CREATED="1557224068611" MODIFIED="1557225479739" LINK="http://www.inf.unibz.it/~ricci/papers/intro-rec-sys-handbook.pdf">
<node TEXT="Introduction to Recommender Systems. Handbook. Francesco Ricci Lior Rokach and Bracha Shapira. Abstract Recommender Systems (RSs) are software tools&#xa0;" ID="ID_1855554154" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Introduction to Recommender System. Part 2 (Neural Network " FOLDED="true" ID="ID_710085574" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://towardsdatascience.com/introduction-to-recommender-system-part-2-adoption-of-neural-network-831972c4cbf7">
<node TEXT="Feb 16 2018  In my last blog post of this series: Introduction to Recommender System. Part 1 (Collaborative Filtering Singular Value Decomposition) I talked&#xa0;" ID="ID_262416117" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Introduction to Recommender Systems" FOLDED="true" ID="ID_97534463" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://www.datascience.com/resources/white-papers/introduction-to-recommendation-engines-for-business">
<node TEXT="Netflix values the recommendation engine powering its content suggestions at $1 billion per year and Amazon says its system drives a 20-35% lift in sales&#xa0;" ID="ID_1709643407" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Recommender system - Wikipedia" FOLDED="true" ID="ID_439876697" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://en.wikipedia.org/wiki/Recommender_system">
<node TEXT="A recommender system or a recommendation system is a subclass of information filtering  Gerhard Friedrich (2010). Recommender Systems:An Introduction." ID="ID_1929140712" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="A Gentle Introduction to Recommender Systems with Implicit Feedback" FOLDED="true" ID="ID_1166700573" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://jessesw.com/Rec-System/">
<node TEXT="May 30 2016  Recommender systems have become a very important part of the retail social networking and entertainment industries. From providing advice&#xa0;" ID="ID_1877698693" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Recommender Systems An introduction" FOLDED="true" ID="ID_1037956297" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://pdfs.semanticscholar.org/5d1d/d378962c7601526f65f69e408f8800a0d3c4.pdf">
<node TEXT="Recommender Systems. An introduction. Dietmar Jannach TU Dortmund Germany. Slides presented at PhD School 2014 University Szeged Hungary." ID="ID_1321541974" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
</node>
</node>
<node TEXT="Content based Systems" ID="ID_1132706626" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Content based Systems#$D$#" FOLDED="true" ID="ID_1520603051" CREATED="1557224068611" MODIFIED="1557225483445">
<icon BUILTIN="stop-sign"/>
<node TEXT="Recommender system - Wikipedia" FOLDED="true" ID="ID_1926473760" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://en.wikipedia.org/wiki/Recommender_system">
<node TEXT="A recommender system or a recommendation system is a subclass of information filtering system that seeks to&#xa0;" ID="ID_1026837490" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="How to build a content-based movie recommender system with " FOLDED="true" ID="ID_406317233" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://towardsdatascience.com/how-to-build-from-scratch-a-content-based-movie-recommender-with-natural-language-processing-25ad400eb243">
<node TEXT="Oct 1 2018  The two main types of recommender systems are either collaborative or content-based filters: these two names are pretty self-explanatory but&#xa0;" ID="ID_1519183697" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Beginners Guide to learn about Content Based Recommender Engine" FOLDED="true" ID="ID_498330135" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://www.analyticsvidhya.com/blog/2015/08/beginners-guide-learn-content-based-recommender-systems/">
<node TEXT="Aug 11 2015  Broadly there are two types of recommendation systems &#x2013; Content Based  Collaborative filtering based. In this article well learn about&#xa0;" ID="ID_739638894" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Content-Based Recommender Systems &#x2013; Carlos Pinela &#x2013; Medium" FOLDED="true" ID="ID_1062750843" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://medium.com/@cfpinela/content-based-recommender-systems-a68c2aee2235">
<node TEXT="Nov 28 2017  Content-Based Recommender Systems are born from the idea of using the content of each item for recommending purposes and trying to&#xa0;" ID="ID_628884504" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="How to Build a Content-Based Recommender System For Your " FOLDED="true" ID="ID_95805943" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://www.offerzen.com/blog/how-to-build-a-content-based-recommender-system-for-your-product">
<node TEXT="Based on this Im going to introduce you to content-based filtering for a movie recommender system. Ill use Python as the programming language for the&#xa0;" ID="ID_1099859113" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Content-based Recommendation Systems" FOLDED="true" ID="ID_1295740813" CREATED="1557224068611" MODIFIED="1557225479739" LINK="http://www.fxpal.com/publications/FXPAL-PR-06-383.pdf">
<node TEXT="Abstract. This chapter discusses content-based recommendation systems i.e. systems that recommend an item to a user based upon a description of the item." ID="ID_1235589144" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Content-based Filtering" FOLDED="true" ID="ID_1964585949" CREATED="1557224068611" MODIFIED="1557225479739" LINK="http://recommender-systems.org/content-based-filtering/">
<node TEXT="Several issues have to be considered when implementing a content-based filtering system. First terms can either be assigned automatically or manually." ID="ID_1093349843" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="What Is Content-Based Filtering? - Hiring | Upwork" FOLDED="true" ID="ID_1426484309" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://www.upwork.com/hiring/data/what-is-content-based-filtering/">
<node TEXT="Content-based filtering is used in a number of applications including information retrieval (as in search engines) as well as recommender systems. In this article&#xa0;" ID="ID_1075966134" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Content-Based Recommendation Systems | SpringerLink" FOLDED="true" ID="ID_1336656964" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://link.springer.com/chapter/10.1007/978-3-540-72079-9_10">
<node TEXT="This chapter discusses content-based recommendation systems i.e. systems that recommend an item to a user based upon a description of the item and a&#xa0;" ID="ID_1217671885" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Lecture 16.2 &#x2014; Recommender Systems | Content Based " FOLDED="true" ID="ID_1017400805" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://www.youtube.com/watch?v=9siFuMMHNIA">
<node TEXT="Feb 9 2017  Lecture 16.2 &#x2014; Recommender Systems | Content Based Recommendations &#x2014; [ Andrew Ng ]. Artificial Intelligence - All in One. Loading." ID="ID_1750589284" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
</node>
</node>
<node TEXT="Blog" ID="ID_1870707605" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Analytics Vidya " ID="ID_943665214" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
</node>
</node>
<node TEXT="Model free collaborative filtering" ID="ID_912750094" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Singular Value Decomposition" ID="ID_455663786" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Model free collaborative filtering#$D$#" FOLDED="true" ID="ID_1562043055" CREATED="1557224068611" MODIFIED="1557225483445">
<icon BUILTIN="stop-sign"/>
<node TEXT="Model-free (or memory-based) collaborative filtering - Machine " FOLDED="true" ID="ID_827230115" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://www.oreilly.com/library/view/machine-learning-algorithms/9781789347999/b0fe1c1c-b442-4c42-9795-90e1cae036bc.xhtml">
<node TEXT="Model-free (or memory-based) collaborative filtering As with the user-based approach lets consider two sets of elements: users and items. However in this case&#xa0;" ID="ID_1153790766" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Collaborative filtering - Wikipedia" FOLDED="true" ID="ID_1596601168" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://en.wikipedia.org/wiki/Collaborative_filtering">
<node TEXT="Collaborative filtering (CF) is a technique used by recommender systems. Collaborative filtering  From Wikipedia the free encyclopedia. Jump to .. A number of applications combine the memory-based and the model-based CF algorithms." ID="ID_51681988" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="A model-free collaborative recommendation system in 20 lines of " FOLDED="true" ID="ID_229962135" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://www.bonaccorso.eu/2017/09/13/a-model-free-collaborative-recommendation-system-in-20-lines-of-python/">
<node TEXT="Sep 13 2017  A brief discussion about model-free or in-memory collaborative filtering recommendation systems with a fast Python implementation." ID="ID_968047178" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="How companies use collaborative filtering to learn exactly what you " FOLDED="true" ID="ID_776166903" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://medium.freecodecamp.org/how-companies-use-collaborative-filtering-to-learn-exactly-what-you-want-a3fc58e22ad9">
<node TEXT="Feb 14 2019  Well consider our collaborative filtering model a success if its able to fill in the zeros. This would mean that its able to predict how each user&#xa0;" ID="ID_865275257" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Where will I get sample data for collaborative filtering? - Quora" FOLDED="true" ID="ID_566265998" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://www.quora.com/Where-will-I-get-sample-data-for-collaborative-filtering">
<node TEXT="Where can I get data set for collaborative filtering system? 577 Views  Free guide to machine learning basics and advanced techniques. Download the ebook&#xa0;" ID="ID_1981090685" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Collaborative filtering enhanced by user free-text reviews topic " FOLDED="true" ID="ID_410026639" CREATED="1557224068611" MODIFIED="1557225479739" LINK="http://ieeexplore.ieee.org/document/6913637/">
<node TEXT="Collaborative filtering enhanced by user free-text reviews topic modelling. Abstract: User based collaborative filtering (UCF) and item based collaborative&#xa0;" ID="ID_1825394152" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Various Implementations of Collaborative Filtering &#x2013; Towards Data " FOLDED="true" ID="ID_540822901" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://towardsdatascience.com/various-implementations-of-collaborative-filtering-100385c6dfe0">
<node TEXT="Dec 28 2017  The most basic models for recommendations systems are collaborative filtering models which are based on assumption that people like things&#xa0;" ID="ID_1959920472" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Collaborative Filtering with the Trace Norm: Learning Bounding " FOLDED="true" ID="ID_866529532" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://www.cs.huji.ac.il/~shais/papers/ShamirShalevCOLT11.pdf">
<node TEXT="and despite previous attempts there are no distribution-free non-trivial .. model which we argue to more closely resemble collaborative filtering as done in&#xa0;" ID="ID_1957779269" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="How to build a collaborative filtering model for personalized " FOLDED="true" ID="ID_1338967629" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://towardsdatascience.com/how-to-build-a-collaborative-filtering-model-for-personalized-recommendations-using-tensorflow-and-b9a77dc1320">
<node TEXT="Apr 10 2018  In this article I will step you through how to use TensorFlows Estimator API to build a WALS collaborative filtering model for product&#xa0;" ID="ID_692743166" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
</node>
<node TEXT="Singular Value Decomposition#$D$#" FOLDED="true" ID="ID_840529629" CREATED="1557224068611" MODIFIED="1557225483445">
<icon BUILTIN="stop-sign"/>
<node TEXT="Singular value decomposition - Wikipedia" FOLDED="true" ID="ID_113487939" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://en.wikipedia.org/wiki/Singular_value_decomposition">
<node TEXT="In linear algebra the singular-value decomposition (SVD) is a factorization of a real or complex matrix. It is the generalization of the eigendecomposition of a&#xa0;" ID="ID_875397454" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Singular Value Decomposition -- from Wolfram MathWorld" FOLDED="true" ID="ID_1129967650" CREATED="1557224068611" MODIFIED="1557225479739" LINK="http://mathworld.wolfram.com/SingularValueDecomposition.html">
<node TEXT="then A can be written using a so-called singular value decomposition of the form  Singular value decomposition is implemented in the Wolfram Language as&#xa0;" ID="ID_531110927" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Singular Value Decomposition" FOLDED="true" ID="ID_1798588033" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/book-chapter-4.pdf">
<node TEXT="The singular value decomposition of a matrix A is the factorization of A into the  Also singular value decomposition is defined for all matrices (rectangular or&#xa0;" ID="ID_568076742" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Singular Value Decomposition (SVD) tutorial" FOLDED="true" ID="ID_742905470" CREATED="1557224068611" MODIFIED="1557225479739" LINK="http://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm">
<node TEXT="Singular Value Decomposition (SVD) tutorial. BE.400 / 7.548. Singular value decomposition takes a rectangular matrix of gene expression data (defined as A&#xa0;" ID="ID_614031061" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Singular Value Decomposition (the SVD) - YouTube" FOLDED="true" ID="ID_453432349" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://www.youtube.com/watch?v=mBcLRGuAFUk">
<node TEXT="May 6 2016  MIT RES.18-009 Learn Differential Equations: Up Close with Gilbert Strang and Cleve Moler Fall 2015 View the complete course:&#xa0;" ID="ID_467212824" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="A Gentle Introduction to Singular-Value Decomposition (SVD) for " FOLDED="true" ID="ID_454764799" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/">
<node TEXT="Feb 26 2018  The Singular-Value Decomposition or SVD for short is a matrix decomposition method for reducing a matrix to its constituent parts in order to&#xa0;" ID="ID_1563338206" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Introduction to Singular Value Decomposition - Deep Learning Book " FOLDED="true" ID="ID_373138098" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.8-Singular-Value-Decomposition/">
<node TEXT="Mar 26 2018  Introduction to the Singular Value Decomposition (SVD). We will use Python to get a practical and visual intuition of the Singular Value&#xa0;" ID="ID_237433991" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="Lecture 47 &#x2014; Singular Value Decomposition | Stanford University " FOLDED="true" ID="ID_1582038591" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://www.youtube.com/watch?v=P5mlg91as1c">
<node TEXT="Apr 13 2016  Copyright Disclaimer Under Section 107 of the Copyright Act 1976 allowance is made for FAIR USE for purposes such as criticism comment&#xa0;" ID="ID_268126860" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="SingularValueDecomposition&#x2014;Wolfram Language Documentation" FOLDED="true" ID="ID_169445744" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://reference.wolfram.com/language/ref/SingularValueDecomposition.html">
<node TEXT="SingularValueDecomposition[m] gives the singular value decomposition for a numerical matrix m as a list of matrices {u w v} where w is a diagonal matrix and&#xa0;" ID="ID_1904826473" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="We Recommend a Singular Value Decomposition" FOLDED="true" ID="ID_1314892495" CREATED="1557224068611" MODIFIED="1557225479739" LINK="http://www.ams.org/publicoutreach/feature-column/fcarc-svd">
<node TEXT="The topic of this article the singular value decomposition is one that should be a part of the standard mathematics undergraduate curriculum but all too often&#xa0;" ID="ID_208019188" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
</node>
</node>
<node TEXT="Alternating least squares" ID="ID_1240672911" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Alternating least squares#$D$#" FOLDED="true" ID="ID_319394984" CREATED="1557224068611" MODIFIED="1557225483446">
<icon BUILTIN="stop-sign"/>
<node TEXT="Alternating Least Square (ALS)" FOLDED="true" ID="ID_217686790" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://towardsdatascience.com/prototyping-a-recommender-system-step-by-step-part-2-alternating-least-square-als-matrix-4a76c58714a1">
<node TEXT="Nov 17 2018  Prototyping a Recommender System Step by Step Part 2: Alternating Least Square (ALS) Matrix Factorization in Collaborative Filtering." ID="ID_483417804" CREATED="1557224068611" MODIFIED="1557224068611"/>
</node>
<node TEXT="What is the Alternating Least Squares method in recommendation " FOLDED="true" ID="ID_418540093" CREATED="1557224068611" MODIFIED="1557225479739" LINK="https://www.quora.com/What-is-the-Alternating-Least-Squares-method-in-recommendation-systems-And-why-does-this-algorithm-work-intuition-behind-this">
<node TEXT="Alternating Least Squares (ALS) represents a different approach to optimizing the loss function. The key insight is that you can turn the non-convex optimization&#xa0;" ID="ID_1041003745" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="14 Matrix Completion via Alternating Least Square(ALS)" FOLDED="true" ID="ID_1567479814" CREATED="1557224068612" MODIFIED="1557225479739" LINK="http://stanford.edu/~rezab/classes/cme323/S15/notes/lec14.pdf">
<node TEXT="May 13 2015  14 Matrix Completion via Alternating Least Square(ALS)  This approach is known as ALS(Alternating Least Squares). For our objective&#xa0;" ID="ID_1567496460" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="Apache Flink 1.2 Documentation: Alternating Least Squares" FOLDED="true" ID="ID_1379712931" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://ci.apache.org/projects/flink/flink-docs-release-1.2/dev/libs/ml/als.html">
<node TEXT="The alternating least squares (ALS) algorithm factorizes a given matrix R into two factors U and V such that R&#x2248;UTV. The unknown row dimension is given as a&#xa0;" ID="ID_557248037" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="Alternating Least Squares &#x2013; Data Science Made Simpler" FOLDED="true" ID="ID_1463040229" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://datasciencemadesimpler.wordpress.com/tag/alternating-least-squares/">
<node TEXT="Posts about Alternating Least Squares written by roireshef." ID="ID_383596492" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="Alternating Least Squares Method for Collaborative Filtering | Bugra " FOLDED="true" ID="ID_1488915118" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://bugra.github.io/work/notes/2014-04-19/alternating-least-squares-method-for-collaborative-filtering/">
<node TEXT="Apr 19 2014  Therefore we will adopt an alternating least squares approach with regularization. By doing so we first estimate Y using X and estimate X by&#xa0;" ID="ID_314197071" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="Collaborative Filtering - RDD-based API - Spark 2.4.2 Documentation" FOLDED="true" ID="ID_796603935" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://spark.apache.org/docs/latest/mllib-collaborative-filtering.html">
<node TEXT="alternating least squares (ALS)  be used to predict missing entries. spark.mllib uses the alternating least squares (ALS) algorithm to learn these latent factors." ID="ID_436693679" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="ALS Implicit Collaborative Filtering &#x2013; Rn Engineering &#x2013; Medium" FOLDED="true" ID="ID_834556531" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://medium.com/radon-dev/als-implicit-collaborative-filtering-5ed653ba39fe">
<node TEXT="Aug 23 2017  Alternating Least Squares (ALS) is a the model well use to fit our data and find similarities. But before we dive into how it works we should look&#xa0;" ID="ID_725851801" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="Alternating Least Squares (ALS) Spark ML - Elena Cuoco" FOLDED="true" ID="ID_1740301308" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://www.elenacuoco.com/2016/12/22/alternating-least-squares-als-spark-ml/">
<node TEXT="Dec 22 2016  Using Alternating Least Squares (ALS) algorithm to solve the Santander Kaggle competition 2016. How to recommend the top seven products&#xa0;" ID="ID_209323873" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="Least-Squares Method - Wikiversity" FOLDED="true" ID="ID_187863090" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://en.wikiversity.org/wiki/Least-Squares_Method">
<node TEXT="Jan 21 2018  We can also classify these methods further: ordinary least squares (OLS) weighted least squares (WLS) and alternating least squares (ALS)&#xa0;" ID="ID_229157631" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
</node>
</node>
</node>
<node TEXT="Fundamentals of Deep Networks" ID="ID_542220959" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Defining Deep learning" ID="ID_1640416969" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Deep Learning#$D$#" FOLDED="true" ID="ID_1117623201" CREATED="1557224068596" MODIFIED="1557225483421">
<icon BUILTIN="stop-sign"/>
<node TEXT="Deep learning - Wikipedia" FOLDED="true" ID="ID_1606748303" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://en.wikipedia.org/wiki/Deep_learning">
<node TEXT="Deep learning is part of a broader family of machine learning methods based on the layers used in artificial neural networks. Learning can be supervised&#xa0;" ID="ID_1720805530" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="Deep Learning" FOLDED="true" ID="ID_1810764886" CREATED="1557224068596" MODIFIED="1557225479614" LINK="http://deeplearning.net/">
<node TEXT="Deep Learning is a new area of Machine Learning research which has been introduced with the objective of moving Machine Learning closer to one of its&#xa0;" ID="ID_1310358674" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="What Is Deep Learning? | How It Works Techniques  Applications " FOLDED="true" ID="ID_932672000" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://www.mathworks.com/discovery/deep-learning.html">
<node TEXT="Deep learning is a machine learning technique that teaches computers to learn by example. Learn more about deep learning with MATLAB examples and tools." ID="ID_1188425439" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="Deep Learning" FOLDED="true" ID="ID_1624643423" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://www.deeplearningbook.org/">
<node TEXT="The Deep Learning textbook is a resource intended to help students and practitioners enter the field of machine learning in general and deep learning in&#xa0;" ID="ID_1175362372" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="Intro to TensorFlow for Deep Learning | Udacity" FOLDED="true" ID="ID_992049899" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://www.udacity.com/course/intro-to-tensorflow-for-deep-learning--ud187">
<node TEXT="Learn how to build deep learning applications with TensorFlow. This course was developed by the TensorFlow team and Udacity as a practical approach to&#xa0;" ID="ID_881481981" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="Deep Learning | NVIDIA Developer" FOLDED="true" ID="ID_1438912833" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://developer.nvidia.com/deep-learning">
<node TEXT="Deep learning is a subset of AI and machine learning that uses multi-layered artificial neural networks to deliver state-of-the-art accuracy in tasks such as object&#xa0;" ID="ID_758129190" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="Deep Learning - MIT Technology Review" FOLDED="true" ID="ID_1509416775" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://www.technologyreview.com/s/513696/deep-learning/">
<node TEXT="Apr 23 2013  Intelligent Machines. Deep Learning. With massive amounts of computational power machines can now recognize objects and translate&#xa0;" ID="ID_1983676999" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="What Is Deep Learning AI? A Simple Guide With 8 Practical Examples" FOLDED="true" ID="ID_951706154" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://www.forbes.com/sites/bernardmarr/2018/10/01/what-is-deep-learning-ai-a-simple-guide-with-8-practical-examples/">
<node TEXT="Oct 1 2018  Artificial intelligence machine learning and deep learning are some of the biggest buzzwords around today. This guide provides a simple&#xa0;" ID="ID_360996103" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="Neural networks and deep learning" FOLDED="true" ID="ID_390667558" CREATED="1557224068596" MODIFIED="1557225479614" LINK="http://neuralnetworksanddeeplearning.com/chap6.html">
<node TEXT="In the last chapter we learned that deep neural networks are often much harder to train than shallow neural networks. Thats unfortunate since we have good&#xa0;" ID="ID_1076059465" CREATED="1557224068596" MODIFIED="1557224068596"/>
</node>
<node TEXT="A Beginners Guide to Neural Networks and Deep Learning | Skymind" FOLDED="true" ID="ID_1618201244" CREATED="1557224068596" MODIFIED="1557225479614" LINK="https://skymind.ai/wiki/neural-network">
<node TEXT="Deep learning maps inputs to outputs. It finds correlations. It is known as a &#x201c;universal approximator&#x201d; because it can learn to approximate an unknown function&#xa0;" ID="ID_1048334232" CREATED="1557224068597" MODIFIED="1557224068597"/>
</node>
</node>
<node TEXT="Defining Deep learning#$D$#" FOLDED="true" ID="ID_982644902" CREATED="1557224068612" MODIFIED="1557225483446">
<icon BUILTIN="stop-sign"/>
<node TEXT="Deep learning - Wikipedia" FOLDED="true" ID="ID_1795386431" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://en.wikipedia.org/wiki/Deep_learning">
<node TEXT="Deep learning is part of a broader family of machine learning methods based on the layers .. Deep reinforcement learning has been used to approximate the value of possible direct marketing actions defined in terms of RFM variables." ID="ID_1653663857" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="What is deep learning (deep neural network)? - Definition from " FOLDED="true" ID="ID_1452524486" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://searchenterpriseai.techtarget.com/definition/deep-learning-deep-neural-network">
<node TEXT="This definition explains the meaning of deep learning and explains how this  rate depends entirely upon the programmers ability to accurately define a feature&#xa0;" ID="ID_1146695438" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="Machine learning - Wikipedia" FOLDED="true" ID="ID_69533094" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://en.wikipedia.org/wiki/Machine_learning">
<node TEXT="Machine learning (ML) is the scientific study of algorithms and statistical models that computer .. The defining characteristic of a rule-based machine learning algorithm is the identification and utilization of a set of relational rules that collectively&#xa0;" ID="ID_1533817919" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="Deep Learning Definition" FOLDED="true" ID="ID_1062628341" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://www.investopedia.com/terms/d/deep-learning.asp">
<node TEXT="Apr 18 2019  Deep learning is an artificial intelligence function that imitates the workings of the human brain in processing data and creating patterns for use&#xa0;" ID="ID_1372948839" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="What is Deep Learning?" FOLDED="true" ID="ID_1796838913" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://machinelearningmastery.com/what-is-deep-learning/">
<node TEXT="Aug 16 2016  In the soon to be published book titled &#x201c;Deep Learning&#x201d; co-authored with Ian Goodfellow and Aaron Courville they define deep learning in&#xa0;" ID="ID_1658993292" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="A Beginners Guide to Neural Networks and Deep Learning | Skymind" FOLDED="true" ID="ID_1403928437" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://skymind.ai/wiki/neural-network">
<node TEXT="It is a strictly defined term that means more than one hidden layer. In deep-learning networks each layer of nodes trains on a distinct set of features based on the&#xa0;" ID="ID_1607139659" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="What is Machine Learning? | Emerj" FOLDED="true" ID="ID_965971853" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://emerj.com/ai-glossary-terms/what-is-machine-learning/">
<node TEXT="Feb 19 2019  We asked 6 machine learning experts (including machine learning godfather Dr. Yoshua Bengio) to define Machine Learning as simply as&#xa0;" ID="ID_1494301715" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="How do you define deep learning in 4 steps? &#x2013; Anthony Sarkis " FOLDED="true" ID="ID_104127407" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://medium.com/@anthony_sarkis/how-do-you-define-deep-learning-in-4-steps-9b8308aacafa">
<node TEXT="Jun 11 2018  Given the way the author defined step 4 and the rest of the article I think the author is actually trying to reference deep learning. If thats the&#xa0;" ID="ID_1115728989" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="What is Deep Learning? - Definition from Techopedia" FOLDED="true" ID="ID_1785747862" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://www.techopedia.com/definition/30325/deep-learning">
<node TEXT="Deep learning is a collection of algorithms used in machine learning used to model high-level abstractions in data through the use of model architectures which&#xa0;" ID="ID_1890843987" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="Machine Learning: What it is and why it matters | SAS" FOLDED="true" ID="ID_1364721718" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://www.sas.com/en_us/insights/analytics/machine-learning.html">
<node TEXT="Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can&#xa0;" ID="ID_1354294960" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
</node>
</node>
<node TEXT="Common Architectural Principles of deep networks" ID="ID_1651589908" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Fundamentals of Deep Networks#$D$#" FOLDED="true" ID="ID_616633896" CREATED="1557224068612" MODIFIED="1557225483446">
<icon BUILTIN="stop-sign"/>
<node TEXT="Fundamentals of Deep Learning - Starting with Artificial Neural " FOLDED="true" ID="ID_1630002499" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/">
<node TEXT="Mar 16 2016  This article explains artificial neural network fundamental of deep learning for beginners. it also explains forward  backward propogation." ID="ID_581319891" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="Fundamentals of Deep Learning: Designing Next-Generation " FOLDED="true" ID="ID_15671333" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://www.amazon.com/Fundamentals-Deep-Learning-Next-Generation-Intelligence/dp/1491925612">
<node TEXT="With the reinvigoration of neural networks in the 2000s deep learning has become an extremely active area of research one thats paving the way for modern&#xa0;" ID="ID_1042539415" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="Fundamentals of Deep Learning - OReilly Media" FOLDED="true" ID="ID_612476351" CREATED="1557224068612" MODIFIED="1557225479739" LINK="http://shop.oreilly.com/product/0636920039709.do">
<node TEXT="With the reinvigoration of neural networks in the 2000s deep learning has become an extremely active area of research one thats paving the way for modern&#xa0;" ID="ID_460181411" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="Fundamentals of Deep Learning [Book]" FOLDED="true" ID="ID_728561076" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://www.oreilly.com/library/view/fundamentals-of-deep/9781491925607/">
<node TEXT="With the reinvigoration of neural networks in the 2000s deep learning has become an extremely active area of research one thats paving the way for modern&#xa0;" ID="ID_64924343" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="Amazon.com: Fundamentals of Deep Learning: Designing Next " FOLDED="true" ID="ID_897466578" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://www.amazon.com/Fundamentals-Deep-Learning-Next-Generation-Intelligence-ebook/dp/B0728KKXWB">
<node TEXT="Editorial Reviews. Book Description. How to Simulate the Mind. About the Author. Nikhil Buduma is a computer science student at MIT with deep interests in&#xa0;" ID="ID_246695091" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="Deep Learning Fundamentals - Cognitive Class" FOLDED="true" ID="ID_37707244" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://cognitiveclass.ai/courses/introduction-deep-learning/">
<node TEXT="DeepLearning.TV. Deep Learning Fundamentals. The further one dives into the ocean the more unfamiliar the territory can become. Deep learning at the&#xa0;" ID="ID_987143996" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="Foundations of Deep Learning | Simons Institute for the Theory of " FOLDED="true" ID="ID_954986510" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://simons.berkeley.edu/programs/dl2019">
<node TEXT="Deep learning is the engine powering many of the recent successes of artificial intelligence. These advances stem from a research effort spanning academia&#xa0;" ID="ID_287877960" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="Fundamentals of Deep Learning" FOLDED="true" ID="ID_1563220889" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://www.researchgate.net/profile/Malini_Chaudhri2/publication/317638461_DEEP_LEARNING_ALGORITHM-zaloni/data/59451d2e0f7e9b6910ee3db1/DEEP-LEARNING-ALGORITHM-zaloni.pdf">
<node TEXT="The OReilly logo is a registered trademark of OReilly Media Inc. Fundamentals of Deep Learning the cover image and related trade dress are trademarks of&#xa0;" ID="ID_982724995" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="Classes Workshops Training | NVIDIA Deep Learning Institute" FOLDED="true" ID="ID_1055158526" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://www.nvidia.com/en-us/deep-learning-ai/education/">
<node TEXT="The NVIDIA Deep Learning Institute (DLI) offers hands-on training in AI and  If youre new to deep learning start with Fundamentals to learn how to train and&#xa0;" ID="ID_1958115897" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="Fundamentals of Deep Learning" FOLDED="true" ID="ID_1267663445" CREATED="1557224068612" MODIFIED="1557225479739" LINK="http://perso.ens-lyon.fr/jacques.jayez/Cours/Implicite/Fundamentals_of_Deep_Learning.pdf">
<node TEXT="Fundamentals of Deep Learning. Designing Next-Generation Machine. Intelligence Algorithms with contributions by Nicholas Locascio. Boston Farnham&#xa0;" ID="ID_1830924629" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
</node>
<node TEXT="Common Architectural Principles of deep networks#$D$#" FOLDED="true" ID="ID_1258099791" CREATED="1557224068612" MODIFIED="1557225483446">
<icon BUILTIN="stop-sign"/>
<node TEXT="4. Major Architectures of Deep Networks - Deep Learning [Book]" FOLDED="true" ID="ID_50127555" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://www.oreilly.com/library/view/deep-learning/9781491924570/ch04.html">
<node TEXT="Major Architectures of Deep Networks The mother art is architecture.  We commonly refer to the sets of weights in a convolutional layer as a filter (or kernel). .. and in principle can compute anything a traditional computer can compute." ID="ID_1842634473" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="Common architectures in convolutional neural networks." FOLDED="true" ID="ID_1236947863" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://www.jeremyjordan.me/convnet-architectures/">
<node TEXT="Apr 19 2018  Common architectures in convolutional neural networks.  principles of successively applying convolutional layers to the input  The general architecture is quite similar to LeNet-5 although this model is considerably larger." ID="ID_1348991549" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="2. Fundamentals of Deep Networks - Getting started with deep " FOLDED="true" ID="ID_1212463191" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://www.oreilly.com/library/view/getting-started-with/9781492037330/ch02.html">
<node TEXT="Chapter 2. Fundamentals of Deep Networks Now here you see it takes all the running you can do to keep in the same place. If you want to get somewhere ." ID="ID_1104856813" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="Top 5 Deep Learning Architectures | Packt Hub" FOLDED="true" ID="ID_1624775867" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://hub.packtpub.com/top-5-deep-learning-architectures/">
<node TEXT="Jul 24 2018  Convolutional Neural Networks or CNNs in short are the popular  Here is an in-depth look at the CNN Architecture and its working  Autoencoders apply the principle of backpropagation in an unsupervised environment." ID="ID_652713024" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="Deep learning - Wikipedia" FOLDED="true" ID="ID_1919331806" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://en.wikipedia.org/wiki/Deep_learning">
<node TEXT="Deep learning is part of a broader family of machine learning methods based on the layers . The principle of elevating raw features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the raw .. Learning in the most common deep architectures is implemented using&#xa0;" ID="ID_1554836547" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="Deep Networks and the Architecture of Complexity | Center for " FOLDED="true" ID="ID_76017281" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://cnec.columbia.edu/deep-networks-and-architecture-complexity">
<node TEXT="Fifty years ago Herbert Simon argued that common hierarchical principles govern the organization of many complex interacting systems in physics biology&#xa0;" ID="ID_731098160" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="Artificial neural network - Wikipedia" FOLDED="true" ID="ID_138963940" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://en.wikipedia.org/wiki/Artificial_neural_network">
<node TEXT="Artificial neural networks (ANN) or connectionist systems are computing systems vaguely . Once sufficiently many layers have been learned the deep architecture may be used .. Such networks are commonly depicted in the manner shown at the top of the figure  Unfortunately these general principles are ill-defined." ID="ID_598424822" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="Neural networks and deep learning" FOLDED="true" ID="ID_271575784" CREATED="1557224068612" MODIFIED="1557225479739" LINK="http://neuralnetworksanddeeplearning.com/chap6.html">
<node TEXT="But what if instead of starting with a network architecture which is tabula rasa we used an .. Those programs worked from first principles and got right down into the details of . This is a common pattern in convolutional neural networks." ID="ID_110139135" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="Deep Convolutional Neural Network Design Patterns" FOLDED="true" ID="ID_613132937" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://arxiv.org/pdf/1611.00847">
<node TEXT="Nov 14 2016  ray of architecture choices and therefore opt to use an older architecture (i.e.  We ask: Do universal principles of deep network design exist? . A common thread throughout many of the more successful architectures is to&#xa0;" ID="ID_836030591" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="Three Classes of Deep Learning Architectures and Their Applications" FOLDED="true" ID="ID_1425473963" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://pdfs.semanticscholar.org/5bd4/177440c17dad736f1e0d2227694d612f5a59.pdf">
<node TEXT="Since 2006 deep structured learning or more commonly called deep learning . common to these shallow learning models is the relatively simple architecture . principle SESM may also be used to effectively initialize the. DNN training." ID="ID_852216271" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
</node>
</node>
<node TEXT="Building blocks of deep networks" ID="ID_941000811" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
<node TEXT="Building blocks of deep networks#$D$#" FOLDED="true" ID="ID_223537781" CREATED="1557224068612" MODIFIED="1557225483446">
<icon BUILTIN="stop-sign"/>
<node TEXT="Building blocks of deep neural networks" FOLDED="true" ID="ID_963369963" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://www.coursera.org/lecture/neural-networks-deep-learning/building-blocks-of-deep-neural-networks-uGCun">
<node TEXT="Aug 8 2017  Video created by deeplearning.ai for the course Neural Networks and Deep Learning. Understand the key computations underlying deep&#xa0;" ID="ID_1952159170" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="18 : An Overview of Deep Learning building blocks 1 An overview of " FOLDED="true" ID="ID_576640018" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://www.cs.cmu.edu/~epxing/Class/10708-17/notes-17/10708-scribe-lecture18.pdf">
<node TEXT="18 : An Overview of Deep Learning building blocks. Lecturer: Maruan Al-Shedivat. Scribes: Lisa Lee Chaoyang Wang. 1 An overview of the DL components." ID="ID_1248879832" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="The building blocks of Deep Learning &#xb7; Deep learning at the " FOLDED="true" ID="ID_1345157346" CREATED="1557224068612" MODIFIED="1557225479739" LINK="http://deepdish.io/2015/11/21/building-blocks-of-deep-learning/">
<node TEXT="The building blocks of Deep Learning. 21 Nov 2015 Gustav Larsson. A feed-forward network is built up of nodes that make a directed acyclic graph (DAG)." ID="ID_695746718" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="Neural Networks Building Blocks &#x2013; Eugenio Culurciello &#x2013; Medium" FOLDED="true" ID="ID_524896492" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://medium.com/@culurciello/neural-networks-building-blocks-a5c47bcd7c8d">
<node TEXT="Nov 1 2017  Neural networks are made of smaller modules or building blocks similarly to atoms in matter and logic gates in digital circuits. Once you know&#xa0;" ID="ID_770880104" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="Artificial Neural Network Building Blocks" FOLDED="true" ID="ID_345866663" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://www.tutorialspoint.com/artificial_neural_network/artificial_neural_network_building_blocks.htm">
<node TEXT="Artificial Neural Network Building Blocks - Learn Artificial Neural Network in simple and easy steps starting from basic to advanced concepts with examples&#xa0;" ID="ID_32344052" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="McCulloch Pitts Neuron &#x2014; Deep Learning Building Blocks" FOLDED="true" ID="ID_1472109599" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://hackernoon.com/mcculloch-pitts-neuron-deep-learning-building-blocks-7928f4e0504d">
<node TEXT="Feb 15 2019  The fundamental block of deep learning is artificial neuron i.e. it takes a weighted aggregate of inputs applies a function and gives an output." ID="ID_320189696" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="Residual blocks &#x2014; Building blocks of ResNet &#x2013; Towards Data Science" FOLDED="true" ID="ID_1651574752" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://towardsdatascience.com/residual-blocks-building-blocks-of-resnet-fd90ca15d6ec">
<node TEXT="Nov 27 2018  Understanding a residual block is quite easy. In traditional neural networks each layer feeds into the next layer. In a network with residual&#xa0;" ID="ID_859771347" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="Building Blocks of Neural Networks - Deep Learning with PyTorch " FOLDED="true" ID="ID_1219658496" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://www.oreilly.com/library/view/deep-learning-with/9781788624336/5a68470a-8fdb-4dc8-9613-e38a9b4354d5.xhtml">
<node TEXT="Building Blocks of Neural Networks Understanding the basic building blocks of a neural network such as tensors tensor operations and gradient descents&#xa0;" ID="ID_1775741587" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="The Building Blocks of Deep Learning - YouTube" FOLDED="true" ID="ID_387125617" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://www.youtube.com/watch?v=lICRfM2UeOo">
<node TEXT="Sep 2 2018  Neural networks at their most basic.  The Building Blocks of Deep Learning. Mike Bernico. Loading Unsubscribe from Mike Bernico? Cancel" ID="ID_1997310253" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
<node TEXT="The Building Blocks of Interpretability" FOLDED="true" ID="ID_1792290389" CREATED="1557224068612" MODIFIED="1557225479739" LINK="https://distill.pub/2018/building-blocks">
<node TEXT="Mar 6 2018  With the growing success of neural networks there is a corresponding need to be able to explain their decisions &#x2014; including building&#xa0;" ID="ID_388288820" CREATED="1557224068612" MODIFIED="1557224068612"/>
</node>
</node>
</node>
<node TEXT="Cheatsheet" ID="ID_1500239675" CREATED="1563517391360" MODIFIED="1563517391360" Folded="true">
<attribute NAME="Type" VALUE="syllabus_point"/>
</node>
</node>
</node>
</node>
</map>
